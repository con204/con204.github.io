<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>advanced on Amazon EKS Workshop</title>
    <link>/tags/advanced/</link>
    <description>Recent content in advanced on Amazon EKS Workshop</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Oct 2021 13:38:20 +0000</lastBuildDate><atom:link href="/tags/advanced/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Advanced</title>
      <link>/advanced/</link>
      <pubDate>Tue, 12 Oct 2021 14:34:29 +0000</pubDate>
      
      <guid>/advanced/</guid>
      <description>Advanced </description>
    </item>
    
    <item>
      <title>CI/CD with CodePipeline</title>
      <link>/intermediate/220_codepipeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/intermediate/220_codepipeline/</guid>
      <description>CI/CD with CodePipeline Continuous integration (CI) and continuous delivery (CD) are essential for deft organizations. Teams are more productive when they can make discrete changes frequently, release those changes programmatically and deliver updates without disruption.
In this module, we will build a CI/CD pipeline using AWS CodePipeline. The CI/CD pipeline will deploy a sample Kubernetes service, we will make a change to the GitHub repository and observe the automated delivery of this change to the cluster.</description>
    </item>
    
    <item>
      <title>Tracing with X-Ray</title>
      <link>/intermediate/245_x-ray/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/intermediate/245_x-ray/</guid>
      <description>Tracing with X-Ray As distributed systems evolve, monitoring and debugging services becomes challenging. Container-orchestration platforms like Kubernetes solve a lot of problems, but they also introduce new challenges for developers and operators in understanding how services interact and where latency exists. AWS X-Ray helps developers analyze and debug distributed services.
In this module, we are going to deploy the X-Ray agent as a DaemonSet, deploy sample front-end and back-end services that are instrumented with the X-Ray SDK, make some sample requests and then examine the traces and service maps in the AWS Management Console.</description>
    </item>
    
    <item>
      <title>GitOps with Weave Flux</title>
      <link>/intermediate/260_weave_flux/</link>
      <pubDate>Sun, 14 Oct 2018 19:56:14 -0400</pubDate>
      
      <guid>/intermediate/260_weave_flux/</guid>
      <description>GitOps with Weave Flux GitOps, a term coined by Weaveworks, is a way to do continuous delivery. Git is used as single source of truth for deploying into your cluster. This is easy for a development team as they are already familiar with git and do not need to know other tools. Weave Flux is a tool that runs in your Kubernetes cluster and implements changes based on monitoring Git and image repositories.</description>
    </item>
    
    <item>
      <title>Custom Resource Definition</title>
      <link>/intermediate/270_custom_resource_definition/</link>
      <pubDate>Tue, 09 Apr 2019 00:00:00 -0300</pubDate>
      
      <guid>/intermediate/270_custom_resource_definition/</guid>
      <description>Custom Resource Definition Introduction In this Chapter, we will review the Custom Resource Definition (CRD) concept, and some examples of usage.
In Kubernetes API, a resource is an endpoint storing the API objects in a collection. As an example, the pods resource contains a collection of Pod objects.
CRD’s are extensions of Kubernetes API that stores collection of API objects of certain kind. They extend the Kubernetes API or allow you to add your own API into the cluster.</description>
    </item>
    
    <item>
      <title>Observability with AWS Distro for Open Telemetry</title>
      <link>/advanced/350_opentelemetry/</link>
      <pubDate>Tue, 12 Oct 2021 13:38:20 +0000</pubDate>
      
      <guid>/advanced/350_opentelemetry/</guid>
      <description>Observability with AWS Distro for Open Telemetry Observability for containerized workloads is essential for business critical environments. In this module, we will configure an OpenTelemetry (OTEL) collector that gathers metrics and traces. With the OTEL collector configured with the following AWS Services: Amazon CloudWatch, Container Insights, Amazon Managed Service for Prometheus (AMP), Amazon Managed Grafana (AMG).</description>
    </item>
    
    <item>
      <title>Service Mesh with Istio</title>
      <link>/advanced/310_servicemesh_with_istio/</link>
      <pubDate>Tue, 13 Nov 2018 16:32:30 +0900</pubDate>
      
      <guid>/advanced/310_servicemesh_with_istio/</guid>
      <description>A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application.
Service mesh solutions have two distinct components that behave somewhat differently:
 The data planeis composed of a set of intelligent proxies (Envoy) deployed as sidecars. These proxies mediate and control all network communication between microservices along with Mixer, a general-purpose policy and telemetry hub.</description>
    </item>
    
    <item>
      <title>Service Mesh using AWS App Mesh</title>
      <link>/advanced/330_servicemesh_using_appmesh/</link>
      <pubDate>Mon, 27 Jan 2020 08:30:11 -0700</pubDate>
      
      <guid>/advanced/330_servicemesh_using_appmesh/</guid>
      <description>Meshifying with AWS App Mesh (Part 1)   Meshifying with AWS App Mesh (Part 2)   At re:invent 2018, we announced AWS App Mesh, a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. AWS App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications.
Service meshes like AWS App Mesh help you to run and monitor HTTP and TCP services at scale.</description>
    </item>
    
    <item>
      <title>Canary Deployment using Flagger in AWS App Mesh</title>
      <link>/advanced/340_appmesh_flagger/</link>
      <pubDate>Sun, 18 Nov 2018 00:00:00 -0500</pubDate>
      
      <guid>/advanced/340_appmesh_flagger/</guid>
      <description>Flagger is a progressive delivery tool that automates the release process for applications running on Kubernetes. Flagger can be configured to automate the release process for Kubernetes workloads with a custom resource named canary.
We saw in the Canary chapter of Service Mesh using App Mesh workshop on how to deploy new version of detail service by changing the VirtualRouter configuration. In this workshop, we will show you how to use Flagger in App Mesh to automate the canary deployment of backend service detail of our Product Catalog Application.</description>
    </item>
    
    <item>
      <title>Batch Processing with Argo Workflow</title>
      <link>/advanced/410_batch/</link>
      <pubDate>Sun, 18 Nov 2018 00:00:00 -0500</pubDate>
      
      <guid>/advanced/410_batch/</guid>
      <description>Batch Processing In this Chapter, we will deploy common batch processing scenarios using Kubernetes and Argo.
What is Argo? Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD (Custom Resource Definition).
 Define workflows where each step in the workflow is a container. Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a directed acyclic graph (DAG).</description>
    </item>
    
    <item>
      <title>Machine Learning using Kubeflow</title>
      <link>/advanced/420_kubeflow/</link>
      <pubDate>Fri, 15 Nov 2019 00:00:00 -0800</pubDate>
      
      <guid>/advanced/420_kubeflow/</guid>
      <description>Machine Learning using Kubeflow   Kubeflow provides a simple, portable, and scalable way of running Machine Learning workloads on Kubernetes.
In this module, we will install Kubeflow on Amazon EKS, run a single-node training and inference using TensorFlow, train and deploy model locally and remotely using Fairing, setup Kubeflow pipeline and review how to call AWS managed services such as Sagemaker for training and inference.</description>
    </item>
    
    <item>
      <title>EMR on EKS</title>
      <link>/advanced/430_emr_on_eks/</link>
      <pubDate>Mon, 15 Mar 2021 12:30:45 -0400</pubDate>
      
      <guid>/advanced/430_emr_on_eks/</guid>
      <description>EMR on EKS EMR on EKS is a deployment option in EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. There are several advantages of running optimized spark runtime provided by Amazon EMR on EKS such as 3x faster performance, fully managed lifecycle of these jobs, built-in monitoring and logging functionality, integrates securely with Kubernetes and more. Because Kubernetes can natively run Spark jobs, if you use multi-tenant EKS environment (shared with other micro-services), your spark jobs are deployed in seconds vs minutes when compared to EC2 based deployments.</description>
    </item>
    
  </channel>
</rss>
