[
{
	"uri": "/",
	"title": "Amazon EKS Workshop",
	"tags": [],
	"description": "",
	"content": " For even more container related content, check out our new show: Containers from the Couch\n In this workshop, we will explore multiple ways to configure VPC, ALB, and EC2 Kubernetes workers, and Amazon Elastic Kubernetes Service.\n"
},
{
	"uri": "/beginner/120_network-policies/calico-enterprise/register/",
	"title": "Registration - GET ACCCESS TO CALICO ENTERPRISE TRIAL",
	"tags": [],
	"description": "",
	"content": "The team at Tigera offers a free access to Calico Enterprise Edition for all EKSWorkshop users. Follow the steps below to get your free trial to access Calico Enterprise and run it directly on your EKS cluster.\n  Go to the Calico Enterprise trial registration website using this link .\n  Fill out your contact details along with your work email address to ensure you receive your trial environment details promptly.\n  Select Amazon EKS as your Kubernetes distro.\n  For the Promo Code, please use PAR-AWS-EKS-WORKSHOP. It is critical for you to use this code as it is used to automatically approve trial requests for EKS Workshop Users.\n  Review and accept the terms of service and click the START YOUR TRIAL button.\n  Somebody from the Tigera team will provision your environment and send you an email with the details you need. This is currently a manual process and can take up to half an hour during business hours M-F 9 am - 5 pm PT. Requests received during non-business hours will be provisioned on the next business day.\n  Once you receive the email titled \u0026ldquo;Your Calico Enterprise Trial Credentials\u0026rdquo; from cet@tigera.io ( make sure it\u0026rsquo;s not going to your spam folder). You will be supplied with a single command to run against your existing EKS cluster. This command will automatically install the required Calico Enterprise components on your EKS cluster so you will be able to use a dedicated management portal to manage this cluster.\n  You may follow the instructions provided in the email to register your EKS cluster and go through the advanced labs!\n  üêØ ‚Üí curl -s https://tigera-installer.storage.googleapis.com/XXXXXXXX--management_install.sh | bash [INFO] Checking for installed CNI Plugin [INFO] Deploying CRDs and Tigera Operator [INFO] Creating Tigera Pull Secret [INFO] Tigera Operator is Available [INFO] Adding Installation CR for Enterprise install [WAIT] Tigera calico is Progressing [INFO] Tigera Calico is Available [INFO] Deploying Tigera Prometheus Operator [INFO] Deploying CRs for Managed Cluster [WAIT] Tigera apiserver is Progressing [INFO] Tigera Apiserver is Available [INFO] Generate New Cluster Registration Manifest [INFO] Creating connection [INFO] All Tigera Components are Available [INFO] Securing Install Your Connected Cluster Name is XXXXXXXX-us-west-2-eks-amazonaws-com Your install was completed successfully. Below are your Calico Enterprise Credentials. PORTAL URL: https://XXXXX-management.try.tigera.io ACCESS TOKEN: XXXX KIBANA USERNAME: elastic KIBANA PASSWORD: XXXXXXXX Lab Environment Access Endpoint: https://XXXXXXX.try.tigera.io:31500 User: XXXXXXX Pass: XXXXXXX "
},
{
	"uri": "/beginner/300_windows/calico_windows/install_calico/",
	"title": "Deploy Calico on the Cluster",
	"tags": [],
	"description": "",
	"content": "Apply the Calico manifest from the aws/amazon-vpc-cni-k8s GitHub project. This creates the daemon sets in the kube-system namespace.\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.6/calico.yaml Let\u0026rsquo;s go over few key features of the Calico manifest:\n We see an annotation throughout; annotations are a way to attach non-identifying metadata to objects. This metadata is not used internally by Kubernetes, so they cannot be used to identify within k8s. Instead, they are used by external tools and libraries. Examples of annotations include build/release timestamps, client library information for debugging, or fields managed by a network policy like Calico in this case.  kind: DaemonSet apiVersion: apps/v1 metadata: name: calico-node namespace: kube-system labels: k8s-app: calico-node spec: selector: matchLabels: k8s-app: calico-node updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: k8s-app: calico-node annotations: # This, along with the CriticalAddonsOnly toleration below, # marks the pod as a critical add-on, ensuring it gets # priority scheduling and that its resources are reserved # if it ever gets evicted. *scheduler**.alpha.kubernetes.io/critical-pod: \u0026#39;\u0026#39;* ...  In contrast, Labels in Kubernetes are intended to be used to specify identifying attributes for objects. They are used by selector queries or with label selectors. Since they are used internally by Kubernetes the structure of keys and values is constrained, to optimize queries.\nWe see that the manifest has a tolerations attribute. Taints and tolerations work together to ensure pods are not scheduled onto inappropriate nodes. Taints are applied to nodes, and the only pods that can tolerate the taint are allowed to run on those nodes.  A taint consists of a key, a value for it and an effect, which can be:\n PreferNoSchedule: Prefer not to schedule intolerant pods to the tainted node NoSchedule: Do not schedule intolerant pods to the tainted node NoExecute: In addition to not scheduling, also evict intolerant pods that are already running on the node.  Like taints, tolerations also have a key value pair and an effect, with the addition of operator. Here in the Calico manifest, we see tolerations has just one attribute: Operator = exists. This means the key value pair is omitted and the toleration will match any taint, ensuring it runs on all nodes.\ntolerations: - operator: Exists  Watch the kube-system daemon sets and wait for the calico-node daemon set to have the DESIRED number of pods in the READY state.\nkubectl get daemonset calico-node --namespace=kube-system Expected Output:\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE calico-node 3 3 3 3 3 \u0026lt;none\u0026gt; 38s  For linux OS containers, this would be all we need to install Calico; however, for Windows we need to add a rolebinding to allow the windows nodes to install Calico.\nCreate the user-rolebinding.yaml file\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/environment/windows/user-rolebinding.yaml --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: nodes-cluster-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:nodes EOF Apply the user role binding to the cluster\nkubectl apply -f ~/environment/windows/user-rolebinding.yaml Calico is now ready to use in the cluster!\n"
},
{
	"uri": "/beginner/120_network-policies/calico/install_calico/",
	"title": "Install Calico",
	"tags": [],
	"description": "",
	"content": "Apply the Calico manifest from the aws/amazon-vpc-cni-k8s GitHub project. This creates the daemon sets in the kube-system namespace.\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.6/calico.yaml Let\u0026rsquo;s go over few key features of the Calico manifest:\n We see an annotation throughout; annotations are a way to attach non-identifying metadata to objects. This metadata is not used internally by Kubernetes, so they cannot be used to identify within k8s. Instead, they are used by external tools and libraries. Examples of annotations include build/release timestamps, client library information for debugging, or fields managed by a network policy like Calico in this case.  kind: DaemonSet apiVersion: apps/v1 metadata: name: calico-node namespace: kube-system labels: k8s-app: calico-node spec: selector: matchLabels: k8s-app: calico-node updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: k8s-app: calico-node annotations: # This, along with the CriticalAddonsOnly toleration below, # marks the pod as a critical add-on, ensuring it gets # priority scheduling and that its resources are reserved # if it ever gets evicted. *scheduler**.alpha.kubernetes.io/critical-pod: \u0026#39;\u0026#39;* ...  In contrast, Labels in Kubernetes are intended to be used to specify identifying attributes for objects. They are used by selector queries or with label selectors. Since they are used internally by Kubernetes the structure of keys and values is constrained, to optimize queries.\nWe see that the manifest has a tolerations attribute. Taints and tolerations work together to ensure pods are not scheduled onto inappropriate nodes. Taints are applied to nodes, and the only pods that can tolerate the taint are allowed to run on those nodes.  A taint consists of a key, a value for it and an effect, which can be:\n PreferNoSchedule: Prefer not to schedule intolerant pods to the tainted node NoSchedule: Do not schedule intolerant pods to the tainted node NoExecute: In addition to not scheduling, also evict intolerant pods that are already running on the node.  Like taints, tolerations also have a key value pair and an effect, with the addition of operator. Here in the Calico manifest, we see tolerations has just one attribute: Operator = exists. This means the key value pair is omitted and the toleration will match any taint, ensuring it runs on all nodes.\ntolerations: - operator: Exists  Watch the kube-system daemon sets and wait for the calico-node daemon set to have the DESIRED number of pods in the READY state.\nkubectl get daemonset calico-node --namespace=kube-system Expected Output:\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE calico-node 3 3 3 3 3 \u0026lt;none\u0026gt; 38s  "
},
{
	"uri": "/beginner/120_network-policies/calico/stars_policy_demo/create_resources/",
	"title": "Create Resources",
	"tags": [],
	"description": "",
	"content": "Before creating network polices, let\u0026rsquo;s create the required resources.\nCreate a new folder for the configuration files.\nmkdir ~/environment/calico_resources cd ~/environment/calico_resources Stars Namespace Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/beginner/120_network-policies/calico/stars_policy_demo/create_resources.files/namespace.yaml Let\u0026rsquo;s examine our file by running cat namespace.yaml.\nkind: Namespace apiVersion: v1 metadata: name: stars  Create a namespace called stars:\nkubectl apply -f namespace.yaml We will create frontend and backend deployments and services in this namespace in later steps.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/beginner/120_network-policies/calico/stars_policy_demo/create_resources.files/management-ui.yaml wget https://eksworkshop.com/beginner/120_network-policies/calico/stars_policy_demo/create_resources.files/backend.yaml wget https://eksworkshop.com/beginner/120_network-policies/calico/stars_policy_demo/create_resources.files/frontend.yaml wget https://eksworkshop.com/beginner/120_network-policies/calico/stars_policy_demo/create_resources.files/client.yaml cat management-ui.yaml apiVersion: v1 kind: Namespace metadata: name: management-ui labels: role: management-ui --- apiVersion: v1 kind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 80 targetPort: 9001 selector: role: management-ui --- apiVersion: apps/v1 kind: Deployment metadata: name: management-ui namespace: management-ui spec: replicas: 1 selector: matchLabels: role: management-ui template: metadata: labels: role: management-ui spec: containers: - name: management-ui image: calico/star-collect:v0.1.0 imagePullPolicy: Always ports: - containerPort: 9001  Create a management-ui namespace, with a management-ui service and deployment within that namespace:\nkubectl apply -f management-ui.yaml cat backend.yaml to see how the backend service is built:\napiVersion: v1 kind: Service metadata: name: backend namespace: stars spec: ports: - port: 6379 targetPort: 6379 selector: role: backend --- apiVersion: apps/v1 kind: Deployment metadata: name: backend namespace: stars spec: replicas: 1 selector: matchLabels: role: backend template: metadata: labels: role: backend spec: containers: - name: backend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=6379 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 6379  Let\u0026rsquo;s examine the frontend service with cat frontend.yaml:\napiVersion: v1 kind: Service metadata: name: frontend namespace: stars spec: ports: - port: 80 targetPort: 80 selector: role: frontend --- apiVersion: apps/v1 kind: Deployment metadata: name: frontend namespace: stars spec: replicas: 1 selector: matchLabels: role: frontend template: metadata: labels: role: frontend spec: containers: - name: frontend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=80 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 80  Create frontend and backend deployments and services within the stars namespace:\nkubectl apply -f backend.yaml kubectl apply -f frontend.yaml Lastly, let\u0026rsquo;s examine how the client namespace, and a client service for a deployment are built. cat client.yaml:\nkind: Namespace apiVersion: v1 metadata: name: client labels: role: client --- apiVersion: apps/v1 kind: Deployment metadata: name: client namespace: client spec: replicas: 1 selector: matchLabels: role: client template: metadata: labels: role: client spec: containers: - name: client image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status ports: - containerPort: 9000 --- apiVersion: v1 kind: Service metadata: name: client namespace: client spec: ports: - port: 9000 targetPort: 9000 selector: role: client  Apply the client configuraiton.\nkubectl apply -f client.yaml Check their status, and wait for all the pods to reach the Running status:\nkubectl get pods --all-namespaces Your output should look like this:\nNAMESPACE NAME READY STATUS RESTARTS AGE client client-nkcfg 1/1 Running 0 24m kube-system aws-node-6kqmw 1/1 Running 0 50m kube-system aws-node-grstb 1/1 Running 1 50m kube-system aws-node-m7jg8 1/1 Running 1 50m kube-system calico-node-b5b7j 1/1 Running 0 28m kube-system calico-node-dw694 1/1 Running 0 28m kube-system calico-node-vtz9k 1/1 Running 0 28m kube-system calico-typha-75667d89cb-4q4zx 1/1 Running 0 28m kube-system calico-typha-horizontal-autoscaler-78f747b679-kzzwq 1/1 Running 0 28m kube-system kube-dns-7cc87d595-bd9hq 3/3 Running 0 1h kube-system kube-proxy-lp4vw 1/1 Running 0 50m kube-system kube-proxy-rfljb 1/1 Running 0 50m kube-system kube-proxy-wzlqg 1/1 Running 0 50m management-ui management-ui-wzvz4 1/1 Running 0 24m stars backend-tkjrx 1/1 Running 0 24m stars frontend-q4r84 1/1 Running 0 24m  It may take several minutes to download all the required Docker images.\n To summarize the different resources we created:\n A namespace called stars frontend and backend deployments and services within stars namespace A namespace called management-ui Deployment and service management-ui for the user interface seen on the browser, in the management-ui namespace A namespace called client client deployment and service in client namespace  "
},
{
	"uri": "/020_prerequisites/self_paced/account/",
	"title": "Create an AWS account",
	"tags": [],
	"description": "",
	"content": " Your account must have the ability to create new IAM roles and scope other IAM permissions.\n   If you don\u0026rsquo;t already have an AWS account with Administrator access: create one now by clicking here\n  Once you have an AWS account, ensure you are following the remaining workshop steps as an IAM user with administrator access to the AWS account: Create a new IAM user to use for the workshop\n  Enter the user details:   Attach the AdministratorAccess IAM Policy:   Click to create the new user:   Take note of the login URL and save:   "
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/gettingstarted/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "After you\u0026rsquo;ve completed the prerequisites and Helm is installed and working; We can deploy our Wordpress site. This Helm chart will deploy MariaDB and Wordpress as well as configure a service ingress point for us to access the site through an elastic load balancer.\nFor our testing we‚Äôll be deploying Wordpress. We could just use a PHP file on the nodes and run NGINX to test as well, but with this Wordpress install you get experience deploying a Helm chart. And can use the load testing tool to hit various URLs on the Wordpress structure to generate additional network traffic load with multiple concurrent connections.\nWe\u0026rsquo;ll be using the following tools in this lab:  Helm: to install Wordpress on our cluster. CloudWatch Container Insights: to collect logs and metrics from our cluster. Siege: to load test our Wordpress and EKS Cluster. CloudWatch Container Insights Dashboard: to visualize our container performance and load. CloudWatch Metrics: to set an alarm for when our WordPress Pod is under heavy load.  Lets get started! "
},
{
	"uri": "/910_conclusion/conclusion/",
	"title": "What Have We Accomplished",
	"tags": [],
	"description": "",
	"content": "We have:\n Deployed an application consisting of microservices Deployed the Kubernetes Dashboard Deployed packages using Helm Deployed a centralized logging infrastructure Configured Automatic scaling of our pods and worker nodes  "
},
{
	"uri": "/beginner/300_windows/calico_windows/install_calico_node/",
	"title": "Install Calico on the Windows node",
	"tags": [],
	"description": "",
	"content": "In this module, we will install Calico on the windows node.\nList Amazon EKS cluster nodes and find the windows node.\nkubectl get nodes -l kubernetes.io/os=windows -L kubernetes.io/os NAME STATUS ROLES AGE VERSION OS ip-192-168-5-189.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 25m v1.17.12-eks-7684af windows  SSH into nodes using SSM via the AWS Console by finding the windows node in the list of running EC2 instances, select the instance named \u0026ldquo;eksworkshop-eksctl-windows-ng-Node\u0026rdquo; and click Connect\nThen select \u0026lsquo;Session Manager\u0026rsquo; and connect\nThis will open a new browser tab with a powershell prompt on the Node. Now we will use that Powershell window to install kubectl on the node\nmkdir c:\\k Invoke-WebRequest https://amazon-eks.s3.us-west-2.amazonaws.com/1.17.12/2020-11-02/bin/windows/amd64/kubectl.exe -OutFile c:\\k\\kubectl.exe $ENV:PATH += \u0026quot;;C:\\k\u0026quot; Now we will download and install Calico\nInvoke-WebRequest https://docs.projectcalico.org/scripts/install-calico-windows.ps1 -OutFile c:\\install-calico-windows.ps1 C:/install-calico-windows.ps1 -ServiceCidr 10.100.0.0/16 -DNSServerIPs 10.100.0.10 Verify the install\nGet-Service -Name Calico*, kube* We should get the following output:\nStatus Name DisplayName ------ ---- ------------ Running CalicoNode Calico Windows Startup Running CalicoFelix Calico Windows Agent Running kubelet kubelet service Running kube-proxy kube-proxy service  Now we have Calcio fully installed. Close the EC2 Connect window and return to Cloud9.\n"
},
{
	"uri": "/beginner/120_network-policies/calico/stars_policy_demo/default_policy/",
	"title": "Default Pod-to-Pod Communication",
	"tags": [],
	"description": "",
	"content": "In Kubernetes, the pods by default can communicate with other pods, regardless of which host they land on. Every pod gets its own IP address so you do not need to explicitly create links between pods. This is demonstrated by the management-ui.\nkind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 80 targetPort: 9001  To open the Management UI, retrieve the DNS name of the Management UI using:\nkubectl get svc -o wide -n management-ui Copy the EXTERNAL-IP from the output, and paste into a browser. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com‚Äù - the full value is the DNS address.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR management-ui LoadBalancer 10.100.239.7 a8b8c5f77eda911e8b1a60ea5d5305a4-720629306.us-east-1.elb.amazonaws.com 80:31919/TCP 9s role=management-ui  The UI here shows the default behavior, of all services being able to reach each other.\n"
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/installwordpress/",
	"title": "Install WordPress",
	"tags": [],
	"description": "",
	"content": "We\u0026rsquo;ll be using the bitnami charts repository to install WordPress to our EKS cluster.\n In your Cloud9 Workspace terminal you just need to run the following commands to deploy WordPress and its database.\n# Create a namespace wordpress kubectl create namespace wordpress-cwi # Add the bitnami Helm Charts Repository helm repo add bitnami https://charts.bitnami.com/bitnami # Deploy WordPress in its own namespace helm -n wordpress-cwi install understood-zebu bitnami/wordpress This chart will create:\n Two persistent volumes claims.. Multiple secrets. One StatefulSet for MariaDB. One Deployment for Wordpress.  You can follow the status of the deployment with this command\nkubectl -n wordpress-cwi rollout status deployment understood-zebu-wordpress "
},
{
	"uri": "/beginner/300_windows/calico_windows/test_network_policy/",
	"title": "Test Network Policies",
	"tags": [],
	"description": "",
	"content": "We to define the Windows and Linux deployments by running the following command.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/environment/windows/sample-deployments.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: windows spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 securityContext: privileged: true nodeSelector: beta.kubernetes.io/os: linux EOF We can now deploy those pods in our cluster.\nkubectl apply -f ~/environment/windows/sample-deployments.yaml Verify that the pods are in ‚ÄòRunning‚Äô state.\nkubectl get pods -o wide --watch -n windows Output:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-54b55f86cd-7x6jm 1/1 Running 0 19s 192.168.177.232 ip-192-168-168-60.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; windows-server-iis-7cff879775-zmrj6 1/1 Running 0 63m 192.168.18.152 ip-192-168-5-189.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Test ping connectivity between pods\nWe would start by verifying that there is network connectivity among all pods. Inside each pod, ping the other two pods‚Äô IP. Update the below with the pod name that you see in the cluster.\nkubectl -n windows exec -it \u0026lt;nginx-pod-name\u0026gt; -- /bin/bash Now ping the IP of the Windows pod\nping \u0026lt;windows-pod-ip\u0026gt; Output: (Note: Control + C to stop the ping test):\nroot@nginx-deployment-54b55f86cd-7x6jm:/# ping 192.168.18.152 PING 192.168.18.152 (192.168.18.152): 48 data bytes 56 bytes from 192.168.18.152: icmp_seq=0 ttl=127 time=1.480 ms 56 bytes from 192.168.18.152: icmp_seq=1 ttl=127 time=1.599 ms 56 bytes from 192.168.18.152: icmp_seq=2 ttl=127 time=1.481 ms 56 bytes from 192.168.18.152: icmp_seq=3 ttl=127 time=6.054 ms ^C--- 192.168.18.152 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max/stddev = 1.480/2.654/6.054/1.964 ms Now we will exit the Nginx pod by typing \u0026ldquo;exit\u0026rdquo; and pressing enter\nSimilarly, ‚Äòexec‚Äô into one of the Windows pods as well. Update the below with the pod names that you see in the cluster.\nkubectl -n windows exec -it \u0026lt;windows-pod-name\u0026gt; -- powershell Now ping the IP of the Nginx pod\nping \u0026lt;nginx-pod-ip\u0026gt; Output:\nPS C:\\\u0026gt; ping 192.168.177.232 Pinging 192.168.177.232 with 32 bytes of data: Reply from 192.168.177.232: bytes=32 time=1ms TTL=254 Reply from 192.168.177.232: bytes=32 time=1ms TTL=254 Reply from 192.168.177.232: bytes=32 time=8ms TTL=254 Reply from 192.168.177.232: bytes=32 time=1ms TTL=254 Ping statistics for 192.168.177.232: Packets: Sent = 4, Received = 4, Lost = 0 (0% loss), Approximate round trip times in milli-seconds: Minimum = 1ms, Maximum = 8ms, Average = 2ms PS C:\\\u0026gt; exit Exit the Windows pod by typing \u0026ldquo;exit\u0026rdquo; and pressing enter.\nNow that we have established basic connectivity accross pods, let\u0026rsquo;s enforce a network policy to restrict ping connectivity.\nWe could use Kubectl to apply the the network policies but Calico has a CLI that offers policy validation and will protect the cluster from malformed policies.\nInstall the Calicoctl pod and create an alias to access this functionality\nkubectl apply -f https://docs.projectcalico.org/archive/v3.15/manifests/calicoctl.yaml alias calicoctl=\u0026quot;kubectl exec -i -n kube-system calicoctl -- /calicoctl\u0026quot; Create and apply the network policy specification to deny ping traffic to all pods.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/environment/windows/deny_icmp.yaml --- kind: GlobalNetworkPolicy apiVersion: projectcalico.org/v3 metadata: name: block-icmp spec: order: 200 selector: all() types: - Ingress - Egress ingress: - action: Deny protocol: ICMP - action: Deny protocol: ICMPv6 egress: - action: Deny protocol: ICMP - action: Deny protocol: ICMPv6 EOF calicoctl apply -f - \u0026lt; ~/environment/windows/deny_icmp.yaml Now we will test ping connectivity between pods just like we did above.\nWe can exec into the and when we try to ping the windows machine we will see 100% packet loss.\nkubectl exec -it \u0026lt;nginx-pod-name\u0026gt; /bin/bash Now ping the IP of the Windows pod\nping \u0026lt;windows-pod-ip\u0026gt; Output:\nroot@nginx-deployment-54b55f86cd-jr662:/# ping 192.168.18.1 PING 192.168.18.1 (192.168.18.1): 48 data bytes ^C--- 192.168.18.1 ping statistics --- 24 packets transmitted, 0 packets received, 100% packet loss  As you can see we are no longer able to ping the windows pod. Now we will exit the Nginx pod by typing \u0026ldquo;exit\u0026rdquo; and pressing enter.\nIf you want you could exec into the pod and you will also be blocked from pinging the nginx pod.\nThat is it! You have configured your Windows worker nodes with Open source Calico for Windows and testing using the network policies.\n"
},
{
	"uri": "/beginner/120_network-policies/calico/stars_policy_demo/apply_network_policies/",
	"title": "Apply Network Policies",
	"tags": [],
	"description": "",
	"content": "In a production level cluster, it is not secure to have open pod to pod communication. Let\u0026rsquo;s see how we can isolate the services from each other.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/beginner/120_network-policies/calico/stars_policy_demo/apply_network_policies.files/default-deny.yaml Let\u0026rsquo;s examine our file by running cat default-deny.yaml.\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny spec: podSelector: matchLabels: {}  Let\u0026rsquo;s go over the network policy. Here we see the podSelector does not have any matchLabels, essentially blocking all the pods from accessing it.\nApply the network policy in the stars namespace (frontend and backend services) and the client namespace (client service):\nkubectl apply -n stars -f default-deny.yaml kubectl apply -n client -f default-deny.yaml Upon refreshing your browser, you see that the management UI cannot reach any of the nodes, so nothing shows up in the UI.\nNetwork policies in Kubernetes use labels to select pods, and define rules on what traffic is allowed to reach those pods. They may specify ingress or egress or both. Each rule allows traffic which matches both the from and ports sections.\nCreate two new network policies.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/beginner/120_network-policies/calico/stars_policy_demo/apply_network_policies.files/allow-ui.yaml wget https://eksworkshop.com/beginner/120_network-policies/calico/stars_policy_demo/apply_network_policies.files/allow-ui-client.yaml Again, we can examine our file contents by running: cat allow-ui.yaml\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui  cat allow-ui-client.yaml kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: client name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui  Challenge: How do we apply our network policies to allow the traffic we want?\n  Expand here to see the solution   kubectl apply -f allow-ui.yaml kubectl apply -f allow-ui-client.yaml    Upon refreshing your browser, you can see that the management UI can reach all the services, but they cannot communicate with each other.\n"
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/accesswp/",
	"title": "Accessing Wordpress",
	"tags": [],
	"description": "",
	"content": "Testing public URL It may take a few minutes for the LoadBalancer to be available.\n You‚Äôll need the URL for your WordPress site. This is easily accomplished by running the command below from your terminal window.\nexport SERVICE_URL=$(kubectl get svc -n wordpress-cwi understood-zebu-wordpress --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\u0026#34;) echo \u0026#34;Public URL: http://$SERVICE_URL/\u0026#34; You should see the Hello World WordPress welcome page. Testing the admin interface export ADMIN_URL=\u0026#34;http://$SERVICE_URL/admin\u0026#34; export ADMIN_PASSWORD=$(kubectl get secret --namespace wordpress-cwi understood-zebu-wordpress -o jsonpath=\u0026#34;{.data.wordpress-password}\u0026#34; | base64 --decode) echo \u0026#34;Admin URL: http://$SERVICE_URL/admin Username: user Password: $ADMIN_PASSWORD\u0026#34; In your favorite browser paste in your Wordpress Admin URL from the Installing Wordpress section. You should be greeted with the following screen. Enter your username and password to make sure they work.\nIf you are taken to the below screen, you have a successfully running Wordpress install backed by MaiaDB in your EKS Cluster.\nNow that we have verified that the site is working we can continue with getting CloudWatch Container Insights installed on our cluster!\n"
},
{
	"uri": "/beginner/170_statefulset/ebs_csi_driver/",
	"title": "Amazon EBS CSI Driver",
	"tags": [],
	"description": "",
	"content": "About Container Storage Interface (CSI) The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage systems to containerized workloads on Container Orchestration Systems (COs) like Kubernetes.\nBy using CSI, third-party storage providers can write and deploy plugins exposing new storage systems in Kubernetes without ever having to touch the core Kubernetes code.\nAbout the Amazon EBS CSI Driver The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver provides a CSI interface that allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes.\nThis topic shows you how to deploy the Amazon EBS CSI Driver to your Amazon EKS cluster and verify that it works.\nConfigure IAM Policy The CSI driver is deployed as a set of Kubernetes Pods. These Pods must have permission to perform EBS API operations, such as creating and deleting volumes, and attaching volumes to the EC2 worker nodes that comprise the cluster.\nFirst, let\u0026rsquo;s download the policy JSON document, and create an IAM Policy from it:\nexport EBS_CSI_POLICY_NAME=\u0026#34;Amazon_EBS_CSI_Driver\u0026#34; mkdir ${HOME}/environment/ebs_statefulset cd ${HOME}/environment/ebs_statefulset # download the IAM policy document curl -sSL -o ebs-csi-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json # Create the IAM policy aws iam create-policy \\  --region ${AWS_REGION} \\  --policy-name ${EBS_CSI_POLICY_NAME} \\  --policy-document file://${HOME}/environment/ebs_statefulset/ebs-csi-policy.json # export the policy ARN as a variable export EBS_CSI_POLICY_ARN=$(aws --region ${AWS_REGION} iam list-policies --query \u0026#39;Policies[?PolicyName==`\u0026#39;$EBS_CSI_POLICY_NAME\u0026#39;`].Arn\u0026#39; --output text) Configure IAM Role for Service Account You can associate an IAM role with a Kubernetes service account. This service account can then provide AWS permissions to the containers in any pod that uses that service account. With this feature, you no longer need to provide extended permissions to the Amazon EKS node IAM role so that pods on that node can call AWS APIs.\nWe\u0026rsquo;ll ask eksctl to create an IAM Role that contains the IAM Policy we just created, and associate it with a Kubernetes Service Account called ebs-csi-controller-irsa that will be used by the CSI Driver:\n# Create an IAM OIDC provider for your cluster eksctl utils associate-iam-oidc-provider \\  --region=$AWS_REGION \\  --cluster=eksworkshop-eksctl \\  --approve # Create a service account eksctl create iamserviceaccount \\  --cluster eksworkshop-eksctl \\  --name ebs-csi-controller-irsa \\  --namespace kube-system \\  --attach-policy-arn $EBS_CSI_POLICY_ARN \\  --override-existing-serviceaccounts \\  --approve Deploy the Amazon EBS CSI Driver Finally, we can deploy the driver using helm.\nIf Helm is not installed, click here the instruction\n # add the aws-ebs-csi-driver as a helm repo helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver # search for the driver helm search repo aws-ebs-csi-driver output NAME CHART VERSION APP VERSION DESCRIPTION aws-ebs-csi-driver/aws-ebs-csi-driver 2.0.2 1.1.3 A Helm chart for AWS EBS CSI Driver  helm upgrade --install aws-ebs-csi-driver \\  --version=1.2.4 \\  --namespace kube-system \\  --set serviceAccount.controller.create=false \\  --set serviceAccount.snapshot.create=false \\  --set enableVolumeScheduling=true \\  --set enableVolumeResizing=true \\  --set enableVolumeSnapshot=true \\  --set serviceAccount.snapshot.name=ebs-csi-controller-irsa \\  --set serviceAccount.controller.name=ebs-csi-controller-irsa \\  aws-ebs-csi-driver/aws-ebs-csi-driver kubectl -n kube-system rollout status deployment ebs-csi-controller Output\nWaiting for deployment \u0026#34;ebs-csi-controller\u0026#34; rollout to finish: 0 of 2 updated replicas are available... Waiting for deployment \u0026#34;ebs-csi-controller\u0026#34; rollout to finish: 1 of 2 updated replicas are available... deployment \u0026#34;ebs-csi-controller\u0026#34; successfully rolled out  "
},
{
	"uri": "/beginner/120_network-policies/calico/stars_policy_demo/directional_traffic/",
	"title": "Allow Directional Traffic",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s see how we can allow directional traffic from client to frontend, and from frontend to backend.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/beginner/120_network-policies/calico/stars_policy_demo/directional_traffic.files/backend-policy.yaml wget https://eksworkshop.com/beginner/120_network-policies/calico/stars_policy_demo/directional_traffic.files/frontend-policy.yaml Let\u0026rsquo;s examine this backend policy with cat backend-policy.yaml: kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELIST FRONTEND USING PODSELECTOR\u0026gt; ports: - protocol: TCP port: 6379  Challenge: After reviewing the manifest, you\u0026rsquo;ll see we have intentionally left few of the configuration fields for you to EDIT. Please edit the configuration as suggested. You can find helpful info in this Kubernetes documentation\n  Expand here to see the solution   kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379    Let\u0026rsquo;s examine the frontend policy with cat frontend-policy.yaml:\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELIST CLIENT USING NAMESPACESELECTOR\u0026gt; ports: - protocol: TCP port: 80  Challenge: Please edit the configuration as suggested. You can find helpful info in this Kubernetes documentation\n   Expand here to see the solution   kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80    To allow traffic from frontend service to the backend service apply the following manifest:\nkubectl apply -f backend-policy.yaml And allow traffic from the client namespace to the frontend service:\nkubectl apply -f frontend-policy.yaml Upon refreshing your browser, you should be able to see the network policies in action:\nLet\u0026rsquo;s have a look at the backend-policy. Its spec has a podSelector that selects all pods with the label role:backend, and allows ingress from all pods that have the label role:frontend and on TCP port 6379, but not the other way round. Traffic is allowed in one direction on a specific port number.\nspec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379  The frontend-policy is similar, except it allows ingress from namespaces that have the label role: client on TCP port 80.\nspec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80  "
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/cwcinstallprep/",
	"title": "Preparing to Install Container Insights",
	"tags": [],
	"description": "",
	"content": " The full documentation for CloudWatch Container Insights can be found here.\n Add the necessary policy to the IAM role for your worker nodes In order for CloudWatch to get the necessary monitoring info, we need to install the CloudWatch Agent to our EKS Cluster.\nFirst, we will need to ensure the Role Name our workers use is set in our environment:\ntest -n \u0026#34;$ROLE_NAME\u0026#34; \u0026amp;\u0026amp; echo ROLE_NAME is \u0026#34;$ROLE_NAME\u0026#34; || echo ROLE_NAME is not set  If ROLE_NAME is not set, please review the test the cluster section.\n We will attach the policy to the nodes IAM Role:\naws iam attach-role-policy \\  --role-name $ROLE_NAME \\  --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy Finally, let\u0026rsquo;s verify that the policy has been attached to the IAM ROLE:\naws iam list-attached-role-policies --role-name $ROLE_NAME | grep CloudWatchAgentServerPolicy || echo \u0026#39;Policy not found\u0026#39; Output \u0026#34;PolicyName\u0026#34;: \u0026#34;CloudWatchAgentServerPolicy\u0026#34;, \u0026#34;PolicyArn\u0026#34;: \u0026#34;arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\u0026#34;  Now we can proceed to the actual install of the CloudWatch Insights.\n"
},
{
	"uri": "/beginner/080_scaling/install_kube_ops_view/",
	"title": "Install Kube-ops-view",
	"tags": [],
	"description": "",
	"content": "Before starting to learn about the various auto-scaling options for your EKS cluster we are going to install Kube-ops-view from Henning Jacobs.\nKube-ops-view provides a common operational picture for a Kubernetes cluster that helps with understanding our cluster setup in a visual way.\nWe will deploy kube-ops-view using Helm configured in a previous module\n The following line updates the stable helm repository and then installs kube-ops-view using a LoadBalancer Service type and creating a RBAC (Resource Base Access Control) entry for the read-only service account to read nodes and pods information from the cluster.\nhelm install kube-ops-view \\ stable/kube-ops-view \\ --set service.type=LoadBalancer \\ --set rbac.create=True The execution above installs kube-ops-view exposing it through a Service using the LoadBalancer type. A successful execution of the command will display the set of resources created and will prompt some advice asking you to use kubectl proxy and a local URL for the service. Given we are using the type LoadBalancer for our service, we can disregard this; Instead we will point our browser to the external load balancer.\nMonitoring and visualization shouldn\u0026rsquo;t be typically be exposed publicly unless the service is properly secured and provide methods for authentication and authorization. You can still deploy kube-ops-view using a Service of type ClusterIP by removing the --set service.type=LoadBalancer section and using kubectl proxy. Kube-ops-view does also support Oauth 2\n To check the chart was installed successfully:\nhelm list should display :\nNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE kube-ops-view 1 Sun Sep 22 11:47:31 2019 DEPLOYED kube-ops-view-1.1.0 0.11 default With this we can explore kube-ops-view output by checking the details about the newly service created.\nkubectl get svc kube-ops-view | tail -n 1 | awk '{ print \u0026quot;Kube-ops-view URL = http://\u0026quot;$4 }' This will display a line similar to Kube-ops-view URL = http://\u0026lt;URL_PREFIX_ELB\u0026gt;.amazonaws.com Opening the URL in your browser will provide the current state of our cluster.\nYou may need to refresh the page and clean your browser cache. The creation and setup of the LoadBalancer may take a few minutes; usually in two minutes you should see kub-ops-view.\n As this workshop moves along and you perform scale up and down actions, you can check the effects and changes in the cluster using kube-ops-view. Check out the different components and see how they map to the concepts that we have already covered during this workshop.\nSpend some time checking the state and properties of your EKS cluster.\n "
},
{
	"uri": "/beginner/191_secrets/kms-custom-keystore/",
	"title": "AWS KMS and Custom Key Store",
	"tags": [],
	"description": "",
	"content": "Considerations for your AWS KMS CMK Before we get to the lab exercise, we wanted to take some time to discuss options for generating your AWS KMS CMK. AWS KMS provides you with two alternatives to store your CMK. Your security requirements may dictate which alternative is suitable for your workloads on Amazon EKS.\nThere is an AWS Online Tech Talk on Encrypting Secrets in Amazon EKS that dives deep into this topic.\n   Custom Key Store (CMK stored within AWS CloudHSM) For most users, the default AWS KMS key store, which is protected by FIPS 140-2 validated cryptographic modules, fulfills their security requirements.\nHowever, you might consider creating a custom key store if your organization has any of the following requirements:\n The key material cannot be stored in a shared environment. The key material must be backed up in multiple AWS Regions. The key material must be subject to a secondary, independent audit path. The hardware security module (HSM) that generates and stores key material must be certified at FIPS 140-2 Level 3.  If any of these requirements apply to you, consider using AWS CloudHSM with AWS KMS to create a custom key store.\nChallenge What level of FIPS 140-2 cryptographic validation does the AWS KMS HSM hold?\n  Expand here to see the solution   The AWS KMS HSMs are validated at Level 2 overall. You can read more about that [here].(https://aws.amazon.com/blogs/security/aws-key-management-service-now-offers-fips-140-2-validated-cryptographic-modules-enabling-easier-adoption-of-the-service-for-regulated-workloads/)\nFIPS 140-2 Level 2 validation is sufficient for many use cases, but check with your security and compliance teams to verify.\n  Keep in mind that the KMS Custom Key Store functionality makes use of a minimum of two AWS CloudHSM instances.\n Cost Aside from compliance considerations, your team will want to consider the cost of using this feature. For comparison, I will list the cost of using a CMK created with the default KMS functionality. Then, I will list of the cost of using a CMK created with the custom key store functionality.\nKMS Standard (Monthly Cost)  1 CMK = $1.00 90 requests = $0.00 (due to the free tier of 20,000 requests) Total Cost = $1.00  KMS Custom Key Store (Monthly Cost)  1 CMK = $1.00 90 requests = $0.00 (due to the free tier of 20,000 requests) 2 CloudHSM Instances = $2,380.80 Total Cost = $2,381.80  Now that we have discussed AWS KMS support for custom key stores, let\u0026rsquo;s move on to the exercise.\n"
},
{
	"uri": "/intermediate/240_monitoring/prereqs/",
	"title": "Prereqs",
	"tags": [],
	"description": "",
	"content": "Is helm installed? We will use helm to install Prometheus \u0026amp; Grafana monitoring tools for this chapter. Please review installing helm chapter for instructions if you don\u0026rsquo;t have it installed.\n# add prometheus Helm repo helm repo add prometheus-community https://prometheus-community.github.io/helm-charts # add grafana Helm repo helm repo add grafana https://grafana.github.io/helm-charts "
},
{
	"uri": "/intermediate/260_weave_flux/prereqs/",
	"title": "Prereqs",
	"tags": [],
	"description": "",
	"content": "Is helm installed? We will use helm to install Weave Flux and a sample Helm chart. Check to see if helm is installed:\nhelm version If helm is not found, see installing helm for instructions.\nAWS CodePipeline and AWS CodeBuild both need AWS Identity and Access Management (IAM) service roles to create a Docker image build pipeline.\nIn this step, we are going to create an IAM role and add an inline policy that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl.\nCreate the bucket and roles:\n# Use your account number below ACCOUNT_ID=$(aws sts get-caller-identity | jq -r '.Account') aws s3 mb s3://eksworkshop-${ACCOUNT_ID}-codepipeline-artifacts cd ~/environment wget https://eksworkshop.com/intermediate/260_weave_flux/iam.files/cpAssumeRolePolicyDocument.json aws iam create-role --role-name eksworkshop-CodePipelineServiceRole --assume-role-policy-document file://cpAssumeRolePolicyDocument.json wget https://eksworkshop.com/intermediate/260_weave_flux/iam.files/cpPolicyDocument.json aws iam put-role-policy --role-name eksworkshop-CodePipelineServiceRole --policy-name codepipeline-access --policy-document file://cpPolicyDocument.json wget https://eksworkshop.com/intermediate/260_weave_flux/iam.files/cbAssumeRolePolicyDocument.json aws iam create-role --role-name eksworkshop-CodeBuildServiceRole --assume-role-policy-document file://cbAssumeRolePolicyDocument.json wget https://eksworkshop.com/intermediate/260_weave_flux/iam.files/cbPolicyDocument.json aws iam put-role-policy --role-name eksworkshop-CodeBuildServiceRole --policy-name codebuild-access --policy-document file://cbPolicyDocument.json "
},
{
	"uri": "/beginner/050_deploy/applications/",
	"title": "Deploy our Sample Applications",
	"tags": [],
	"description": "",
	"content": "apiVersion: apps/v1 kind: Deployment metadata: name: ecsdemo-nodejs labels: app: ecsdemo-nodejs namespace: default spec: replicas: 1 selector: matchLabels: app: ecsdemo-nodejs strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: ecsdemo-nodejs spec: containers: - image: brentley/ecsdemo-nodejs:latest imagePullPolicy: Always name: ecsdemo-nodejs ports: - containerPort: 3000 protocol: TCP  In the sample file above, we describe the service and how it should be deployed. We will write this description to the kubernetes api using kubectl, and kubernetes will ensure our preferences are met as the application is deployed.\nThe containers listen on port 3000, and native service discovery will be used to locate the running containers and communicate with them.\n"
},
{
	"uri": "/beginner/120_network-policies/calico/stars_policy_demo/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Clean up the demo by deleting the namespaces:\nkubectl delete namespace client stars management-ui "
},
{
	"uri": "/intermediate/210_jenkins/codecommit/",
	"title": "CodeCommit Repository, Access, and Code",
	"tags": [],
	"description": "",
	"content": "We\u0026rsquo;ll start by creating a CodeCommit repository to store our example application. This repository will store our application code and Jenkinsfile.\naws codecommit create-repository --repository-name eksworkshop-app We\u0026rsquo;ll create an IAM user with our HTTPS Git credentials for AWS CodeCommit to clone our repository and to push additional commits. This user needs an IAM Policy for access to CodeCommit.\naws iam create-user \\  --user-name git-user aws iam attach-user-policy \\  --user-name git-user \\  --policy-arn arn:aws:iam::aws:policy/AWSCodeCommitPowerUser aws iam create-service-specific-credential \\  --user-name git-user --service-name codecommit.amazonaws.com \\  | tee /tmp/gituser_output.json GIT_USERNAME=$(cat /tmp/gituser_output.json | jq -r \u0026#39;.ServiceSpecificCredential.ServiceUserName\u0026#39;) GIT_PASSWORD=$(cat /tmp/gituser_output.json | jq -r \u0026#39;.ServiceSpecificCredential.ServicePassword\u0026#39;) CREDENTIAL_ID=$(cat /tmp/gituser_output.json | jq -r \u0026#39;.ServiceSpecificCredential.ServiceSpecificCredentialId\u0026#39;) The repository will require some initial code so we\u0026rsquo;ll clone the repository and add a simple Go application.\nsudo pip install git-remote-codecommit git clone codecommit::${AWS_REGION}://eksworkshop-app cd eksworkshop-app server.go contains our simple application.\ncat \u0026lt;\u0026lt; EOF \u0026gt; server.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func helloWorld(w http.ResponseWriter, r *http.Request){ fmt.Fprintf(w, \u0026#34;Hello World\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, helloWorld) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } EOF server_test.go contains our unit tests.\ncat \u0026lt;\u0026lt; EOF \u0026gt; server_test.go package main import ( \u0026#34;net/http\u0026#34; \u0026#34;net/http/httptest\u0026#34; \u0026#34;testing\u0026#34; ) func Test_helloWorld(t *testing.T) { req, err := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;http://domain.com/\u0026#34;, nil) if err != nil { t.Fatal(err) } res := httptest.NewRecorder() helloWorld(res, req) exp := \u0026#34;Hello World\u0026#34; act := res.Body.String() if exp != act { t.Fatalf(\u0026#34;Expected %s got %s\u0026#34;, exp, act) } } EOF The Jenkinsfile will contain our pipeline declaration, the additional containers in our build agent pods, and which container will be used for each step of the pipeline.\ncat \u0026lt;\u0026lt; EOF \u0026gt; Jenkinsfile pipeline { agent { kubernetes { yaml \u0026#34;\u0026#34;\u0026#34; apiVersion: v1 kind: Pod spec: containers: - name: golang image: golang:1.13 command: - cat tty: true \u0026#34;\u0026#34;\u0026#34; } } stages { stage(\u0026#39;Run tests\u0026#39;) { steps { container(\u0026#39;golang\u0026#39;) { sh \u0026#39;go test\u0026#39; } } } stage(\u0026#39;Build\u0026#39;) { steps { container(\u0026#39;golang\u0026#39;) { sh \u0026#39;go build -o eksworkshop-app\u0026#39; archiveArtifacts \u0026#34;eksworkshop-app\u0026#34; } } } } } EOF We\u0026rsquo;ll add the code our code, commit the change, and then push the code to our repository.\ngit add --all \u0026amp;\u0026amp; git commit -m \u0026#34;Initial commit.\u0026#34; \u0026amp;\u0026amp; git push cd ~/environment "
},
{
	"uri": "/beginner/170_statefulset/storageclass/",
	"title": "Define Storageclass",
	"tags": [],
	"description": "",
	"content": "Introduction Dynamic Volume Provisioning allows storage volumes to be created on-demand. StorageClass should be pre-created to define which provisioner should be used and what parameters should be passed when dynamic provisioning is invoked.\nDefine Storage Class Copy/Paste the following commands into your Cloud9 Terminal.\ncat \u0026lt;\u0026lt; EoF \u0026gt; ${HOME}/environment/ebs_statefulset/mysql-storageclass.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: mysql-gp2 provisioner: ebs.csi.aws.com # Amazon EBS CSI driver parameters: type: gp2 encrypted: \u0026#39;true\u0026#39; # EBS volumes will always be encrypted by default volumeBindingMode: WaitForFirstConsumer # EBS volumes are AZ specific reclaimPolicy: Delete mountOptions: - debug EoF You can see that:\n The provisioner is ebs.csi.aws.com. The volume type is General Purpose SSD volumes (gp2). The encrypted parameter will ensure the EBS volumes are encrypted by default. The volumeBindingMode is WaitForFirstConsumer to ensure persistent volume will be provisioned after Pod is created so they reside in the same AZ  Create storageclass mysql-gp2 by following command.\nkubectl create -f ${HOME}/environment/ebs_statefulset/mysql-storageclass.yaml You can verify the StorageClass and its options with this command.\nkubectl describe storageclass mysql-gp2 Name: mysql-gp2 IsDefaultClass: No Annotations: \u0026lt;none\u0026gt; Provisioner: ebs.csi.aws.com Parameters: encrypted=true,type=gp2 AllowVolumeExpansion: \u0026lt;unset\u0026gt; MountOptions: debug ReclaimPolicy: Delete VolumeBindingMode: WaitForFirstConsumer Events: \u0026lt;none\u0026gt;  Below is a preview of how the storageClassName will be used in defining the StatefulSet. We will specify mysql-gp2 as the storageClassName in volumeClaimTemplates at ‚ÄúCreate StatefulSet‚Äù section later.\nvolumeClaimTemplates: - metadata: name: data spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: mysql-gp2 resources: requests: storage: 10Gi  "
},
{
	"uri": "/beginner/060_helm/helm_intro/install/",
	"title": "Install Helm CLI",
	"tags": [],
	"description": "",
	"content": "Install the Helm CLI Before we can get started configuring Helm, we\u0026rsquo;ll need to first install the command line tools that you will interact with. To do this, run the following:\ncurl -sSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash We can verify the version\nhelm version --short Let\u0026rsquo;s configure our first Chart repository. Chart repositories are similar to APT or yum repositories that you might be familiar with on Linux, or Taps for Homebrew on macOS.\nDownload the stable repository so we have something to start with:\nhelm repo add stable https://charts.helm.sh/stable Once this is installed, we will be able to list the charts you can install:\nhelm search repo stable Finally, let\u0026rsquo;s configure Bash completion for the helm command:\nhelm completion bash \u0026gt;\u0026gt; ~/.bash_completion . /etc/profile.d/bash_completion.sh . ~/.bash_completion source \u0026lt;(helm completion bash) "
},
{
	"uri": "/beginner/070_healthchecks/livenessprobe/",
	"title": "Configure Liveness Probe",
	"tags": [],
	"description": "",
	"content": "Configure the Probe Use the command below to create a directory\nmkdir -p ~/environment/healthchecks Run the following code block to populate the manifest file ~/environment/healthchecks/liveness-app.yaml. In the configuration file, the livenessProbe field determines how kubelet should check the container in order to consider whether it is healthy or not. kubelet uses the periodSeconds field to do frequent check on the Container. In this case, kubelet checks the liveness probe every 5 seconds. The initialDelaySeconds field is used to tell kubelet that it should wait for 5 seconds before doing the first probe. To perform a probe, kubelet sends a HTTP GET request to the server hosting this pod and if the handler for the servers /health returns a success code, then the container is considered healthy. If the handler returns a failure code, the kubelet kills the container and restarts it.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/healthchecks/liveness-app.yaml apiVersion: v1 kind: Pod metadata: name: liveness-app spec: containers: - name: liveness image: brentley/ecsdemo-nodejs livenessProbe: httpGet: path: /health port: 3000 initialDelaySeconds: 5 periodSeconds: 5 EoF Let\u0026rsquo;s create the pod using the manifest:\nkubectl apply -f ~/environment/healthchecks/liveness-app.yaml The above command creates a pod with liveness probe.\nkubectl get pod liveness-app The output looks like below. Notice the RESTARTS\nNAME READY STATUS RESTARTS AGE liveness-app 1/1 Running 0 11s  The kubectl describe command will show an event history which will show any probe failures or restarts.\nkubectl describe pod liveness-app Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 38s default-scheduler Successfully assigned default/liveness-app to ip-192-168-18-63.ec2.internal Normal Pulling 37s kubelet Pulling image \u0026#34;brentley/ecsdemo-nodejs\u0026#34; Normal Pulled 37s kubelet Successfully pulled image \u0026#34;brentley/ecsdemo-nodejs\u0026#34; in 108.556215ms Normal Created 37s kubelet Created container liveness Normal Started 37s kubelet Started container liveness  Introduce a Failure We will run the next command to send a SIGUSR1 signal to the nodejs application. By issuing this command we will send a kill signal to the application process in the docker runtime.\nkubectl exec -it liveness-app -- /bin/kill -s SIGUSR1 1 Describe the pod after waiting for 15-20 seconds and you will notice the kubelet actions of killing the container and restarting it. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 72s default-scheduler Successfully assigned default/liveness-app to ip-192-168-18-63.ec2.internal Normal Pulled 71s kubelet Successfully pulled image \u0026#34;brentley/ecsdemo-nodejs\u0026#34; in 100.615806ms Warning Unhealthy 37s (x3 over 47s) kubelet Liveness probe failed: Get http://192.168.13.176:3000/health: context deadline exceeded (Client.Timeout exceeded while awaiting headers) Normal Killing 37s kubelet Container liveness failed liveness probe, will be restarted Normal Pulling 6s (x2 over 71s) kubelet Pulling image \u0026#34;brentley/ecsdemo-nodejs\u0026#34; Normal Created 6s (x2 over 71s) kubelet Created container liveness Normal Started 6s (x2 over 71s) kubelet Started container liveness Normal Pulled 6s kubelet Successfully pulled image \u0026#34;brentley/ecsdemo-nodejs\u0026#34; in 118.19123ms  When the nodejs application entered a debug mode with SIGUSR1 signal, it did not respond to the health check pings and kubelet killed the container. The container was subject to the default restart policy.\nkubectl get pod liveness-app The output looks like below:\nNAME READY STATUS RESTARTS AGE liveness-app 1/1 Running 1 12m  Challenge: How can we check the status of the container health checks?\n  Expand here to see the solution   kubectl logs liveness-app You can also use kubectl logs to retrieve logs from a previous instantiation of a container with --previous flag, in case the container has crashed\nkubectl logs liveness-app --previous   "
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/cwcinstall/",
	"title": "Installing Container Insights",
	"tags": [],
	"description": "",
	"content": "To complete the setup of Container Insights, you can follow the quick start instructions in this section.\nFrom your Cloud9 Terminal you will just need to run the following command.\ncurl -s https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed \u0026#34;s/{{cluster_name}}/eksworkshop-eksctl/;s/{{region_name}}/${AWS_REGION}/\u0026#34; | kubectl apply -f - The command above will:\n Create the Namespace amazon-cloudwatch. Create all the necessary security objects for both DaemonSet:  SecurityAccount. ClusterRole. ClusterRoleBinding.   Deploy Cloudwatch-Agent (responsible for sending the metrics to CloudWatch) as a DaemonSet. Deploy fluentd (responsible for sending the logs to Cloudwatch) as a DaemonSet. Deploy ConfigMap configurations for both DaemonSets.  You can find the full information and manual install steps here.\n You can verify all the DaemonSets have been deployed by running the following command.\nkubectl -n amazon-cloudwatch get daemonsets Output\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE cloudwatch-agent 3 3 3 3 3 \u0026lt;none\u0026gt; 2m43s fluentd-cloudwatch 3 3 3 3 3 \u0026lt;none\u0026gt; 2m43s  That\u0026rsquo;s it. It\u0026rsquo;s that simple to install the agent and get it up and running. You can follow the manual steps in the full documentation, but with the Quickstart the deployment of the Daemon is easy and quick!\nNow onto verifying the data is being collected! "
},
{
	"uri": "/intermediate/241_pixie/prereqs/",
	"title": "Prereqs",
	"tags": [],
	"description": "",
	"content": "To get ready to use Pixie to debug an application on a Kubernetes cluster, we will:\n Add a node group to scale the EKS cluster. Deploy a demo microservices app. Visit the microservices app front-end and trigger a bug.  Add a node group To accommodate the pods deployed by Pixie and the microservices demo application, we will add a managed node group to scale the eksworkshop-eksctl cluster.\nCurl the node group config file and use eksctl to create the new pixie-demo-ng node group:\ncurl -O https://raw.githubusercontent.com/pixie-labs/pixie-demos/main/eks-workshop/clusterconfig.yaml envsubst \u0026lt; clusterconfig.yaml | eksctl create nodegroup -f -  The creation of the node group will take about 3 - 5 minutes.\n Confirm that the new nodes joined the cluster correctly. You should see 3 nodes added to the cluster.\nkubectl get nodes --sort-by=.metadata.creationTimestamp NAME STATUS ROLES AGE VERSION ip-192-168-4-73.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 6m24s v1.17.12-eks-7684af ip-192-168-47-147.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 6m21s v1.17.12-eks-7684af ip-192-168-87-132.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 6m19s v1.17.12-eks-7684af ip-192-168-53-105.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 117s v1.17.12-eks-7684af ip-192-168-88-75.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 108s v1.17.12-eks-7684af ip-192-168-26-175.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 103s v1.17.12-eks-7684af  Deploy the demo microservices app To test out Pixie, we will deploy a modified version of Weavework‚Äôs Sock Shop microservices application. This demo app does not contain any manual instrumentation. Minimal modifications were made to set pod resource limits and create a bug in one of the services.\nCurl the config file and deploy the demo using kubectl:\ncurl -O https://raw.githubusercontent.com/pixie-labs/pixie-demos/main/eks-workshop/complete-demo.yaml kubectl apply -f complete-demo.yaml  Deploying the microservices demo will take about 3-5 minutes.\n Confirm that the application components have been deployed to the px-sock-shop namespace:\nkubectl get pods -n px-sock-shop You should see output similar to that below. All pods should be ready and available before proceeding.\nNAME READY STATUS RESTARTS AGE carts-5fc45568c4-nhv2q 1/1 Running 0 35m carts-db-64ff6c747f-zhh7z 1/1 Running 0 35m catalogue-8f6fdb6d8-dl5fd 1/1 Running 0 35m catalogue-db-69cf48ff8-pz9w8 1/1 Running 0 35m front-end-5756d95c69-7n8pc 1/1 Running 0 35m load-test-5d887bfd7d-p7vfd 1/1 Running 0 35m orders-77c57c89dc-qm2gj 1/1 Running 0 35m orders-db-df75f545f-fbcnl 1/1 Running 0 35m payment-7f95f9f77-9c2rm 1/1 Running 0 35m queue-master-bd556c45-xq6pp 1/1 Running 0 35m rabbitmq-68d55c844f-swknh 1/1 Running 0 35m shipping-745b9d8755-glb8x 1/1 Running 0 35m user-5cf8959676-v6jtx 1/1 Running 0 35m user-db-794cfdf85b-4f6rq 1/1 Running 0 35m  Visit the Sock Shop application The Sock Shop\u0026rsquo;s front-end service is exposed onto an external IP address using a LoadBalancer. Grab the Load Balancer address using:\nexport SERVICE_IP=$(kubectl -n px-sock-shop get svc front-end --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\u0026#34;) echo http://$SERVICE_IP/ You should see output similar to that below.\nworkshop:~/environment $ echo http://$SERVICE_IP/ http://a22bf691105874cf0a5468a2ddce7f19-2030728129.us-west-2.elb.amazonaws.com/  When the front-end service is first deployed, it can take several minutes for the Load Balancer to be created and DNS updated. During this time the link above may display a ‚Äúsite unreachable‚Äù message.\n To visit the Sock Shop app, navigate to the Load Balancer address in your browser. Click the ‚ÄúCatalogue‚Äù tab along the top of the page and you should see a variety of sock products.\nTrigger the microservices application bug This app has several bugs. One bug in the app is that filtering the catalogue doesn\u0026rsquo;t work when two or more filters are selected.\n Navigate to the \u0026ldquo;Catalogue\u0026rdquo; tab Select at least two tags from the left \u0026ldquo;Filter\u0026rdquo; panel, for example geek and formal. Press Apply. Notice that no socks show up when two or more filters are selected. Press Clear to clear the filters between retries. You can repeat this as many times as you want.  Make sure to trigger this bug for yourself. We will use Pixie to investigate this application bug.\n "
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/verifycwci/",
	"title": "Verify CloudWatch Container Insights is working",
	"tags": [],
	"description": "",
	"content": "To verify that data is being collected in CloudWatch, launch the CloudWatch Containers UI in your browser using the link generated by the command below\necho \u0026#34; Use the URL below to access Cloudwatch Container Insights in $AWS_REGION: https://console.aws.amazon.com/cloudwatch/home?region=${AWS_REGION}#cw:dashboard=Container;context=~(clusters~\u0026#39;eksworkshop-eksctl~dimensions~(~)~performanceType~\u0026#39;Service)\u0026#34; From here you can see the metrics are being collected and presented to CloudWatch. You can switch between various drop downs to see EKS Services, EKS Cluster and more.\nWe can now continue with load testing the cluster to see how these metrics can look under load. "
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/prepareloadtest/",
	"title": "Preparing your Load Test",
	"tags": [],
	"description": "",
	"content": "Now that we have monitoring enabled we will simulate heavy load to our EKS Cluster hosting our Wordpress install. While generating the load, we can watch CloudWatch Container Insights for the performance metrics.\nInstall Siege for load testing on your Workspace sudo yum install siege -y Verify Siege is working by typing the below into your terminal window.\nsiege --version Output example (version may vary). SIEGE 3.0.8 Copyright (C) 2014 by Jeffrey Fulmer, et al. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  "
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/runloadtest/",
	"title": "Running the Load Test",
	"tags": [],
	"description": "",
	"content": "Run Siege to Load Test your Wordpress Site Now that Siege has been installed, we\u0026rsquo;re going to generate some load to our Wordpress site and see the metrics change in CloudWatch Container Insights.\nFrom your terminal window, run the following command.\nexport WP_ELB=$(kubectl -n wordpress-cwi get svc understood-zebu-wordpress -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[].hostname}\u0026#34;) siege -q -t 15S -c 200 -i http://${WP_ELB} This command tells Siege to run 200 concurrent connections to your Wordpress site at varying URLS for 15 seconds.\nAfter the 15 seconds, you should see an output like the one below.\nLifting the server siege... done. Transactions: 614 hits Availability: 100.00 % Elapsed time: 14.33 secs Data transferred: 4.14 MB Response time: 3.38 secs Transaction rate: 42.85 trans/sec Throughput: 0.29 MB/sec Concurrency: 144.79 Successful transactions: 614 Failed transactions: 0 Longest transaction: 5.55 Shortest transaction: 0.19 FILE: /home/ec2-user/siege.log You can disable this annoying message by editing the .siegerc file in your home directory; change the directive \u0026#39;show-logfile\u0026#39; to false.  Now let\u0026rsquo;s go view our newly collected metrics! "
},
{
	"uri": "/beginner/115_sg-per-pod/09_prerequisite/",
	"title": "Prerequisite",
	"tags": [],
	"description": "",
	"content": " Security groups for pods are supported by most Nitro-based Amazon EC2 instance families, including the m5, c5, r5, p3, m6g, c6g, and r6g instance families. The t3 instance family is not supported and so we will create a second NodeGroup using one m5.large instance.\n mkdir ${HOME}/environment/sg-per-pod cat \u0026lt;\u0026lt; EoF \u0026gt; ${HOME}/environment/sg-per-pod/nodegroup-sec-group.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} managedNodeGroups: - name: nodegroup-sec-group desiredCapacity: 1 instanceType: m5.large EoF eksctl create nodegroup -f ${HOME}/environment/sg-per-pod/nodegroup-sec-group.yaml kubectl get nodes \\  --selector node.kubernetes.io/instance-type=m5.large NAME STATUS ROLES AGE VERSION ip-192-168-34-45.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 4m57s v1.17.12-eks-7684af  "
},
{
	"uri": "/advanced/340_appmesh_flagger/install_appmesh/",
	"title": "Install AWS App Mesh Controller",
	"tags": [],
	"description": "",
	"content": "Prerequisites We assume that we have already have the following setup before we start this chapter.\n [Required] An existing EKS Cluster eksworkshop-eksctl and Nodegroup created from EKS Workshop [Optional] increased the disk size on your Cloud9 instance.  Install App Mesh Helm Chart Check that Helm is installed.\nhelm list This command should either return a list of helm charts that have already been deployed or nothing.\nIf you get an error message, see installing helm for instructions.\n Add EKS Helm Repo The AWS App Mesh Controller for Kubernetes is easily installed using Helm. To get started, add the EKS Charts repository.\nhelm repo add eks https://aws.github.io/eks-charts \u0026#34;eks\u0026#34; has been added to your repositories  Install App Mesh Controller Create the namespace appmesh-system, enable OIDC and create IRSA (IAM for Service Account) for AWS App Mesh installation\n# Create the namespace kubectl create ns appmesh-system # Install the App Mesh CRDs kubectl apply -k \u0026#34;github.com/aws/eks-charts/stable/appmesh-controller//crds?ref=master\u0026#34; # Create your OIDC identity provider for the cluster eksctl utils associate-iam-oidc-provider \\  --cluster eksworkshop-eksctl \\  --approve # Download the IAM policy for AWS App Mesh Kubernetes Controller curl -o controller-iam-policy.json https://raw.githubusercontent.com/aws/aws-app-mesh-controller-for-k8s/master/config/iam/controller-iam-policy.json # Create an IAM policy called AWSAppMeshK8sControllerIAMPolicy aws iam create-policy \\  --policy-name AWSAppMeshK8sControllerIAMPolicy \\  --policy-document file://controller-iam-policy.json # Create an IAM role for the appmesh-controller service account eksctl create iamserviceaccount --cluster eksworkshop-eksctl \\  --namespace appmesh-system \\  --name appmesh-controller \\  --attach-policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSAppMeshK8sControllerIAMPolicy \\  --override-existing-serviceaccounts \\  --approve Install App Mesh Controller into the appmesh-system namespace\nhelm upgrade -i appmesh-controller eks/appmesh-controller \\  --namespace appmesh-system \\  --set region=$AWS_REGION \\  --set serviceAccount.create=false \\  --set serviceAccount.name=appmesh-controller Release \u0026#34;appmesh-controller\u0026#34; has been upgraded. Happy Helming! NAME: appmesh-controller LAST DEPLOYED: Wed Jan 20 21:07:01 2021 NAMESPACE: appmesh-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: AWS App Mesh controller installed!  Confirm that the controller version is¬†v1.0.0¬†or later.\nkubectl get deployment appmesh-controller \\  -n appmesh-system \\  -o json | jq -r \u0026#34;.spec.template.spec.containers[].image\u0026#34; | cut -f2 -d \u0026#39;:\u0026#39; v1.3.0  Confirm all the App Mesh CRDs are created in the Cluster\nkubectl get crds | grep appmesh gatewayroutes.appmesh.k8s.aws 2020-11-02T16:02:14Z meshes.appmesh.k8s.aws 2020-11-02T16:02:15Z virtualgateways.appmesh.k8s.aws 2020-11-02T16:02:15Z virtualnodes.appmesh.k8s.aws 2020-11-02T16:02:15Z virtualrouters.appmesh.k8s.aws 2020-11-02T16:02:15Z virtualservices.appmesh.k8s.aws 2020-11-02T16:02:15Z  Get all the resources created in appmesh-system Namespace\nkubectl -n appmesh-system get all NAME READY STATUS RESTARTS AGE pod/appmesh-controller-fcc7c4ffc-mldhk 1/1 Running 0 47s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/appmesh-controller-webhook-service ClusterIP 10.100.xx.yy \u0026lt;none\u0026gt; 443/TCP 27m √• NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/appmesh-controller 1/1 1 1 27m NAME D kubectl get crds ESIRED CURRENT READY AGE replicaset.apps/appmesh-controller-fcc7c4ffc 1 1 1 47s  Congratulations on installing the AWS App Mesh Controller in your EKS Cluster!\n"
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/viewvetrics/",
	"title": "Viewing our collected metrics",
	"tags": [],
	"description": "",
	"content": "Now let\u0026rsquo;s navigate back to CloudWatch Container Insights browser tab to view the data we\u0026rsquo;ve generated.\nFrom here you can choose a number of different views. We‚Äôre going to narrow down our timelines to a custom time range of just 30 minute so we can zoom into our recently collected insights.\nTo do so go to the Time Range option at the top right of The CloudWatch Container Insights windows and selecting 30 minutes.\nOnce zoomed in on the time frame we can see the large spike in resource usage for the load we just generated to the Wordpress service in our EKS Cluster.\nAs mentioned previous you can view some different metrics based on the Dropdown menu options. Let\u0026rsquo;s take a quick look at some of those items.\n"
},
{
	"uri": "/beginner/110_irsa/preparation/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "Enabling IAM Roles for Service Accounts on your Cluster  The IAM roles for service accounts feature is available on new Amazon EKS Kubernetes version 1.16 or higher, and clusters that were updated to versions 1.14 or 1.13 on or after September 3rd, 2019.  If your EKS cluster version is older than 1.16 your outputs may very. Please consider reading the updating an Amazon EKS Cluster section in the User Guide.\n kubectl version --short Output: Client Version: v1.20.4-eks-6b7464 Server Version: v1.20.4-eks-6b7464  If your aws cli version is lower than 1.19.122, use Installing the AWS CLI in the User Guide\n aws --version Output: aws-cli/1.19.112 Python/2.7.18 Linux/4.14.232-177.418.amzn2.x86_64 botocore/1.20.112  Retrieve OpenID Connect issuer URL Your EKS cluster has an OpenID Connect issuer URL associated with it, and this will be used when configuring the IAM OIDC Provider. You can check it with:\naws eks describe-cluster --name eksworkshop-eksctl --query cluster.identity.oidc.issuer --output text Output: https://oidc.eks.us-east-1.amazonaws.com/id/09D1E682ADD23F8431B986E4B2E35BCB  "
},
{
	"uri": "/beginner/200_secrets/deploying-secrets-variables/",
	"title": "Creating and Deploying Secrets",
	"tags": [],
	"description": "",
	"content": "Since 1.14, Kubectl supports the management of Kubernetes objects using Kustomize. Kustomize provides resource Generators to create Secrets and ConfigMaps. The Kustomize generators should be specified in a kustomization.yaml file. A Kustomize file for generating a Secret from literal key-value pairs looks as follows: namespace: octank secretGenerator: - name: database-credentials literals: - username=admin - password=Tru5tN0! generatorOptions: disableNameSuffixHash: true  Run the following set of commands to generate a Secret using Kubectl and Kustomize.\nmkdir -p ~/environment/secrets cd ~/environment/secrets wget https://eksworkshop.com/beginner/200_secrets/secrets.files/kustomization.yaml kubectl kustomize . \u0026gt; secret.yaml The generated Secret with base64 encoded value for username and password keys is as follows: apiVersion: v1 kind: Secret type: Opaque metadata: name: database-credentials namespace: octank data: password: VHJ1NXROMCE= username: YWRtaW4=  You can now deploy this Secret to your EKS cluster.\nkubectl create namespace octank kubectl apply -f secret.yaml Exposing Secrets as Environment Variables You may expose the keys, namely, username and password, in the database-credentials Secret to a Pod as environment variables using a Pod manifest as shown below: apiVersion: v1 kind: Pod metadata: name: someName namespace: someNamespace spec: containers: - name: someContainer image: someImage env: - name: DATABASE_USER valueFrom: secretKeyRef: name: database-credentials key: username - name: DATABASE_PASSWORD valueFrom: secretKeyRef: name: database-credentials key: password  Run the following set of commands to deploy a pod that references the database-credentials Secret created above.\nwget https://eksworkshop.com/beginner/200_secrets/secrets.files/pod-variable.yaml kubectl apply -f pod-variable.yaml kubectl get pod -n octank View the output logs from the pod to verfiy that the environment variables DATABASE_USER and DATABASE_PASSWORD have been assigned the expected literal values\nkubectl logs pod-variable -n octank The output should look as follows: DATABASE_USER = admin DATABASE_PASSWROD = Tru5tN0!  "
},
{
	"uri": "/beginner/185_bottlerocket/prerequisites/",
	"title": "Prerequisite",
	"tags": [],
	"description": "",
	"content": "Amazon provides official AMIs in the following AWS regions:\n   Region Name Region     Africa (Cape Town) af-south-1   Asia Pacific (Hong Kong) ap-east-1   Asia Pacific (Tokyo) ap-northeast-1   Asia Pacific (Seoul) ap-northeast-2   Asia Pacific (Osaka) ap-northeast-3   Asia Pacific (Mumbai) ap-south-1   Asia Pacific (Singapore) ap-southeast-1   Asia Pacific (Sydney) ap-southeast-2   Canada (Central) ca-central-1   Europe (Frankfurt) eu-central-1   Europe (Stockholm) eu-north-1   Europe (Milan) eu-south-1   Europe (Ireland) eu-west-1   Europe (London) eu-west-2   Europe (Paris) eu-west-3   Middle East (Bahrain) me-south-1   South America (S√£o Paulo) sa-east-1   US East (N. Virginia) us-east-1   US East (Ohio) us-east-2   US West (N. California) us-west-1   US West (Oregon) us-west-2    Don\u0026rsquo;t continue the lab unless you use one of these region.\n "
},
{
	"uri": "/advanced/340_appmesh_flagger/flagger_setup/",
	"title": "Flagger Set Up",
	"tags": [],
	"description": "",
	"content": "Install App Mesh Prometheus Helm Chart helm upgrade -i appmesh-prometheus eks/appmesh-prometheus \\ \t--namespace appmesh-system \\ \t--set serviceAccount.create=false \\ \t--set serviceAccount.name=appmesh-controller Release \u0026#34;appmesh-prometheus\u0026#34; does not exist. Installing it now. NAME: appmesh-prometheus LAST DEPLOYED: Sat Mar 13 20:59:29 2021 NAMESPACE: appmesh-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: AWS App Mesh Prometheus installed!  Install Flagger Add Flagger Helm repository:\nhelm repo add flagger https://flagger.app \u0026#34;flagger\u0026#34; has been added to your repositories  Install Flagger\u0026rsquo;s Canary CRD:\nkubectl apply -f https://raw.githubusercontent.com/weaveworks/flagger/master/artifacts/flagger/crd.yaml customresourcedefinition.apiextensions.k8s.io/canaries.flagger.app created customresourcedefinition.apiextensions.k8s.io/metrictemplates.flagger.app created customresourcedefinition.apiextensions.k8s.io/alertproviders.flagger.app created  Deploy Flagger in the appmesh-system namespace:\nhelm upgrade -i flagger flagger/flagger \\  --namespace=appmesh-system \\  --set crd.create=false \\  --set meshProvider=appmesh:v1beta2 \\  --set metricsServer=http://appmesh-prometheus:9090 \\  --set serviceAccount.create=false \\  --set serviceAccount.name=appmesh-controller Release \u0026#34;flagger\u0026#34; does not exist. Installing it now. NAME: flagger LAST DEPLOYED: Mon Dec 14 15:23:32 2020 NAMESPACE: appmesh-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Flagger installed  Set up the horizontol pod autoscaler Install the Horizontal Pod Autoscaler (HPA) metrics provider:\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.1/components.yaml serviceaccount/metrics-server created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrole.rbac.authorization.k8s.io/system:metrics-server created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created service/metrics-server created deployment.apps/metrics-server created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created  After a minute, the metrics API should report CPU and memory usage for pods. You can verify status of the metrics API:\nkubectl get apiservice v1beta1.metrics.k8s.io -o json | jq \u0026#39;.status\u0026#39; { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2021-04-21T07:21:25Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;all checks passed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Passed\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ] }  kubectl -n kube-system top pods NAME CPU(cores) MEMORY(bytes) aws-node-g8bhs 3m 40Mi aws-node-rgbnf 3m 39Mi aws-node-wd5f9 3m 40Mi coredns-556765db45-4mvnb 2m 7Mi coredns-556765db45-z6hmw 2m 7Mi kube-proxy-gr7j8 1m 8Mi kube-proxy-jnjmd 1m 7Mi kube-proxy-zsn8g 1m 8Mi metrics-server-866b7d5b74-cvrgm 0m 4Mi  "
},
{
	"uri": "/advanced/430_emr_on_eks/prereqs/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "In this chapter, we will prepare your EKS cluster so that it is integrated with EMR on EKS. If you don\u0026rsquo;t have EKS cluster, please review instructions from start the workshop and launch using eksctl modules\nCreate namespace and RBAC permissions Let\u0026rsquo;s create a namespace \u0026lsquo;spark\u0026rsquo; in our EKS cluster. After this, we will use the automation powered by eksctl for creating RBAC permissions and for adding EMR on EKS service-linked role into aws-auth configmap\nkubectl create namespace spark eksctl create iamidentitymapping --cluster eksworkshop-eksctl --namespace spark --service-name \u0026quot;emr-containers\u0026quot; Enable IAM Roles for Service Account (IRSA) Your cluster should already have OpenID Connect provider URL. Only configuration that is needed is to associate IAM with OIDC. You can do that by running this command\neksctl utils associate-iam-oidc-provider --cluster eksworkshop-eksctl --approve Create IAM Role for job execution Let\u0026rsquo;s create the role that EMR will use for job execution. This is the role, EMR jobs will assume when they run on EKS.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/emr-trust-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;elasticmapreduce.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } EoF aws iam create-role --role-name EMRContainers-JobExecutionRole --assume-role-policy-document file://~/environment/emr-trust-policy.json Next, we need to attach the required IAM policies to the role so it can write logs to s3 and cloudwatch.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/EMRContainers-JobExecutionRole.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:ListBucket\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;logs:PutLogEvents\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:DescribeLogGroups\u0026quot;, \u0026quot;logs:DescribeLogStreams\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:logs:*:*:*\u0026quot; ] } ] } EoF aws iam put-role-policy --role-name EMRContainers-JobExecutionRole --policy-name EMR-Containers-Job-Execution --policy-document file://~/environment/EMRContainers-JobExecutionRole.json Update trust relationship for job execution role Now we need to update the trust relationship between IAM role we just created with EMR service identity.\naws emr-containers update-role-trust-policy --cluster-name eksworkshop-eksctl --namespace spark --role-name EMRContainers-JobExecutionRole Register EKS cluster with EMR The final step is to register EKS cluster with EMR.\naws emr-containers create-virtual-cluster \\ --name eksworkshop-eksctl \\ --container-provider '{ \u0026quot;id\u0026quot;: \u0026quot;eksworkshop-eksctl\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;EKS\u0026quot;, \u0026quot;info\u0026quot;: { \u0026quot;eksInfo\u0026quot;: { \u0026quot;namespace\u0026quot;: \u0026quot;spark\u0026quot; } } }' After you register, you should get confirmation that your EMR virtual cluster is created. A virtual cluster is an EMR concept which means that EMR service is registered to Kubernetes namespace and it can run jobs in that namespace. \u0026#34;id\u0026#34;: \u0026#34;av6h2hk8fsyu12m5ru8zjg8ht\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;eksworkshop-eksctl\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:emr-containers:us-west-2:xxxxxxxxxxxx:/virtualclusters/av6h2hk8fsyu12m5ru8zjg8ht\u0026#34;  Create EKS Managed Node Group Lets add a EKS managed nodegroup to this EKS cluster to have more resources to run sample spark jobs.\nCreate a config file (addnodegroup.yaml) with details of a new EKS managed nodegroup.\ncat \u0026lt;\u0026lt; EOF \u0026gt; addnodegroup.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} managedNodeGroups: - name: emrnodegroup desiredCapacity: 3 instanceType: m5.xlarge ssh: enableSsm: true EOF Create the new EKS managed nodegroup.\neksctl create nodegroup --config-file=addnodegroup.yaml  Launching a new EKS managed nodegroup will take a few minutes.\n Check if the new nodegroup has been added to your cluster.\nkubectl get nodes # if we see 6 nodes in total with the 3 newly added nodes, we know we have authenticated correctly Let\u0026rsquo;s create a s3 bucket to upload sample scripts and logs.\nexport s3DemoBucket=s3://emr-eks-demo-${ACCOUNT_ID}-${AWS_REGION} aws s3 mb $s3DemoBucket "
},
{
	"uri": "/beginner/150_spotnodegroups/spotnodegroups/",
	"title": "Add Spot managed node group",
	"tags": [],
	"description": "",
	"content": "We have our EKS cluster and nodes already, but we need some Spot Instances configured to run the workload. We will be creating a Spot managed node group to utilize Spot Instances. Managed node groups automatically create a label - eks.amazonaws.com/capacityType - to identify which nodes are Spot Instances and which are On-Demand Instances so that we can schedule the appropriate workloads to run on Spot Instances. We will use eksctl to launch new nodes running on Spot Instances that will connect to the EKS cluster.\nFirst, we can check that the current nodes are running On-Demand by checking the eks.amazonaws.com/capacityType label is set to ON_DEMAND. The output of the command shows the CAPACITYTYPE for the current nodes is set to ON_DEMAND.\nkubectl get nodes \\  --label-columns=eks.amazonaws.com/capacityType \\  --selector=eks.amazonaws.com/capacityType=ON_DEMAND Create Spot managed node group We will now create the a Spot managed node group using the \u0026ndash;spot option in eksctl create nodegroup command.\neksctl create nodegroup \\  --cluster=eksworkshop-eksctl --region=${AWS_REGION} \\  --managed --spot --name=ng-spot \\  --instance-types=m5.large,m4.large,m5d.large,m5a.large,m5ad.large,m5n.large,m5dn.large  Note, the instances above might not be present in your region. To select instances that meet that criteria in your region, you could install https://github.com/aws/amazon-ec2-instance-selector and execute the command ec2-instance-selector --base-instance-type m5.large --flexible to get a diversified selection of instances available in your region of choice that meet the criteria of being similar to m4.large (in vCPU and memory terms)\n Spot managed node group creates a label eks.amazonaws.com/capacityType and sets it to SPOT for the nodes.\nThe Spot managed node group created follows Spot best practices including using capacity-optimized as the spotAllocationStrategy, which will launch instances from the Spot Instance pools with the most available capacity (when EC2 needs the capacity back), aiming to decrease the number of Spot interruptions in our cluster.\nThe creation of the nodes will take about 3 minutes.\n Confirm the Nodes Confirm that the new nodes joined the cluster correctly. You should see 2 more nodes added to the cluster.\nkubectl get nodes --sort-by=.metadata.creationTimestamp You can use the eks.amazonaws.com/capacityType to identify the lifecycle of the nodes. The output of this command should return 2 nodes with the CAPACITYTYPE set to SPOT.\nkubectl get nodes \\  --label-columns=eks.amazonaws.com/capacityType \\  --selector=eks.amazonaws.com/capacityType=SPOT "
},
{
	"uri": "/intermediate/246_monitoring_amp_amg/create_amp_workspace/",
	"title": "Create AMP workspace",
	"tags": [],
	"description": "",
	"content": "Create a new AMP workspace Go to the AMP console and type-in a name for the AMP workspace and click on Create\nAlternatively, you can also use AWS CLI to create the workspace using the following command:\naws amp create-workspace --alias eks-workshop --region $AWS_REGION The AMP workspace should be created in just a few seconds. Once created, you will be able to see the workspace as shown below:\n"
},
{
	"uri": "/beginner/115_sg-per-pod/10_secgroup/",
	"title": "Security groups creation",
	"tags": ["beginner"],
	"description": "",
	"content": "Create and configure the security groups First, let\u0026rsquo;s create the RDS security group (RDS_SG). It will be used by the Amazon RDS instance to control network access.\nexport VPC_ID=$(aws eks describe-cluster \\  --name eksworkshop-eksctl \\  --query \u0026#34;cluster.resourcesVpcConfig.vpcId\u0026#34; \\  --output text) # create RDS security group aws ec2 create-security-group \\  --description \u0026#39;RDS SG\u0026#39; \\  --group-name \u0026#39;RDS_SG\u0026#39; \\  --vpc-id ${VPC_ID} # save the security group ID for future use export RDS_SG=$(aws ec2 describe-security-groups \\  --filters Name=group-name,Values=RDS_SG Name=vpc-id,Values=${VPC_ID} \\  --query \u0026#34;SecurityGroups[0].GroupId\u0026#34; --output text) echo \u0026#34;RDS security group ID: ${RDS_SG}\u0026#34; Now, let\u0026rsquo;s create the pod security group (POD_SG).\n# create the POD security group aws ec2 create-security-group \\  --description \u0026#39;POD SG\u0026#39; \\  --group-name \u0026#39;POD_SG\u0026#39; \\  --vpc-id ${VPC_ID} # save the security group ID for future use export POD_SG=$(aws ec2 describe-security-groups \\  --filters Name=group-name,Values=POD_SG Name=vpc-id,Values=${VPC_ID} \\  --query \u0026#34;SecurityGroups[0].GroupId\u0026#34; --output text) echo \u0026#34;POD security group ID: ${POD_SG}\u0026#34; The pod needs to communicate with its node for DNS resolution, so we will update the Node Group security group accordingly.\nexport NODE_GROUP_SG=$(aws ec2 describe-security-groups \\  --filters Name=tag:Name,Values=eks-cluster-sg-eksworkshop-eksctl-* Name=vpc-id,Values=${VPC_ID} \\  --query \u0026#34;SecurityGroups[0].GroupId\u0026#34; \\  --output text) echo \u0026#34;Node Group security group ID: ${NODE_GROUP_SG}\u0026#34; # allow POD_SG to connect to NODE_GROUP_SG using TCP 53 aws ec2 authorize-security-group-ingress \\  --group-id ${NODE_GROUP_SG} \\  --protocol tcp \\  --port 53 \\  --source-group ${POD_SG} # allow POD_SG to connect to NODE_GROUP_SG using UDP 53 aws ec2 authorize-security-group-ingress \\  --group-id ${NODE_GROUP_SG} \\  --protocol udp \\  --port 53 \\  --source-group ${POD_SG} Finally, we will add two inbound traffic (ingress) rules to the RDS_SG security group:\n One for Cloud9 (to populate the database). One to allow POD_SG security group to connect to the database.  # Cloud9 IP export C9_IP=$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4) # allow Cloud9 to connect to RDS aws ec2 authorize-security-group-ingress \\  --group-id ${RDS_SG} \\  --protocol tcp \\  --port 5432 \\  --cidr ${C9_IP}/32 # Allow POD_SG to connect to the RDS aws ec2 authorize-security-group-ingress \\  --group-id ${RDS_SG} \\  --protocol tcp \\  --port 5432 \\  --source-group ${POD_SG} "
},
{
	"uri": "/beginner/190_fsx_lustre/launching-fsx/",
	"title": "Creating an Fsx Lustre File System",
	"tags": [],
	"description": "",
	"content": "The Amazon FSx for Lustre Container Storage Interface (CSI) driver provides a CSI interface that allows Amazon EKS clusters to manage the lifecycle of Amazon FSx for Lustre file systems. For detailed descriptions of the available parameters and complete examples that demonstrate the driver\u0026rsquo;s features, see the Amazon FSx for Lustre Container Storage Interface (CSI) driver project on GitHub.\nPrerequisites You must complete the Start the Workshop and Launch using eksctl modules if you haven\u0026rsquo;t already, in order to meet the following prerequisites:\n  Version 1.18.163 or later of the AWS CLI installed. You can check your currently-installed version with the aws \u0026ndash;version command. To install or upgrade the AWS CLI, see Installing the AWS CLI.\n  An existing Amazon EKS cluster. If you don\u0026rsquo;t currently have a cluster, see Getting started with Amazon EKS to create one.\n  Version 0.31.0-rc.0 or later of eksctl installed. You can check your currently-installed version with the eksctl version command. To install or upgrade eksctl, see Installing or upgrading eksctl.\n  The latest version of kubectl installed that aligns to your cluster version. You can check your currently-installed version with the kubectl version \u0026ndash;short \u0026ndash;client command. For more information, see Installing kubectl.\n  Have the correct shell variables from the prerequisite modules\n  Now let\u0026rsquo;s setup these variables needed for the module:\nACCOUNT_ID=$(aws sts get-caller-identity --query \u0026quot;Account\u0026quot; --output text) CLUSTER_NAME=eksworkshop-eksctl VPC_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --query \u0026quot;cluster.resourcesVpcConfig.vpcId\u0026quot; --output text) SUBNET_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --query \u0026quot;cluster.resourcesVpcConfig.subnetIds[0]\u0026quot; --output text) SECURITY_GROUP_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --query \u0026quot;cluster.resourcesVpcConfig.securityGroupIds\u0026quot; --output text) CIDR_BLOCK=$(aws ec2 describe-vpcs --vpc-ids $VPC_ID --query \u0026quot;Vpcs[].CidrBlock\u0026quot; --output text) S3_LOGS_BUCKET=eks-fsx-lustre-$(cat /dev/urandom | LC_ALL=C tr -dc \u0026quot;[:alpha:]\u0026quot; | tr '[:upper:]' '[:lower:]' | head -c 32) SECURITY_GROUP_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --query \u0026quot;cluster.resourcesVpcConfig.clusterSecurityGroupId\u0026quot; --output text) To deploy the Amazon FSx for Lustre CSI driver to an Amazon EKS cluster   Create an AWS Identity and Access Management OIDC provider and associate it with your cluster.\neksctl utils associate-iam-oidc-provider \\ --region $AWS_REGION \\ --cluster $CLUSTER_NAME \\ --approve   Create an IAM policy and service account that allows the driver to make calls to AWS APIs on your behalf.\ncat \u0026lt;\u0026lt; EOF \u0026gt; fsx-csi-driver.json { \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[ { \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;iam:CreateServiceLinkedRole\u0026quot;, \u0026quot;iam:AttachRolePolicy\u0026quot;, \u0026quot;iam:PutRolePolicy\u0026quot; ], \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:iam::*:role/aws-service-role/s3.data-source.lustre.fsx.amazonaws.com/*\u0026quot; }, { \u0026quot;Action\u0026quot;:\u0026quot;iam:CreateServiceLinkedRole\u0026quot;, \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;:{ \u0026quot;StringLike\u0026quot;:{ \u0026quot;iam:AWSServiceName\u0026quot;:[ \u0026quot;fsx.amazonaws.com\u0026quot; ] } } }, { \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;fsx:CreateFileSystem\u0026quot;, \u0026quot;fsx:DeleteFileSystem\u0026quot;, \u0026quot;fsx:DescribeFileSystems\u0026quot; ], \u0026quot;Resource\u0026quot;:[ \u0026quot;*\u0026quot; ] } ] } EOF   Create the policy.\n   aws iam create-policy \\ --policy-name Amazon_FSx_Lustre_CSI_Driver \\ --policy-document file://fsx-csi-driver.json  Create a Kubernetes service account for the driver and attach the policy to the service account. Replacing the ARN of the policy with the ARN returned in the previous step.\neksctl create iamserviceaccount \\ --region $AWS_REGION \\ --name fsx-csi-controller-sa \\ --namespace kube-system \\ --cluster $CLUSTER_NAME \\ --attach-policy-arn arn:aws:iam::$ACCOUNT_ID:policy/Amazon_FSx_Lustre_CSI_Driver \\ --approve Output:\nYou\u0026rsquo;ll see several lines of output as the service account is created. The last line of output is similar to the following example line.\n[‚Ñπ] created serviceaccount \u0026quot;kube-system/fsx-csi-controller-sa\u0026quot;   Save the Role ARN that was created via CloudFormation into a variable.\nexport ROLE_ARN=$(aws cloudformation describe-stacks --stack-name eksctl-eksworkshop-eksctl-addon-iamserviceaccount-kube-system-fsx-csi-controller-sa --query \u0026quot;Stacks[0].Outputs[0].OutputValue\u0026quot; --output text)   Deploy the driver with the following command.\nkubectl apply -k \u0026quot;github.com/kubernetes-sigs/aws-fsx-csi-driver/deploy/kubernetes/overlays/stable/?ref=master\u0026quot; Output:\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply serviceaccount/fsx-csi-controller-sa configured clusterrole.rbac.authorization.k8s.io/fsx-csi-external-provisioner-role created clusterrolebinding.rbac.authorization.k8s.io/fsx-csi-external-provisioner-binding created deployment.apps/fsx-csi-controller created daemonset.apps/fsx-csi-node created csidriver.storage.k8s.io/fsx.csi.aws.com created   Patch the driver deployment to add the service account that you created in step 3, replacing the ARN with the ARN that you saved in step 4.\n  kubectl annotate serviceaccount -n kube-system fsx-csi-controller-sa \\ eks.amazonaws.com/role-arn=$ROLE_ARN --overwrite=true "
},
{
	"uri": "/intermediate/201_resource_management/basic-pod-limits/",
	"title": "Basic Pod CPU and Memory Management",
	"tags": [],
	"description": "",
	"content": "We will create four pods:\n A request deployment with Request cpu = 0.5 and memory = 1G A limit-cpu deployment with Limit cpu = 0.5 and memory = 1G A limit-memory deployment with Limit cpu = 1 and memory = 1G A restricted deployment with Request of cpu = 1/memory = 1G and Limit cpu = 1.8/memory=2G  Deploy Metrics Server Follow the instructions in the module Deploy the Metrics Server to enable the Kubernetes Metrics Server.\nVerify that the metrics-server deployment is running the desired number of pods with the following command.\nkubectl get deployment metrics-server -n kube-system Output: NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1/1 1 1 19s  CPU units are expressed as 1 CPU or 1000m, which equals to 1vCPU/Core. Additional details can be found here\n Deploy Pods In order to generate cpu and memory load we will use stress-ng with the following flags.\n vm-keep: maintain consistent memory usage vm-bytes: bytes given to each worker vm: number of workers to spawn ex. vm=1 uses 1000m CPU vm=2 uses 2000m CPU oomable: will not respawn after being killed by OOM killer verbose: show all information output timeout: length of test  # Deploy Limits pod with hard limit on cpu at 500m but wants 1000m kubectl run --limits=memory=1G,cpu=0.5 --image hande007/stress-ng basic-limit-cpu-pod --restart=Never -- --vm-keep --vm-bytes 512m --timeout 600s --vm 1 --oomable --verbose # Deploy Request pod with soft limit on memory  kubectl run --requests=memory=1G,cpu=0.5 --image hande007/stress-ng basic-request-pod --restart=Never -- --vm-keep --vm-bytes 2g --timeout 600s --vm 1 --oomable --verbose # Deploy restricted pod with limits and requests wants cpu 2 and memory at 1G kubectl run --requests=cpu=1,memory=1G --limits=cpu=1.8,memory=2G --image hande007/stress-ng basic-restricted-pod --restart=Never -- --vm-keep --vm-bytes 1g --timeout 600s --vm 2 --oomable --verbose # Deploy Limits pod with hard limit on memory at 1G but wants 2G kubectl run --limits=memory=1G,cpu=1 --image hande007/stress-ng basic-limit-memory-pod --restart=Never -- --vm-keep --vm-bytes 2g --timeout 600s --vm 1 --oomable --verbose Verify Current Resource Usage Check if pods are running properly. It is expected that basic-limit-memory-pod is not not running due to it asking for 2G of memory when it is assigned a Limit of 1G.\nkubectl get pod Output: NAME READY STATUS RESTARTS AGE basic-limit-cpu-pod 1/1 Running 0 69s basic-limit-memory-pod 0/1 OOMKilled 0 68s basic-request-pod 1/1 Running 0 68s basic-restricted-pod 1/1 Running 0 67s  Next we check the current utilization\n# After at least 60 seconds of generating metrics kubectl top pod Output: NAME CPU(cores) MEMORY(bytes) basic-limit-cpu-pod 501m 516Mi basic-request-pod 1000m 2055Mi basic-restricted-pod 1795m 1029Mi  Running multiple stress-ng on the same node will consume less CPU per pod. For example if the expected CPU is 1000 but only running 505 there may be other pods on the nodes consuming CPU.\n Kubernetes Requests and Limits can be applied to higher level abstractions like Deployment.   Expand here to see the example   apiVersion: apps/v1 kind: Deployment metadata: labels: app: stress-deployment name: stress-deployment spec: replicas: 1 selector: matchLabels: app: stress-deployment template: metadata: labels: app: stress-deployment spec: containers: - args: - --vm-keep - --vm-bytes - 1500m - --timeout - 600s - --vm - \u0026quot;3\u0026quot; - --oomable - --verbose image: hande007/stress-ng name: stress-deployment resources: limits: cpu: 2200m memory: 2G requests: cpu: \u0026quot;1\u0026quot; memory: 1G   \nCleanup Clean up the pods before moving on to free up resources\nkubectl delete pod basic-request-pod kubectl delete pod basic-limit-memory-pod kubectl delete pod basic-limit-cpu-pod kubectl delete pod basic-restricted-pod "
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/viewlogs/",
	"title": "Viewing our collected logs",
	"tags": [],
	"description": "",
	"content": "Now that we have a good understanding of the load, let\u0026rsquo;s explore the logs generated by WordPress and sent to Cloudwatch by the Fluentd agent.\nFrom the CloudWatch Container Insights browser tab:\n Scroll down to the Pod performance section. Select the WordPress pod. Select application logs from the Action menu.  The last action will open the CloudWatch Logs Insights UI in another tab.\nClick the Run query button and expand one of log line to look at it.\nFluentd has split the JSON files into multiple fields that could be easily parsed for debugging or to be included into Custom Application Dashboard.\nCloudWatch Logs Insights enables you to explore, analyze, and visualize your logs instantly, allowing you to troubleshoot operational problems with ease. You can learn more about CloudWatch Logs Insights here.\n "
},
{
	"uri": "/beginner/091_iam-groups/intro/",
	"title": "Kubernetes Authentication",
	"tags": [],
	"description": "",
	"content": "In the intro to RBAC module, we have seen how we can give access to individual users to Kubernetes.\nIf you have different teams which needs different kind of cluster access, it would be difficult to manually add or remove access for each EKS clusters you want them to give or remove access from.\nWe can leverage on AWS IAM Groups to easily add or remove users and give them permission to whole cluster, or just part of it depending on which groups they belong to.\nIn this lesson, we will create 3 IAM roles that we will map to 3 IAM groups.\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/add_nodegroup_fargate/",
	"title": "EKS Fargate and Observability setup",
	"tags": [],
	"description": "",
	"content": "In this chapter, we will perform the following tasks in your existing EKS cluster eksworkshop-eksctl.\n Create Fargate Profile Enable OIDC Provider Create Namespace for Application Deployment Create IRSA (IAM Role for Service Account) for Application Namespace prodcatalog-ns Enable Observability for Logs and Metrics  PreRequisite   We assume that we have an existing EKS Cluster eksworkshop-eksctl created from EKS Workshop.\n  We also assume that we have increased the disk size on your Cloud9 instance as we need to build docker images for our application.\n  We will be using AWS Console to navigate and explore resources in Amazon EKS, AWS App Mesh, Amazon Cloudwatch, AWS X-Ray in this workshop. So ensure that you have completed Console Credentials to get full access to your existing EKS Cluster eksworkshop-eksctl in the EKS console.\n  Check if AWS_REGION and ACCOUNT_ID are set correctly\ntest -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set test -n \u0026#34;$ACCOUNT_ID\u0026#34; \u0026amp;\u0026amp; echo ACCOUNT_ID is \u0026#34;$ACCOUNT_ID\u0026#34; || echo ACCOUNT_ID is not set If not, export the ACCOUNT_ID and AWS_REGION to ENV export ACCOUNT_ID=\u0026lt;your_account_id\u0026gt; export AWS_REGION=\u0026lt;your_aws_region\u0026gt;    Clone the repository to your local workspace with this command:\n  cd ~/environment git clone https://github.com/aws-containers/eks-app-mesh-polyglot-demo.git cd eks-app-mesh-polyglot-demo Now lets, create the Fargate Profile in our EKS cluster to deploy one prodcatalog service in our Product Catalog Application.\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/port_to_app_mesh/mesh_resources_overview/",
	"title": "Mesh Resources and Design",
	"tags": [],
	"description": "",
	"content": "App Mesh Design In the image above you see all the services in Product Catalog Application are running within App Mesh. Each of the services has a VirtualNode defined (frontend-node, prodcatalog, and proddetail-v1), as well as VirtualService (fontend-node, prodcatalog and proddetail). These VirtualServices send traffic to VirtualRouters within the mesh, which in turn specify routing rules. This drives traffic to their respective VirtualNodes and ultimately to the service endpoints within Kubernetes.\nHow will it be different using App Mesh?  Functionally, the mesh-enabled version will do exactly what the current version does;  requests made by frontend-node will be served by the prodcatalog backend service; requests made by prodcatalog will be served by the proddetail-v1 backend service;   The difference will be that we\u0026rsquo;ll use AWS App Mesh to create new Virtual Services called prodcatalog and proddetail.  Requests made by frontend-node service will logically send traffic to VirtualRouter instances which will be configured to route traffic to the service endpoints within your cluster, to prodcatalog. Requests made by prodcatalog service will logically send traffic to VirtualRouter instances which will be configured to route traffic to the service endpoints within your cluster, to proddetail-v1.    App Mesh Resources Mesh To port the Product Catalog Apps to App Mesh, first you will need create a mesh. You\u0026rsquo;ll also apply labels to the prodcatalog-ns namespace to affiliate your new mesh with it, and to enable automatic sidecar injection for pods within it. Also add the the gateway label which we will use in next chapter for setting up VirtualGateway. Looking at the section of mesh.yaml shown below, you can see we\u0026rsquo;ve added the required labels to the prodcatalog-ns namespace and specified our mesh named prodcatalog-mesh.\n--- apiVersion: v1 kind: Namespace metadata: name: prodcatalog-ns labels: mesh: prodcatalog-mesh gateway: ingress-gw appmesh.k8s.aws/sidecarInjectorWebhook: enabled --- apiVersion: appmesh.k8s.aws/v1beta2 kind: Mesh metadata: name: prodcatalog-mesh spec: namespaceSelector: matchLabels: mesh: prodcatalog-mesh ---  VirtualNode Kubernetes application objects that run within App Mesh must be defined as VirtualNode. This provides App Mesh an abstraction to objects such as Kubernetes Deployments and Services, and provides endpoints for communication and routing configuration. Looking at the meshed_app.yaml, below is the frontend-node service\u0026rsquo;s VirtualNode specification.\n--- apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualNode metadata: name: frontend-node namespace: prodcatalog-ns spec: podSelector: matchLabels: app: frontend-node version: v1 listeners: - portMapping: port: 9000 protocol: http backends: - virtualService: virtualServiceRef: name: prodcatalog - virtualService: virtualServiceRef: name: prodsummary serviceDiscovery: dns: hostname: frontend-node.prodcatalog-ns.svc.cluster.local logging: accessLog: file: path: /dev/stdout ---  Note that it uses a podSelector to identify which Pods are members of this VirtualNode, as well as a pointer to the frontend-node Service.\nVirtualService and VirtualRouter There are also VirtualService and VirtualRouter specifications for each of the product catalog detail versions, establishing traffic routing to their respective endpoints. This is accomplished by adding Routes which point to the proddetail-v1 virtual nodes. App Mesh also provides the VirtualService construct which allows you to specify a logical service path for application traffic. In this example, they send traffic to VirtualRouters, which then route traffic to the VirtualNodes. Looking at the meshed_app.yaml, below is the proddetail VirtualService and VirtualRouter which will route the traffic to version 1 of backend service proddetail-v1.\n--- apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualService metadata: name: proddetail namespace: prodcatalog-ns spec: awsName: proddetail.prodcatalog-ns.svc.cluster.local provider: virtualRouter: virtualRouterRef: name: proddetail-router --- apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualRouter metadata: name: proddetail-router namespace: prodcatalog-ns spec: listeners: - portMapping: port: 3000 protocol: http routes: - name: proddetail-route httpRoute: match: prefix: / action: weightedTargets: - virtualNodeRef: name: proddetail-v1 weight: 100 ---  With the basic constructs understood, it\u0026rsquo;s time to create the mesh and its resources.\n"
},
{
	"uri": "/advanced/420_kubeflow/install/",
	"title": "Install",
	"tags": [],
	"description": "",
	"content": "In this chapter, we will install Kubeflow on Amazon EKS cluster. If you don\u0026rsquo;t have an EKS cluster, please follow instructions from getting started guide and then launch your EKS cluster using eksctl chapter\nIncrease cluster size We need more resources for completing the Kubeflow chapter of the EKS Workshop. First, we\u0026rsquo;ll increase the size of our cluster to 6 nodes\nexport NODEGROUP_NAME=$(eksctl get nodegroups --cluster eksworkshop-eksctl -o json | jq -r '.[0].Name') eksctl scale nodegroup --cluster eksworkshop-eksctl --name $NODEGROUP_NAME --nodes 6 --nodes-max 6  Scaling the nodegroup will take 2 - 3 minutes.\n Install Kubeflow on Amazon EKS curl --silent --location \u0026quot;https://github.com/kubeflow/kfctl/releases/download/v1.0.1/kfctl_v1.0.1-0-gf3edb9b_linux.tar.gz\u0026quot; | tar xz -C /tmp sudo mv -v /tmp/kfctl /usr/local/bin Setup your configuration Next step is to export environment variables needed for Kubeflow install. We chose default kfctl configuration file for simplicity of workshop experience. However, we recommend to install Cognito configuration and add authentication and SSL (via ACM) for production. For additional steps needed to enable Cognito, please follow Kubeflow documentation\n cat \u0026lt;\u0026lt; EoF \u0026gt; kf-install.sh export AWS_CLUSTER_NAME=eksworkshop-eksctl export KF_NAME=\\${AWS_CLUSTER_NAME} export BASE_DIR=${HOME}/environment export KF_DIR=\\${BASE_DIR}/\\${KF_NAME} # export CONFIG_URI=\u0026quot;https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_aws_cognito.v1.0.1.yaml\u0026quot; export CONFIG_URI=\u0026quot;https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_aws.v1.0.1.yaml\u0026quot; export CONFIG_FILE=\\${KF_DIR}/kfctl_aws.yaml EoF source kf-install.sh Create Kubeflow setup directory\nmkdir -p ${KF_DIR} cd ${KF_DIR} Download configuration file\nwget -O kfctl_aws.yaml $CONFIG_URI We will use IAM Roles for Service Account in our configuration. IAM Roles for Service Account offers fine grained access control so that when Kubeflow interacts with AWS resources (such as ALB creation), it will use roles that are pre-defined by kfctl. kfctl will setup OIDC Identity Provider for your EKS cluster and create two IAM roles (kf-admin-${AWS_CLUSTER_NAME} and kf-user-${AWS_CLUSTER_NAME}) in your account. kfctl will then build trust relationship between OIDC endpoint and Kubernetes Service Accounts (SA) so that only SA can perform actions that are defined in the IAM role. Because we are using this feature, we will disable using IAM roles defined at the Worker nodes. In addition, we will replace EKS Cluster Name and AWS Region in your $(CONFIG_FILE).\nsed -i '/region: us-west-2/ a \\ enablePodIamPolicy: true' ${CONFIG_FILE} sed -i -e 's/kubeflow-aws/'\u0026quot;$AWS_CLUSTER_NAME\u0026quot;'/' ${CONFIG_FILE} sed -i \u0026quot;s@us-west-2@$AWS_REGION@\u0026quot; ${CONFIG_FILE} sed -i \u0026quot;s@roles:@#roles:@\u0026quot; ${CONFIG_FILE} sed -i \u0026quot;s@- eksctl-eksworkshop-eksctl-nodegroup-ng-a2-NodeInstanceRole-xxxxxxx@#- eksctl-eksworkshop-eksctl-nodegroup-ng-a2-NodeInstanceRole-xxxxxxx@\u0026quot; ${CONFIG_FILE} Until https://github.com/kubeflow/kubeflow/issues/3827 is fixed, install aws-iam-authenticator\ncurl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator chmod +x aws-iam-authenticator sudo mv aws-iam-authenticator /usr/local/bin Deploy Kubeflow Apply configuration and deploy Kubeflow on your cluster:\ncd ${KF_DIR} kfctl apply -V -f ${CONFIG_FILE} Run below command to check the status\nkubectl -n kubeflow get all  Installing Kubeflow and its toolset may take 2 - 3 minutes. Few pods may initially give Error or CrashLoopBackOff status. Give it some time, they will auto-heal and will come to Running state\n   Expand here to see the output   $ kubectl -n kubeflow get all NAME READY STATUS RESTARTS AGE pod/admission-webhook-bootstrap-stateful-set-0 1/1 Running 0 5m50s pod/admission-webhook-deployment-64cb96ddbf-x2zfm 1/1 Running 0 5m12s pod/alb-ingress-controller-c76dd95d-z2kc7 1/1 Running 0 5m45s pod/application-controller-stateful-set-0 1/1 Running 0 6m32s pod/argo-ui-778676df64-w4lpj 1/1 Running 0 5m51s pod/centraldashboard-7dd7dd685d-fjnr2 1/1 Running 0 5m51s pod/jupyter-web-app-deployment-89789fd5-pmwmf 1/1 Running 0 5m50s pod/katib-controller-6b789b6cb5-rc7xz 1/1 Running 1 5m48s pod/katib-db-manager-64f548b47c-6p6nv 1/1 Running 0 5m48s pod/katib-mysql-57884cb488-6g9zk 1/1 Running 0 5m48s pod/katib-ui-5c5cc6bd77-mwmrl 1/1 Running 0 5m48s pod/metacontroller-0 1/1 Running 0 5m51s pod/metadata-db-76c9f78f77-pjvh8 1/1 Running 0 5m49s pod/metadata-deployment-674fdd976b-946k6 1/1 Running 0 5m49s pod/metadata-envoy-deployment-5688989bd6-j5bdh 1/1 Running 0 5m49s pod/metadata-grpc-deployment-5579bdc87b-fc88k 1/1 Running 2 5m49s pod/metadata-ui-9b8cd699d-drm2p 1/1 Running 0 5m49s pod/minio-755ff748b-hdfwk 1/1 Running 0 5m47s pod/ml-pipeline-79b4f85cbc-hcttq 1/1 Running 5 5m47s pod/ml-pipeline-ml-pipeline-visualizationserver-5fdffdc5bf-nqjb5 1/1 Running 0 5m46s pod/ml-pipeline-persistenceagent-645cb66874-rgrt4 1/1 Running 1 5m47s pod/ml-pipeline-scheduledworkflow-6c978b6b85-dxgw4 1/1 Running 0 5m46s pod/ml-pipeline-ui-6995b7bccf-ktwb2 1/1 Running 0 5m47s pod/ml-pipeline-viewer-controller-deployment-8554dc7b9f-n4ccc 1/1 Running 0 5m46s pod/mpi-operator-5bf8b566b7-gkbz9 1/1 Running 0 5m45s pod/mysql-598bc897dc-srtpt 1/1 Running 0 5m47s pod/notebook-controller-deployment-7db57b9ccf-4pqkw 1/1 Running 0 5m49s pod/nvidia-device-plugin-daemonset-4s9tv 1/1 Running 0 5m46s pod/nvidia-device-plugin-daemonset-5p8kn 1/1 Running 0 5m46s pod/nvidia-device-plugin-daemonset-84jv6 1/1 Running 0 5m46s pod/nvidia-device-plugin-daemonset-d7x5f 1/1 Running 0 5m46s pod/nvidia-device-plugin-daemonset-m8cpr 1/1 Running 0 5m46s pod/profiles-deployment-b45dbc6f-7jfqw 2/2 Running 0 5m46s pod/pytorch-operator-5fd5f94bdd-dbddk 1/1 Running 0 5m49s pod/seldon-controller-manager-679fc777cd-58vzl 1/1 Running 0 5m45s pod/spark-operatorcrd-cleanup-tc4nw 0/2 Completed 0 5m50s pod/spark-operatorsparkoperator-c7b64b87f-w6glw 1/1 Running 0 5m50s pod/spartakus-volunteer-5b7d86d9cd-2z4dn 1/1 Running 0 5m49s pod/tensorboard-6544748d94-dr87g 1/1 Running 0 5m48s pod/tf-job-operator-7d7c8fb8bb-bh2j9 1/1 Running 0 5m48s pod/workflow-controller-945c84565-ctx84 1/1 Running 0 5m51s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/admission-webhook-service ClusterIP 10.100.34.137 \u0026lt;none\u0026gt; 443/TCP 5m50s service/application-controller-service ClusterIP 10.100.122.252 \u0026lt;none\u0026gt; 443/TCP 6m32s service/argo-ui NodePort 10.100.56.77 \u0026lt;none\u0026gt; 80:32722/TCP 5m51s service/centraldashboard ClusterIP 10.100.122.184 \u0026lt;none\u0026gt; 80/TCP 5m51s service/jupyter-web-app-service ClusterIP 10.100.184.50 \u0026lt;none\u0026gt; 80/TCP 5m50s service/katib-controller ClusterIP 10.100.96.16 \u0026lt;none\u0026gt; 443/TCP,8080/TCP 5m48s service/katib-db-manager ClusterIP 10.100.161.38 \u0026lt;none\u0026gt; 6789/TCP 5m48s service/katib-mysql ClusterIP 10.100.186.115 \u0026lt;none\u0026gt; 3306/TCP 5m48s service/katib-ui ClusterIP 10.100.110.39 \u0026lt;none\u0026gt; 80/TCP 5m48s service/metadata-db ClusterIP 10.100.92.177 \u0026lt;none\u0026gt; 3306/TCP 5m49s service/metadata-envoy-service ClusterIP 10.100.17.145 \u0026lt;none\u0026gt; 9090/TCP 5m49s service/metadata-grpc-service ClusterIP 10.100.238.212 \u0026lt;none\u0026gt; 8080/TCP 5m49s service/metadata-service ClusterIP 10.100.183.244 \u0026lt;none\u0026gt; 8080/TCP 5m49s service/metadata-ui ClusterIP 10.100.28.97 \u0026lt;none\u0026gt; 80/TCP 5m49s service/minio-service ClusterIP 10.100.185.36 \u0026lt;none\u0026gt; 9000/TCP 5m48s service/ml-pipeline ClusterIP 10.100.45.162 \u0026lt;none\u0026gt; 8888/TCP,8887/TCP 5m48s service/ml-pipeline-ml-pipeline-visualizationserver ClusterIP 10.100.211.60 \u0026lt;none\u0026gt; 8888/TCP 5m47s service/ml-pipeline-tensorboard-ui ClusterIP 10.100.150.113 \u0026lt;none\u0026gt; 80/TCP 5m47s service/ml-pipeline-ui ClusterIP 10.100.135.60 \u0026lt;none\u0026gt; 80/TCP 5m47s service/mysql ClusterIP 10.100.37.144 \u0026lt;none\u0026gt; 3306/TCP 5m48s service/notebook-controller-service ClusterIP 10.100.250.183 \u0026lt;none\u0026gt; 443/TCP 5m49s service/profiles-kfam ClusterIP 10.100.24.246 \u0026lt;none\u0026gt; 8081/TCP 5m47s service/pytorch-operator ClusterIP 10.100.104.208 \u0026lt;none\u0026gt; 8443/TCP 5m49s service/seldon-webhook-service ClusterIP 10.100.68.153 \u0026lt;none\u0026gt; 443/TCP 5m46s service/tensorboard ClusterIP 10.100.25.5 \u0026lt;none\u0026gt; 9000/TCP 5m49s service/tf-job-operator ClusterIP 10.100.165.41 \u0026lt;none\u0026gt; 8443/TCP 5m48s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/nvidia-device-plugin-daemonset 5 5 5 5 5 \u0026lt;none\u0026gt; 5m46s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/admission-webhook-deployment 1/1 1 1 5m50s deployment.apps/alb-ingress-controller 1/1 1 1 5m46s deployment.apps/argo-ui 1/1 1 1 5m51s deployment.apps/centraldashboard 1/1 1 1 5m51s deployment.apps/jupyter-web-app-deployment 1/1 1 1 5m50s deployment.apps/katib-controller 1/1 1 1 5m48s deployment.apps/katib-db-manager 1/1 1 1 5m48s deployment.apps/katib-mysql 1/1 1 1 5m48s deployment.apps/katib-ui 1/1 1 1 5m48s deployment.apps/metadata-db 1/1 1 1 5m49s deployment.apps/metadata-deployment 1/1 1 1 5m49s deployment.apps/metadata-envoy-deployment 1/1 1 1 5m49s deployment.apps/metadata-grpc-deployment 1/1 1 1 5m49s deployment.apps/metadata-ui 1/1 1 1 5m49s deployment.apps/minio 1/1 1 1 5m48s deployment.apps/ml-pipeline 1/1 1 1 5m48s deployment.apps/ml-pipeline-ml-pipeline-visualizationserver 1/1 1 1 5m47s deployment.apps/ml-pipeline-persistenceagent 1/1 1 1 5m48s deployment.apps/ml-pipeline-scheduledworkflow 1/1 1 1 5m47s deployment.apps/ml-pipeline-ui 1/1 1 1 5m47s deployment.apps/ml-pipeline-viewer-controller-deployment 1/1 1 1 5m47s deployment.apps/mpi-operator 1/1 1 1 5m46s deployment.apps/mysql 1/1 1 1 5m48s deployment.apps/notebook-controller-deployment 1/1 1 1 5m49s deployment.apps/profiles-deployment 1/1 1 1 5m47s deployment.apps/pytorch-operator 1/1 1 1 5m49s deployment.apps/seldon-controller-manager 1/1 1 1 5m46s deployment.apps/spark-operatorsparkoperator 1/1 1 1 5m50s deployment.apps/spartakus-volunteer 1/1 1 1 5m49s deployment.apps/tensorboard 1/1 1 1 5m49s deployment.apps/tf-job-operator 1/1 1 1 5m48s deployment.apps/workflow-controller 1/1 1 1 5m51s NAME DESIRED CURRENT READY AGE replicaset.apps/admission-webhook-deployment-64cb96ddbf 1 1 1 5m50s replicaset.apps/alb-ingress-controller-c76dd95d 1 1 1 5m45s replicaset.apps/argo-ui-778676df64 1 1 1 5m51s replicaset.apps/centraldashboard-7dd7dd685d 1 1 1 5m51s replicaset.apps/jupyter-web-app-deployment-89789fd5 1 1 1 5m50s replicaset.apps/katib-controller-6b789b6cb5 1 1 1 5m48s replicaset.apps/katib-db-manager-64f548b47c 1 1 1 5m48s replicaset.apps/katib-mysql-57884cb488 1 1 1 5m48s replicaset.apps/katib-ui-5c5cc6bd77 1 1 1 5m48s replicaset.apps/metadata-db-76c9f78f77 1 1 1 5m49s replicaset.apps/metadata-deployment-674fdd976b 1 1 1 5m49s replicaset.apps/metadata-envoy-deployment-5688989bd6 1 1 1 5m49s replicaset.apps/metadata-grpc-deployment-5579bdc87b 1 1 1 5m49s replicaset.apps/metadata-ui-9b8cd699d 1 1 1 5m49s replicaset.apps/minio-755ff748b 1 1 1 5m47s replicaset.apps/ml-pipeline-79b4f85cbc 1 1 1 5m47s replicaset.apps/ml-pipeline-ml-pipeline-visualizationserver-5fdffdc5bf 1 1 1 5m46s replicaset.apps/ml-pipeline-persistenceagent-645cb66874 1 1 1 5m47s replicaset.apps/ml-pipeline-scheduledworkflow-6c978b6b85 1 1 1 5m46s replicaset.apps/ml-pipeline-ui-6995b7bccf 1 1 1 5m47s replicaset.apps/ml-pipeline-viewer-controller-deployment-8554dc7b9f 1 1 1 5m46s replicaset.apps/mpi-operator-5bf8b566b7 1 1 1 5m45s replicaset.apps/mysql-598bc897dc 1 1 1 5m47s replicaset.apps/notebook-controller-deployment-7db57b9ccf 1 1 1 5m49s replicaset.apps/profiles-deployment-b45dbc6f 1 1 1 5m46s replicaset.apps/pytorch-operator-5fd5f94bdd 1 1 1 5m49s replicaset.apps/seldon-controller-manager-679fc777cd 1 1 1 5m45s replicaset.apps/spark-operatorsparkoperator-c7b64b87f 1 1 1 5m50s replicaset.apps/spartakus-volunteer-5b7d86d9cd 1 1 1 5m49s replicaset.apps/tensorboard-6544748d94 1 1 1 5m48s replicaset.apps/tf-job-operator-7d7c8fb8bb 1 1 1 5m48s replicaset.apps/workflow-controller-945c84565 1 1 1 5m51s NAME READY AGE statefulset.apps/admission-webhook-bootstrap-stateful-set 1/1 5m50s statefulset.apps/application-controller-stateful-set 1/1 6m32s statefulset.apps/metacontroller 1/1 5m51s NAME COMPLETIONS DURATION AGE job.batch/spark-operatorcrd-cleanup 1/1 42s 5m50s    "
},
{
	"uri": "/beginner/130_exposing-service/connecting/",
	"title": "Connecting Applications with Services",
	"tags": [],
	"description": "",
	"content": "Before discussing the Kubernetes approach to networking, it is worthwhile to contrast it with the ‚Äúnormal‚Äù way networking works with Docker.\nBy default, Docker uses host-private networking, so containers can talk to other containers only if they are on the same machine. In order for Docker containers to communicate across nodes, there must be allocated ports on the machine‚Äôs own IP address, which are then forwarded or proxied to the containers. This obviously means that containers must either coordinate which ports they use very carefully or ports must be allocated dynamically.\nCoordinating ports across multiple developers is very difficult to do at scale and exposes users to cluster-level issues outside of their control. Kubernetes assumes that pods can communicate with other pods, regardless of which host they land on. We give every pod its own cluster-private-IP address so you do not need to explicitly create links between pods or map container ports to host ports. This means that containers within a Pod can all reach each other‚Äôs ports on localhost, and all pods in a cluster can see each other without NAT.\nExposing pods to the cluster If you created a default deny policy in the previous section, delete it by running:\nif [ -f ~/environment/calico_resources/default-deny.yaml ]; then kubectl delete -f ~/environment/calico_resources/default-deny.yaml fi Create a nginx deployment, and note that it has a container port specification:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/run-my-nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx namespace: my-nginx spec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 EoF This makes it accessible from any node in your cluster. Check the nodes the Pod is running on:\n# create the namespace kubectl create ns my-nginx # create the nginx deployment with 2 replicas kubectl -n my-nginx apply -f ~/environment/run-my-nginx.yaml kubectl -n my-nginx get pods -o wide The output being something like this: NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE my-nginx-756f645cd7-gsl4g 1/1 Running 0 63s 192.168.59.188 ip-192-168-38-150.us-west-2.compute.internal \u0026lt;none\u0026gt; my-nginx-756f645cd7-t8b6w 1/1 Running 0 63s 192.168.79.210 ip-192-168-92-222.us-west-2.compute.internal \u0026lt;none\u0026gt;  Check your pods‚Äô IPs:\nkubectl -n my-nginx get pods -o yaml | grep \u0026#39;podIP:\u0026#39; Output being like: podIP: 192.168.59.188 podIP: 192.168.79.210  Creating a Service So we have pods running nginx in a flat, cluster wide, address space. In theory, you could talk to these pods directly, but what happens when a node dies? The pods die with it, and the Deployment will create new ones, with different IPs. This is the problem a Service solves.\nA Kubernetes Service is an abstraction which defines a logical set of Pods running somewhere in your cluster, that all provide the same functionality. When created, each Service is assigned a unique IP address (also called clusterIP). This address is tied to the lifespan of the Service, and will not change while the Service is alive. Pods can be configured to talk to the Service, and know that communication to the Service will be automatically load-balanced out to some pod that is a member of the Service.\nYou can create a Service for your 2 nginx replicas with kubectl expose:\nkubectl -n my-nginx expose deployment/my-nginx Output: service/my-nginx exposed  This specification will create a Service which targets TCP port 80 on any Pod with the run: my-nginx label, and expose it on an abstracted Service port (targetPort: is the port the container accepts traffic on, port: is the abstracted Service port, which can be any port other pods use to access the Service). View Service API object to see the list of supported fields in service definition. Check your Service:\nkubectl -n my-nginx get svc my-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx ClusterIP 10.100.225.196 \u0026lt;none\u0026gt; 80/TCP 25s  As mentioned previously, a Service is backed by a group of Pods. These Pods are exposed through endpoints. The Service‚Äôs selector will be evaluated continuously and the results will be POSTed to an Endpoints object also named my-nginx. When a Pod dies, it is automatically removed from the endpoints, and new Pods matching the Service‚Äôs selector will automatically get added to the endpoints. Check the endpoints, and note that the IPs are the same as the Pods created in the first step:\nkubectl -n my-nginx describe svc my-nginx Name: my-nginx Namespace: my-nginx Labels: run=my-nginx Annotations: \u0026lt;none\u0026gt; Selector: run=my-nginx Type: ClusterIP IP: 10.100.225.196 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP Endpoints: 192.168.59.188:80,192.168.79.210:80 Session Affinity: None Events: \u0026lt;none\u0026gt;  You should now be able to curl the nginx Service on CLUSTER-IP: PORT from any pods in your cluster.\nThe Service IP is completely virtual, it never hits the wire.\n Let\u0026rsquo;s try that by :\nSetting a variable called MyClusterIP with the my-nginx Service IP.\n# Create a variable set with the my-nginx service IP export MyClusterIP=$(kubectl -n my-nginx get svc my-nginx -ojsonpath=\u0026#39;{.spec.clusterIP}\u0026#39;) Creating a new deployment called load-generator (with the MyClusterIP variable also set inside the container) and get an interactive shell on a pod + container.\n# Create a new deployment and allocate a TTY for the container in the pod kubectl -n my-nginx run --generator=run-pod/v1 -i --tty load-generator --env=\u0026#34;MyClusterIP=${MyClusterIP}\u0026#34; --image=busybox /bin/sh  Click here for more information on the -env parameter.\n Connecting to the nginx welcome page using the ClusterIP.\nwget -q -O - ${MyClusterIP} | grep \u0026#39;\u0026lt;title\u0026gt;\u0026#39; The output will be\n\u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt;  Type exit to log out of the container:\nexit "
},
{
	"uri": "/beginner/190_ocean/register/",
	"title": "Create a Free Spot.io Account",
	"tags": [],
	"description": "",
	"content": "In this section, you will create a free Spot.io account, and subsequently link that account to your AWS account. A video tutorial covering this section can be found at the bottom of the page.\nCreating a Spot.io Account Begin by heading over to spot.io and clicking the \u0026ldquo;Get Started for Free\u0026rdquo; button on the top right. This will take you to the sign up page, where you create a free account.\nYou will need to provide the following:\n Full Name Company Email Country  If you are running this workshop on your own, you will be able to keep your account, as it is free of charge. For more information see https://spot.io/pricing/.\n Next, set your password, and verify your email by clicking the link in the verification email you will receive. This brings you to the console login page.\nOnce you‚Äôre logged in, you will find yourself in the Console.\nConnecting Your AWS Account Now, you will link your AWS account in order to enable the Spot.io platform to manage AWS resources on your behalf.\n Prior to connecting your AWS account, you can access a preview of the console, by clicking on ‚ÄúGet a Console Walkthrough‚Äú.\n 3. Complete the process by following the on-screen instructions. This will create the IAM Role \u0026amp; Policy necessary for the management of resources on your behalf. Step 4 should be automated by the CFN stack, you should find your console ready for use once the stack has been created.\n Feel free to follow the video tutorial below:\n  "
},
{
	"uri": "/beginner/191_secrets/create-a-secret/",
	"title": "Create a Secret",
	"tags": [],
	"description": "",
	"content": "Encrypt Your Secret Create a namespace for this exercise:\nkubectl create ns secretslab Output:\nnamespace/secretslab created  Create a text file containing your secret:\necho -n \u0026#34;am i safe?\u0026#34; \u0026gt; ./test-creds Create your secret\nkubectl create secret \\  generic test-creds \\  --from-file=test-creds=./test-creds \\  --namespace secretslab Output:\nsecret/test-creds created  Retrieve the secret via the CLI:\nkubectl get secret test-creds \\  -o jsonpath=\u0026#34;{.data.test-creds}\u0026#34; \\  --namespace secretslab | \\  base64 --decode Output: am i safe?  At the conclusion of this lab, we will validate the Decrypt API call in CloudTrail. It will take some time for the event to be viewable in CloudTrail. So, let\u0026rsquo;s go to the next step and attempt to retrieve the secret using a Kubernetes pod.\n"
},
{
	"uri": "/intermediate/270_custom_resource_definition/creating_crd/",
	"title": "Creating a CRD",
	"tags": [],
	"description": "",
	"content": "When you create a new CustomResourceDefinition (CRD), the Kubernetes API Server creates a new RESTful resource path for each version you specify. The CRD can be either namespaced or cluster-scoped, as specified in the CRD‚Äôs scope field. As with existing built-in objects, deleting a namespace deletes all custom objects in that namespace. CustomResourceDefinitions themselves are non-namespaced and are available to all namespaces.\nFor example, if you save the following CustomResourceDefinition to resourcedefinition.yaml:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/resourcedefinition.yaml apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: \u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt; name: crontabs.stable.example.com spec: # group name to use for REST API: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; group: stable.example.com # list of versions supported by this CustomResourceDefinition versions: - name: v1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: - ct EoF And create it:\nkubectl apply -f ~/environment/resourcedefinition.yaml It might take a few seconds for the endpoint to be created. You can also watch the Established condition of your CustomResourceDefinition to be true or watch the discovery information of the API server for your resource to show up.\nNow, let\u0026rsquo;s check the recently created CRD.\nkubectl get crd crontabs.stable.example.com The result will be something like this:\nNAME CREATED AT crontabs.stable.example.com 2019-05-09T16:50:55Z Now, let\u0026rsquo;s see the Custom Resource in detail:\nkubectl describe crd crontabs.stable.example.com The output:\nName: crontabs.stable.example.com Namespace: Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026quot;apiVersion\u0026quot;:\u0026quot;apiextensions.k8s.io/v1beta1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;CustomResourceDefinition\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;crontabs.stable.example.com\u0026quot;,\u0026quot;names... API Version: apiextensions.k8s.io/v1beta1 Kind: CustomResourceDefinition Metadata: Creation Timestamp: 2019-05-09T16:50:55Z Generation: 1 Resource Version: 3193124 Self Link: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/crontabs.stable.example.com UID: 9cad2caf-727a-11e9-9fb0-0e8a8b871ace Spec: Additional Printer Columns: JSON Path: .metadata.creationTimestamp Description: CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC. Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata Name: Age Type: date Group: stable.example.com Names: Kind: CronTab List Kind: CronTabList Plural: crontabs Short Names: ct Singular: crontab Scope: Namespaced Version: v1 Versions: Name: v1 Served: true Storage: true Status: Accepted Names: Kind: CronTab List Kind: CronTabList Plural: crontabs Short Names: ct Singular: crontab Conditions: Last Transition Time: 2019-05-09T16:50:55Z Message: no conflicts found Reason: NoConflicts Status: True Type: NamesAccepted Last Transition Time: \u0026lt;nil\u0026gt; Message: the initial names have been accepted Reason: InitialNamesAccepted Status: True Type: Established Stored Versions: v1 Events: \u0026lt;none\u0026gt; Or we can check the resource directly from the Kubernetes API. First, we start the proxy in one tab of the Cloud9 environment:\nkubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true And in another tab we check the existance of the Custom Resource\ncurl -i 127.0.0.1:8080/apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/crontabs.stable.example.com The response being something like this:\nHTTP/1.1 200 OK Audit-Id: ec046098-8373-4c74-8ce7-a6a43951df6e Content-Length: 2582 Content-Type: application/json Date: Thu, 09 May 2019 18:07:05 GMT { \u0026quot;kind\u0026quot;: \u0026quot;CustomResourceDefinition\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;apiextensions.k8s.io/v1beta1\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;crontabs.stable.example.com\u0026quot;, \u0026quot;selfLink\u0026quot;: \u0026quot;/apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/crontabs.stable.example.com\u0026quot;, \u0026quot;uid\u0026quot;: \u0026quot;24babfb5-7285-11e9-a54d-0615623ca50e\u0026quot;, \u0026quot;resourceVersion\u0026quot;: \u0026quot;3271016\u0026quot;, \u0026quot;generation\u0026quot;: 1, \u0026quot;creationTimestamp\u0026quot;: \u0026quot;2019-05-09T18:06:18Z\u0026quot;, \u0026quot;annotations\u0026quot;: { \u0026quot;kubectl.kubernetes.io/last-applied-configuration\u0026quot;: \u0026quot;{\\\u0026quot;apiVersion\\\u0026quot;:\\\u0026quot;apiextensions.k8s.io/v1beta1\\\u0026quot;,\\\u0026quot;kind\\\u0026quot;:\\\u0026quot;CustomResourceDefinition\\\u0026quot;,\\\u0026quot;metadata\\\u0026quot;:{\\\u0026quot;annotations\\\u0026quot;:{},\\\u0026quot;name\\\u0026quot;:\\\u0026quot;crontabs.stable.example.com\\\u0026quot;,\\\u0026quot;namespace\\\u0026quot;:\\\u0026quot;\\\u0026quot;},\\\u0026quot;spec\\\u0026quot;:{\\\u0026quot;group\\\u0026quot;:\\\u0026quot;stable.example.com\\\u0026quot;,\\\u0026quot;names\\\u0026quot;:{\\\u0026quot;kind\\\u0026quot;:\\\u0026quot;CronTab\\\u0026quot;,\\\u0026quot;plural\\\u0026quot;:\\\u0026quot;crontabs\\\u0026quot;,\\\u0026quot;shortNames\\\u0026quot;:[\\\u0026quot;ct\\\u0026quot;],\\\u0026quot;singular\\\u0026quot;:\\\u0026quot;crontab\\\u0026quot;},\\\u0026quot;scope\\\u0026quot;:\\\u0026quot;Namespaced\\\u0026quot;,\\\u0026quot;versions\\\u0026quot;:[{\\\u0026quot;name\\\u0026quot;:\\\u0026quot;v1\\\u0026quot;,\\\u0026quot;served\\\u0026quot;:true,\\\u0026quot;storage\\\u0026quot;:true}]}}\\n\u0026quot; } }, \u0026quot;spec\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;stable.example.com\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;names\u0026quot;: { \u0026quot;plural\u0026quot;: \u0026quot;crontabs\u0026quot;, \u0026quot;singular\u0026quot;: \u0026quot;crontab\u0026quot;, \u0026quot;shortNames\u0026quot;: [ \u0026quot;ct\u0026quot; ], \u0026quot;kind\u0026quot;: \u0026quot;CronTab\u0026quot;, \u0026quot;listKind\u0026quot;: \u0026quot;CronTabList\u0026quot; }, \u0026quot;scope\u0026quot;: \u0026quot;Namespaced\u0026quot;, \u0026quot;versions\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;served\u0026quot;: true, \u0026quot;storage\u0026quot;: true } ], \u0026quot;additionalPrinterColumns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;Age\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.\\n\\nPopulated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata\u0026quot;, \u0026quot;JSONPath\u0026quot;: \u0026quot;.metadata.creationTimestamp\u0026quot; } ] }, \u0026quot;status\u0026quot;: { \u0026quot;conditions\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;NamesAccepted\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-05-09T18:06:18Z\u0026quot;, \u0026quot;reason\u0026quot;: \u0026quot;NoConflicts\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;no conflicts found\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;Established\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;lastTransitionTime\u0026quot;: null, \u0026quot;reason\u0026quot;: \u0026quot;InitialNamesAccepted\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;the initial names have been accepted\u0026quot; } ], \u0026quot;acceptedNames\u0026quot;: { \u0026quot;plural\u0026quot;: \u0026quot;crontabs\u0026quot;, \u0026quot;singular\u0026quot;: \u0026quot;crontab\u0026quot;, \u0026quot;shortNames\u0026quot;: [ \u0026quot;ct\u0026quot; ], \u0026quot;kind\u0026quot;: \u0026quot;CronTab\u0026quot;, \u0026quot;listKind\u0026quot;: \u0026quot;CronTabList\u0026quot; }, \u0026quot;storedVersions\u0026quot;: [ \u0026quot;v1\u0026quot; ] } } "
},
{
	"uri": "/beginner/190_efs/launching-efs/",
	"title": "Creating an EFS File System",
	"tags": [],
	"description": "",
	"content": "An EFS file system may be created and configured either from the AWS Management Console or using AWS CLI. An EFS file system may be accessed concurrently by worker nodes (EC2 instances) running inside the EKS cluster VPC. Instances connect to a file system by using a network interface called a mount target.\nFirst, let\u0026rsquo;s define a set of environment variables pertaining to the name of your EKS cluster, VPC where it is deployed and the IPv4 CIDR block associated with that VPC.\nCLUSTER_NAME=eksworkshop-eksctl VPC_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --query \u0026quot;cluster.resourcesVpcConfig.vpcId\u0026quot; --output text) CIDR_BLOCK=$(aws ec2 describe-vpcs --vpc-ids $VPC_ID --query \u0026quot;Vpcs[].CidrBlock\u0026quot; --output text) Next, create a security group to be associated with the mount targets. Then, add an ingress rule to this security group that allows all inbound traffic using NFS protocol on port 2049 from IP addresses that belong to the CIDR block of the EKS cluster VPC. This rule will allow NFS access to the file system from all worker nodes in the EKS cluster.\nMOUNT_TARGET_GROUP_NAME=\u0026quot;eks-efs-group\u0026quot; MOUNT_TARGET_GROUP_DESC=\u0026quot;NFS access to EFS from EKS worker nodes\u0026quot; MOUNT_TARGET_GROUP_ID=$(aws ec2 create-security-group --group-name $MOUNT_TARGET_GROUP_NAME --description \u0026quot;$MOUNT_TARGET_GROUP_DESC\u0026quot; --vpc-id $VPC_ID | jq --raw-output '.GroupId') aws ec2 authorize-security-group-ingress --group-id $MOUNT_TARGET_GROUP_ID --protocol tcp --port 2049 --cidr $CIDR_BLOCK Now, create an EFS file system.\nFILE_SYSTEM_ID=$(aws efs create-file-system | jq --raw-output '.FileSystemId') Check the LifeCycleState of the file system using the following command and wait until it changes from creating to available before you proceed to the next step.\naws efs describe-file-systems --file-system-id $FILE_SYSTEM_ID The EKS cluster that you created comprises worker nodes that are resident in the public subnets of the cluster VPC. Each public subnet resides in a different Availability Zone. As mentioned earlier, worker nodes connect to an EFS file system by using a mount target. It is best to create a mount target in each of the EKS cluster VPC\u0026rsquo;s Availability Zones so that worker nodes across your EKS cluster can all have access to the file system.\nThe following set of commands identifies the public subnets in your cluster VPC and creates a mount target in each one of them as well as associate that mount target with the security group you created above.\nTAG1=tag:alpha.eksctl.io/cluster-name TAG2=tag:kubernetes.io/role/elb subnets=($(aws ec2 describe-subnets --filters \u0026quot;Name=$TAG1,Values=$CLUSTER_NAME\u0026quot; \u0026quot;Name=$TAG2,Values=1\u0026quot; | jq --raw-output '.Subnets[].SubnetId')) for subnet in ${subnets[@]} do echo \u0026quot;creating mount target in \u0026quot; $subnet aws efs create-mount-target --file-system-id $FILE_SYSTEM_ID --subnet-id $subnet --security-groups $MOUNT_TARGET_GROUP_ID done  When eksctl provisions your VPC and EKS cluster, it assigns the following tags to all public subnets in the cluster VPC. The above command leverages these tags to identify the public subnets.\nkubernetes.io/cluster/eksworkshop-eksctl = shared\nkubernetes.io/role/elb = 1\n Check the LifeCycleStateof the mount targets using the following command and wait until it changes from creatingto availablebefore you proceed to the next step. It will take a few minutes for all the mount targets to transition to available state. You may also check on the status of mount targets from the EFS Dashboard on the AWS Management Console. Select the file system you just created and then click on Manage network access to see the mount targets.\naws efs describe-mount-targets --file-system-id $FILE_SYSTEM_ID | jq --raw-output '.MountTargets[].LifeCycleState' "
},
{
	"uri": "/beginner/140_assigning_pods/node_selector/",
	"title": "nodeSelector",
	"tags": [],
	"description": "",
	"content": "nodeSelector is the simplest recommended form of node selection constraint. nodeSelector is a field of PodSpec. It specifies a map of key-value pairs. For the pod to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels (it can have additional labels as well). The most common usage is one key-value pair.\nAttach a label to the node Run kubectl get nodes to get the names of your cluster‚Äôs nodes.\nkubectl get nodes Output will look like:\nNAME STATUS ROLES AGE VERSION ip-192-168-155-36.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 2d23h v1.20.4-eks-6b7464 ip-192-168-168-110.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 2d23h v1.20.4-eks-6b7464 ip-192-168-97-12.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 2d23h v1.20.4-eks-6b7464  We will add a new label disktype=ssd to the first node on this list.\nBut first, let\u0026rsquo;s confirm the label hasn\u0026rsquo;t been assigned to any nodes by filtering the previous using the selector option.\nkubectl get nodes --selector disktype=ssd Output:\nNo resources found  To add a label to the first node, we can run these commands:\n# export the first node name as a variable export FIRST_NODE_NAME=$(kubectl get nodes -o json | jq -r \u0026#39;.items[0].metadata.name\u0026#39;) # add the label to the node kubectl label nodes ${FIRST_NODE_NAME} disktype=ssd Output will look like:\nnode/ip-192-168-155-36.us-east-2.compute.internal labeled  We can verify that it worked by re-running the kubectl get nodes --selector command.\nkubectl get nodes --selector disktype=ssd Output will look like:\nNAME STATUS ROLES AGE VERSION ip-192-168-155-36.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 2d23h v1.20.4-eks-6b7464  Deploy a nginx pod only to the node with the new label We will now create a simple pod creating a file with the nodeSelector in the pod spec.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/pod-nginx.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd EoF Then you run:\nkubectl apply -f ~/environment/pod-nginx.yaml The Pod will get scheduled on the node that you attached the label to. You can verify that it worked by running:\nkubectl get pods -o wide Output will look like:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 0/1 ContainerCreating 0 4s \u0026lt;none\u0026gt; ip-192-168-155-36.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  The NODE name should match the output of the command below:\nkubectl get nodes --selector disktype=ssd Sample Output\nNAME STATUS ROLES AGE VERSION ip-192-168-155-36.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 2d23h v1.20.4-eks-6b7464  "
},
{
	"uri": "/beginner/180_fargate/prerequisites/",
	"title": "Prerequisite",
	"tags": [],
	"description": "",
	"content": "AWS Fargate with Amazon EKS is currently available in the following Regions:\n   Region Name Region     US East (Ohio) us-east-2   US East (N. Virginia) us-east-1   US West (N. California) us-west-1   US West (Oregon) us-west-2   Africa (Cape Town) af-south-1   Asia Pacific (Hong Kong) ap-east-1   Asia Pacific (Mumbai) ap-south-1   Asia Pacific (Seoul) ap-northeast-2   Asia Pacific (Singapore) ap-southeast-1   Asia Pacific (Sydney) ap-southeast-2   Asia Pacific (Tokyo) ap-northeast-1 (apne1-az1, apne1-az2, \u0026amp; apne1-az4 only)   Canada (Central) ca-central-1   Europe (Frankfurt) eu-central-1   Europe (Ireland) eu-west-1   Europe (London) eu-west-2   Europe (Milan) eu-south-1   Europe (Paris) eu-west-3   Europe (Stockholm) eu-north-1   South America (S√£o Paulo) sa-east-1   Middle East (Bahrain) me-south-1    Don\u0026rsquo;t continue the lab unless you use one of these region.\n "
},
{
	"uri": "/beginner/160_advanced-networking/secondary_cidr/",
	"title": "Using Secondary CIDRs with EKS",
	"tags": [],
	"description": "",
	"content": "Using Secondary CIDRs with EKS You can expand your VPC network by adding additional CIDR ranges. This capability can be used if you are running out of IP ranges within your existing VPC or if you have consumed all available RFC 1918 CIDR ranges within your corporate network. EKS supports additional IPv4 CIDR blocks in the 100.64.0.0/10 and 198.19.0.0/16 ranges. You can review this announcement from our what\u0026rsquo;s new blog\nIn this tutorial, we will walk you through the configuration that is needed so that you can launch your Pod networking on top of secondary CIDRs\n"
},
{
	"uri": "/advanced/410_batch/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Batch processing refers to performing units of work, referred to as a job in a repetitive and unattended fashion. Jobs are typically grouped together and processed in batches (hence the name).\nKubernetes includes native support for running Jobs. Jobs can run multiple pods in parallel until receiving a set number of completions. Each pod can contain multiple containers as a single unit of work.\nArgo enhances the batch processing experience by introducing a number of features:\n Steps based declaration of workflows Artifact support Step level inputs \u0026amp; outputs Loops Conditionals Visualization (using Argo Dashboard) \u0026hellip;and more  In this module, we will build a simple Kubernetes Job, recreate that job in Argo, and add common features and workflows for more advanced batch processing.\n"
},
{
	"uri": "/advanced/310_servicemesh_with_istio/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " This chapter has been updated to Istio 1.10.x Click here to know more about this new release.\n Istio is a completely open source service mesh that layers transparently onto existing distributed applications. It\u0026rsquo;s also a platform, including APIs, that let it integrate into any logging platform, or telemetry or policy system.\nIstio works by having a small network proxy sit alongside each microservice called \u0026ldquo;sidecar\u0026rdquo;. It\u0026rsquo;s role is to intercept all of the service‚Äôs traffic, and handles it more intelligently than a simple layer 3 network can. Envoy proxy is used as the sidecar and was originally written at Lyft and is now a CNCF project.\nAn Istio service mesh is logically split into a data plane and a control plane.\n The data plane is composed of a set of intelligent proxies (Envoy) deployed as sidecars. These proxies mediate and control all network communication between microservices. They also collect and report telemetry on all mesh traffic. The control plane manages and configures the proxies to route traffic.  The following diagram shows the different components that make up each plane:\n  Envoy Proxy Processes the inbound/outbound traffic from inter-service and service-to-external-service transparently.    Pilot Pilot provides service discovery for the Envoy sidecars, traffic management capabilities for intelligent routing (e.g., A/B tests, canary deployments, etc.), and resiliency (timeouts, retries, circuit breakers, etc.)    Citadel Citadel enables strong service-to-service and end-user authentication with built-in identity and credential management.    Galley Galley is Istio‚Äôs configuration validation, ingestion, processing and distribution component. It is responsible for insulating the rest of the Istio components from the details of obtaining user configuration from the underlying platform (e.g. Kubernetes).    "
},
{
	"uri": "/advanced/350_opentelemetry/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "AWS Distro for OpenTelemetry is a secure, production-ready, AWS-supported distribution of the OpenTelemetry project. Part of the Cloud Native Computing Foundation, OpenTelemetry provides open source APIs, libraries, and agents to collect distributed traces and metrics for application monitoring.\nBenefits of using ADOT:\n AWS Distro for OpenTelemetry empowers you to implement broad yet efficient, secure yet flexible, observability solutions It helps you optimize your production environments by ensuring predictable resource utilization, and can increase your analytical visibility while protecting your investment in standardized observability tools It is backed by AWS Support, testing, and certification.  The following diagram shows how we\u0026rsquo;ll be configuring our OpenTelemetry Collector in this module:\n"
},
{
	"uri": "/intermediate/330_app_mesh/deploy_dj_app/",
	"title": "Deploy the DJ app",
	"tags": [],
	"description": "",
	"content": "To understand AWS App Mesh, its best to also understand any applications that run on top of it. So in this chapter, we\u0026rsquo;ll first walk you through creating a simple EKS-based k8s application called \u0026ldquo;The DJ App\u0026rdquo;.\nArmed with the knowledge of how the DJ app works without a service mesh, you\u0026rsquo;ll better understand the service mesh functionality App Mesh brings to the equation.\n"
},
{
	"uri": "/beginner/120_network-policies/calico/",
	"title": "Create Network Policies Using Calico",
	"tags": [],
	"description": "",
	"content": "In this Chapter, we will create some network policies using Calico and see the rules in action.\nNetwork policies allow you to define rules that determine what type of traffic is allowed to flow between different services. Using network policies you can also define rules to restrict traffic. They are a means to improve your cluster\u0026rsquo;s security.\nFor example, you can only allow traffic from frontend to backend in your application.\nNetwork policies also help in isolating traffic within namespaces. For instance, if you have separate namespaces for development and production, you can prevent traffic flow between them by restrict pod to pod communication within the same namespace.\n"
},
{
	"uri": "/intermediate/240_monitoring/deploy-prometheus/",
	"title": "Deploy Prometheus",
	"tags": [],
	"description": "",
	"content": "Deploy Prometheus First we are going to install Prometheus. In this example, we are primarily going to use the standard configuration, but we do override the storage class. We will use gp2 EBS volumes for simplicity and demonstration purpose. When deploying in production, you would use io1 volumes with desired IOPS and increase the default storage size in the manifests to get better performance. Run the following command:\nkubectl create namespace prometheus helm install prometheus prometheus-community/prometheus \\  --namespace prometheus \\  --set alertmanager.persistentVolume.storageClass=\u0026#34;gp2\u0026#34; \\  --set server.persistentVolume.storageClass=\u0026#34;gp2\u0026#34; Make note of the prometheus endpoint in helm response (you will need this later). It should look similar to below:\nThe Prometheus server can be accessed via port 80 on the following DNS name from within your cluster: prometheus-server.prometheus.svc.cluster.local Check if Prometheus components deployed as expected\nkubectl get all -n prometheus You should see response similar to below. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/prometheus-alertmanager-868f8db8c4-67j2x 2/2 Running 0 78s pod/prometheus-kube-state-metrics-6df5d44568-c4tkn 1/1 Running 0 78s pod/prometheus-node-exporter-dh6f4 1/1 Running 0 78s pod/prometheus-node-exporter-v8rd8 1/1 Running 0 78s pod/prometheus-node-exporter-vcbjq 1/1 Running 0 78s pod/prometheus-pushgateway-759689fbc6-hvjjm 1/1 Running 0 78s pod/prometheus-server-546c64d959-qxbzd 2/2 Running 0 78s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/prometheus-alertmanager ClusterIP 10.100.38.47 \u0026lt;none\u0026gt; 80/TCP 78s service/prometheus-kube-state-metrics ClusterIP 10.100.165.139 \u0026lt;none\u0026gt; 8080/TCP 78s service/prometheus-node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 78s service/prometheus-pushgateway ClusterIP 10.100.150.237 \u0026lt;none\u0026gt; 9091/TCP 78s service/prometheus-server ClusterIP 10.100.209.224 \u0026lt;none\u0026gt; 80/TCP 78s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/prometheus-node-exporter 3 3 3 3 3 \u0026lt;none\u0026gt; 78s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/prometheus-alertmanager 1/1 1 1 78s deployment.apps/prometheus-kube-state-metrics 1/1 1 1 78s deployment.apps/prometheus-pushgateway 1/1 1 1 78s deployment.apps/prometheus-server 1/1 1 1 78s NAME DESIRED CURRENT READY AGE replicaset.apps/prometheus-alertmanager-868f8db8c4 1 1 1 78s replicaset.apps/prometheus-kube-state-metrics-6df5d44568 1 1 1 78s replicaset.apps/prometheus-pushgateway-759689fbc6 1 1 1 78s replicaset.apps/prometheus-server-546c64d959 1 1 1 78s  In order to access the Prometheus server URL, we are going to use the kubectl port-forward command to access the application. In Cloud9, run:\nkubectl port-forward -n prometheus deploy/prometheus-server 8080:9090 In your Cloud9 environment, click Tools / Preview / Preview Running Application. Scroll to the end of the URL and append:\n/targets  In the web UI, you can see all the targets and metrics being monitored by Prometheus:\n"
},
{
	"uri": "/010_introduction/",
	"title": "Introduction",
	"tags": ["beginner", "kubeflow", "appmesh", "CON203", "CON205", "CON206"],
	"description": "",
	"content": "Introduction to Kubernetes   A walkthrough of basic Kubernetes concepts.\nWelcome to the Amazon EKS Workshop!\nThe intent of this workshop is to educate users about the features of Amazon EKS.\nBackground in EKS, Kubernetes, Docker, and container workflows are not required, but they are recommended.\nThis chapter will introduce you to the basic workings of Kubernetes, laying the foundation for the hands-on portion of the workshop.\nSpecifically, we will walk you through the following topics:\n Kubernetes (k8s) Basics   Kubernetes Architecture   Amazon EKS   "
},
{
	"uri": "/beginner/090_rbac/intro/",
	"title": "What is RBAC?",
	"tags": [],
	"description": "",
	"content": "According to the official kubernetes docs:\n Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within an enterprise.\n The core logical components of RBAC are:\nEntity\nA group, user, or service account (an identity representing an application that wants to execute certain operations (actions) and requires permissions to do so).\nResource\nA pod, service, or secret that the entity wants to access using the certain operations.\nRole\nUsed to define rules for the actions the entity can take on various resources.\nRole binding\nThis attaches (binds) a role to an entity, stating that the set of rules define the actions permitted by the attached entity on the specified resources.\nThere are two types of Roles (Role, ClusterRole) and the respective bindings (RoleBinding, ClusterRoleBinding). These differentiate between authorization in a namespace or cluster-wide.\nNamespace\nNamespaces are an excellent way of creating security boundaries, they also provide a unique scope for object names as the \u0026lsquo;namespace\u0026rsquo; name implies. They are intended to be used in multi-tenant environments to create virtual kubernetes clusters on the same physical cluster.\nObjectives for this module In this module, we\u0026rsquo;re going to explore k8s RBAC by creating an IAM user called rbac-user who is authenticated to access the EKS cluster but is only authorized (via RBAC) to list, get, and watch pods and deployments in the \u0026lsquo;rbac-test\u0026rsquo; namespace.\nTo achieve this, we\u0026rsquo;ll create an IAM user, map that user to a kubernetes role, then perform kubernetes actions under that user\u0026rsquo;s context.\n"
},
{
	"uri": "/beginner/050_deploy/deploynodejs/",
	"title": "Deploy NodeJS Backend API",
	"tags": [],
	"description": "",
	"content": "Let‚Äôs bring up the NodeJS Backend API!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-nodejs "
},
{
	"uri": "/030_eksctl/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "For this module, we need to download the eksctl binary:\ncurl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp sudo mv -v /tmp/eksctl /usr/local/bin Confirm the eksctl command works:\neksctl version Enable eksctl bash-completion\neksctl completion bash \u0026gt;\u0026gt; ~/.bash_completion . /etc/profile.d/bash_completion.sh . ~/.bash_completion "
},
{
	"uri": "/beginner/080_scaling/deploy_hpa/",
	"title": "Configure Horizontal Pod AutoScaler (HPA)",
	"tags": [],
	"description": "",
	"content": "Deploy the Metrics Server Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines.\nThese metrics will drive the scaling behavior of the deployments.\nWe will deploy the metrics server using Kubernetes Metrics Server.\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml Lets' verify the status of the metrics-server APIService (it could take a few minutes).\nkubectl get apiservice v1beta1.metrics.k8s.io -o json | jq \u0026#39;.status\u0026#39; { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-11-10T06:39:13Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;all checks passed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Passed\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ] }  We are now ready to scale a deployed application\n"
},
{
	"uri": "/intermediate/230_logging/prereqs/",
	"title": "Configure IRSA for Fluent Bit",
	"tags": [],
	"description": "",
	"content": " Click here if you are not familiar wit IAM Roles for Service Accounts (IRSA).\n With IAM roles for service accounts on Amazon EKS clusters, you can associate an IAM role with a Kubernetes service account. This service account can then provide AWS permissions to the containers in any pod that uses that service account. With this feature, you no longer need to provide extended permissions to the node IAM role so that pods on that node can call AWS APIs.\nEnabling IAM roles for service accounts on your cluster To use IAM roles for service accounts in your cluster, we will first create an OIDC identity provider\neksctl utils associate-iam-oidc-provider \\  --cluster eksworkshop-eksctl \\  --approve Creating an IAM role and policy for your service account Next, we will create an IAM policy that limits the permissions needed by the Fluent Bit containers to connect to the Elasticsearch cluster. We will also create an IAM role for your Kubernetes service accounts to use before you associate it with a service account.\nmkdir ~/environment/logging/ export ES_DOMAIN_NAME=\u0026#34;eksworkshop-logging\u0026#34; cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/logging/fluent-bit-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;es:ESHttp*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } ] } EoF aws iam create-policy \\  --policy-name fluent-bit-policy \\  --policy-document file://~/environment/logging/fluent-bit-policy.json Create an IAM role Finally, create an IAM role for the fluent-bit Service Account in the logging namespace.\nkubectl create namespace logging eksctl create iamserviceaccount \\  --name fluent-bit \\  --namespace logging \\  --cluster eksworkshop-eksctl \\  --attach-policy-arn \u0026#34;arn:aws:iam::${ACCOUNT_ID}:policy/fluent-bit-policy\u0026#34; \\  --approve \\  --override-existing-serviceaccounts Make sure your service account with the ARN of the IAM role is annotated kubectl -n logging describe sa fluent-bit Output\nName: fluent-bit Namespace: logging Labels: \u0026lt;none\u0026gt; Annotations: eks.amazonaws.com/role-arn: arn:aws:iam::197520326489:role/eksctl-eksworkshop-eksctl-addon-iamserviceac-Role1-1V7G71K6ZN8ID Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: fluent-bit-token-qxdtx Tokens: fluent-bit-token-qxdtx Events: \u0026lt;none\u0026gt;  "
},
{
	"uri": "/beginner/060_helm/helm_micro/create_chart/",
	"title": "Create a Chart",
	"tags": [],
	"description": "",
	"content": "Helm charts have a structure similar to:\n/eksdemo ‚îú‚îÄ‚îÄ charts/ ‚îú‚îÄ‚îÄ Chart.yaml ‚îú‚îÄ‚îÄ templates/ ‚îÇ¬†‚îú‚îÄ‚îÄ deployment.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ _helpers.tpl ‚îÇ¬†‚îú‚îÄ‚îÄ hpa.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ ingress.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ NOTES.txt ‚îÇ¬†‚îú‚îÄ‚îÄ serviceaccount.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ service.yaml ‚îÇ¬†‚îî‚îÄ‚îÄ tests ‚îÇ¬†‚îî‚îÄ‚îÄ test-connection.yaml ‚îî‚îÄ‚îÄ values.yaml  We\u0026rsquo;ll follow this template, and create a new chart called eksdemo with the following commands:\ncd ~/environment helm create eksdemo cd eksdemo "
},
{
	"uri": "/beginner/170_statefulset/configmap/",
	"title": "Create ConfigMap",
	"tags": [],
	"description": "",
	"content": "Introduction ConfigMap allow you to decouple configuration artifacts and secrets from image content to keep containerized applications portable. Using ConfigMap, you can independently control the MySQL configuration.\nCreate the mysql Namespace We will create a new Namespace called mysql that will host all the components.\nkubectl create namespace mysql Create ConfigMap Run the following commands to create the ConfigMap.\ncd ${HOME}/environment/ebs_statefulset cat \u0026lt;\u0026lt; EoF \u0026gt; ${HOME}/environment/ebs_statefulset/mysql-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: mysql-config namespace: mysql labels: app: mysql data: master.cnf: | # Apply this config only on the leader. [mysqld] log-bin slave.cnf: | # Apply this config only on followers. [mysqld] super-read-only EoF The ConfigMap stores master.cnf, slave.cnf and passes them when initializing leader and follower pods defined in StatefulSet:\n master.cnf is for the MySQL leader pod which has binary log option (log-bin) to provides a record of the data changes to be sent to follower servers. slave.cnf is for follower pods which have super-read-only option.  Create \u0026ldquo;mysql-config\u0026rdquo; ConfigMap.\nkubectl create -f ${HOME}/environment/ebs_statefulset/mysql-configmap.yaml "
},
{
	"uri": "/intermediate/210_jenkins/serviceaccount/",
	"title": "Creating the Jenkins Service Account",
	"tags": [],
	"description": "",
	"content": "We\u0026rsquo;ll create a service account for Kubernetes to grant to pods if they need to perform CodeCommit API actions (e.g. GetCommit, ListBranches). This will allow Jenkins to respond to new repositories, branches, and commits.\nIf you have not completed the IAM Roles for Service Accounts lab, please complete the Create an OIDC identity provider step now. You do not need to complete any other sections of that lab.\n eksctl create iamserviceaccount \\  --name jenkins \\  --namespace default \\  --cluster eksworkshop-eksctl \\  --attach-policy-arn arn:aws:iam::aws:policy/AWSCodeCommitPowerUser \\  --approve \\  --override-existing-serviceaccounts "
},
{
	"uri": "/beginner/040_dashboard/dashboard/",
	"title": "Deploy the Official Kubernetes Dashboard",
	"tags": [],
	"description": "",
	"content": "The official Kubernetes dashboard is not deployed by default, but there are instructions in the official documentation\nWe can deploy the dashboard with the following command:\nexport DASHBOARD_VERSION=\u0026#34;v2.0.0\u0026#34; kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/${DASHBOARD_VERSION}/aio/deploy/recommended.yaml Since this is deployed to our private cluster, we need to access it via a proxy. kube-proxy is available to proxy our requests to the dashboard service. In your workspace, run the following command:\nkubectl proxy --port=8080 --address=0.0.0.0 --disable-filter=true \u0026amp; This will start the proxy, listen on port 8080, listen on all interfaces, and will disable the filtering of non-localhost requests.\nThis command will continue to run in the background of the current terminal\u0026rsquo;s session.\nWe are disabling request filtering, a security feature that guards against XSRF attacks. This isn\u0026rsquo;t recommended for a production environment, but is useful for our dev environment.\n "
},
{
	"uri": "/intermediate/330_app_mesh/port_to_app_mesh/mesh_resources_overview/",
	"title": "Mesh Resources and Design",
	"tags": [],
	"description": "",
	"content": "To port the DJ App to App Mesh, you will need create a mesh as well as the various mesh components. You\u0026rsquo;ll also apply labels to the prod namespace to affiliate your new mesh with it, and to enable automatic sidecar injection for pods within it.\nApp Mesh Resources Kubernetes application objects that run within App Mesh must be defined as Virtual Nodes. This provides App Mesh an abstraction to objects such as Kubernetes Deployments and Services, and provides endpoints for communication and routing configuration.\nApp Mesh also provides the Virtual Service construct which allows you to specify a logical service path for application traffic. In this example, they send traffic to Virtual Routers, which then route traffic to the Virtual Nodes.\nIn the image below you see DJ App running within App Mesh. Each of the services (dj, metal-v1, and jazz-v1) has a virtual node defined, and each music category has a virtual service (metal and jazz). These virtual services send traffic to virtual routers within the mesh, which in turn specify routing rules. This drives traffic to their respective virtual nodes and ultimately to the service endpoints within Kubernetes.\nYou\u0026rsquo;ll find the YAML which specify these resources in the application repo\u0026rsquo;s 2_meshed_application directory.\nLooking at the YAML, you can see we\u0026rsquo;ve added the required labels to the prod namespace and specified our mesh named dj-app.\n--- apiVersion: v1 kind: Namespace metadata: name: prod labels: mesh: dj-app appmesh.k8s.aws/sidecarInjectorWebhook: enabled --- apiVersion: appmesh.k8s.aws/v1beta2 kind: Mesh metadata: name: dj-app spec: namespaceSelector: matchLabels: mesh: dj-app ---  Included are the specifications for the App Mesh resources shown in the image above. For example, here is the dj service\u0026rsquo;s VirtualNode specification.\n--- apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualNode metadata: name: dj namespace: prod spec: podSelector: matchLabels: app: dj listeners: - portMapping: port: 9080 protocol: http healthCheck: protocol: http path: \u0026#39;/ping\u0026#39; healthyThreshold: 2 unhealthyThreshold: 2 timeoutMillis: 2000 intervalMillis: 5000 backends: - virtualService: virtualServiceRef: name: jazz - virtualService: virtualServiceRef: name: metal serviceDiscovery: dns: hostname: dj.prod.svc.cluster.local ---  Note that it uses a podSelector to identify which Pods are members of this virtual node, as well as a pointer to the dj Service.\nThere are also VirtualService and VirtualRouter specifications for each of the music categories, establishing traffic routing to their respective endpoints. This is accomplished by adding Routes which point to the jazz-v1 and metal-v1 virtual nodes.\nShown here is the jazz virtual service and virtual router.\n--- apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualService metadata: name: jazz namespace: prod spec: awsName: jazz.prod.svc.cluster.local provider: virtualRouter: virtualRouterRef: name: jazz-router --- apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualRouter metadata: name: jazz-router namespace: prod spec: listeners: - portMapping: port: 9080 protocol: http routes: - name: jazz-route httpRoute: match: prefix: / action: weightedTargets: - virtualNodeRef: name: jazz-v1 weight: 100 ---  With the basic constructs understood, it\u0026rsquo;s time to create the mesh and its resources.\n"
},
{
	"uri": "/beginner/070_healthchecks/readinessprobe/",
	"title": "Configure Readiness Probe",
	"tags": [],
	"description": "",
	"content": "Configure the Probe Run the following code block to populate ~/environment/healthchecks/readiness-deployment.yaml. The readinessProbe definition explains how a linux command can be configured as healthcheck. We create an empty file /tmp/healthy to configure readiness probe and use the same to understand how kubelet helps to update a deployment with only healthy pods.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/healthchecks/readiness-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: readiness-deployment spec: replicas: 3 selector: matchLabels: app: readiness-deployment template: metadata: labels: app: readiness-deployment spec: containers: - name: readiness-deployment image: alpine command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;touch /tmp/healthy \u0026amp;\u0026amp; sleep 86400\u0026quot;] readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 3 EoF We will now create a deployment to test readiness probe:\nkubectl apply -f ~/environment/healthchecks/readiness-deployment.yaml The above command creates a deployment with 3 replicas and readiness probe as described in the beginning.\nkubectl get pods -l app=readiness-deployment The output looks similar to below:\nNAME READY STATUS RESTARTS AGE readiness-deployment-7869b5d679-922mx 1/1 Running 0 31s readiness-deployment-7869b5d679-vd55d 1/1 Running 0 31s readiness-deployment-7869b5d679-vxb6g 1/1 Running 0 31s  Let us also confirm that all the replicas are available to serve traffic when a service is pointed to this deployment.\nkubectl describe deployment readiness-deployment | grep Replicas: The output looks like below:\nReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable  Introduce a Failure Pick one of the pods from above 3 and issue a command as below to delete the /tmp/healthy file which makes the readiness probe fail.\nkubectl exec -it \u0026lt;YOUR-READINESS-POD-NAME\u0026gt; -- rm /tmp/healthy readiness-deployment-7869b5d679-922mx was picked in our example cluster. The /tmp/healthy file was deleted. This file must be present for the readiness check to pass. Below is the status after issuing the command.\nkubectl get pods -l app=readiness-deployment The output looks similar to below: NAME READY STATUS RESTARTS AGE readiness-deployment-7869b5d679-922mx 0/1 Running 0 4m readiness-deployment-7869b5d679-vd55d 1/1 Running 0 4m readiness-deployment-7869b5d679-vxb6g 1/1 Running 0 4m  Traffic will not be routed to the first pod in the above deployment. The ready column confirms that the readiness probe for this pod did not pass and hence was marked as not ready.\nWe will now check for the replicas that are available to serve traffic when a service is pointed to this deployment.\nkubectl describe deployment readiness-deployment | grep Replicas: The output looks like below:\nReplicas: 3 desired | 3 updated | 3 total | 2 available | 1 unavailable  When the readiness probe for a pod fails, the endpoints controller removes the pod from list of endpoints of all services that match the pod.\nChallenge: How would you restore the pod to Ready status?   Expand here to see the solution   Run the below command with the name of the pod to recreate the /tmp/healthy file. Once the pod passes the probe, it gets marked as ready and will begin to receive traffic again.\nkubectl exec -it \u0026lt;YOUR-READINESS-POD-NAME\u0026gt; -- touch /tmp/healthy kubectl get pods -l app=readiness-deployment   \n"
},
{
	"uri": "/intermediate/220_codepipeline/role/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "In an AWS CodePipeline, we are going to use AWS CodeBuild to deploy a sample Kubernetes service. This requires an AWS Identity and Access Management (IAM) role capable of interacting with the EKS cluster.\nIn this step, we are going to create an IAM role and add an inline policy that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl.\nCreate the role:\ncd ~/environment TRUST=\u0026quot;{ \\\u0026quot;Version\\\u0026quot;: \\\u0026quot;2012-10-17\\\u0026quot;, \\\u0026quot;Statement\\\u0026quot;: [ { \\\u0026quot;Effect\\\u0026quot;: \\\u0026quot;Allow\\\u0026quot;, \\\u0026quot;Principal\\\u0026quot;: { \\\u0026quot;AWS\\\u0026quot;: \\\u0026quot;arn:aws:iam::${ACCOUNT_ID}:root\\\u0026quot; }, \\\u0026quot;Action\\\u0026quot;: \\\u0026quot;sts:AssumeRole\\\u0026quot; } ] }\u0026quot; echo '{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;eks:Describe*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }' \u0026gt; /tmp/iam-role-policy aws iam create-role --role-name EksWorkshopCodeBuildKubectlRole --assume-role-policy-document \u0026quot;$TRUST\u0026quot; --output text --query 'Role.Arn' aws iam put-role-policy --role-name EksWorkshopCodeBuildKubectlRole --policy-name eks-describe --policy-document file:///tmp/iam-role-policy "
},
{
	"uri": "/intermediate/200_migrate_to_eks/create-kind-cluster/",
	"title": "Create kind cluster",
	"tags": [],
	"description": "",
	"content": "While our EKS cluster is being created we can create a kind cluster locally. Before we create one lets make sure our network rules are set up\nThis is going to manually create some iptables rules to route traffic to your Cloud9 instance. If you reboot the VM you will have to run these commands again as they persistent.\n echo \u0026#39;net.ipv4.conf.all.route_localnet = 1\u0026#39; | sudo tee /etc/sysctl.conf sudo sysctl -p /etc/sysctl.conf sudo iptables -t nat -A PREROUTING -p tcp -d 169.254.170.2 --dport 80 -j DNAT --to-destination 127.0.0.1:51679 sudo iptables -t nat -A OUTPUT -d 169.254.170.2 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 51679 Create a config file for the kind cluster\ncat \u0026gt; kind.yaml \u0026lt;\u0026lt;EOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.19.11@sha256:07db187ae84b4b7de440a73886f008cf903fcf5764ba8106a9fd5243d6f32729 extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 EOF Create the local kind cluster\nkind create cluster --config kind.yaml Set the default context to the EKS cluster.\nkubectl config use-context \u0026#34;${INSTANCE_ID}@${CLUSTER}.${AWS_REGION}.eksctl.io\u0026#34; "
},
{
	"uri": "/intermediate/241_pixie/deploy_pixie/",
	"title": "Deploy Pixie",
	"tags": [],
	"description": "",
	"content": "Install Pixie\u0026rsquo;s CLI Install Pixie‚Äôs CLI tool using the install script:\nbash -c \u0026#34;$(curl -fsSL https://withpixie.ai/install.sh)\u0026#34;  Press enter to accept the Terms \u0026amp; Conditions. Press enter to accept the default install path. Visit the provided URL. Select SIGN UP if you do not have an existing Pixie account, otherwise log in with your Google account. Copy and paste the auth token generated in the browser into the CLI.  Deploy Pixie Deploy the hosted version of Pixie to the eksworkshop-eksctl cluster using the CLI:\npx deploy --cluster_name eksworkshop-eksctl --pem_memory_limit=1Gi  Enter y to confirm the cluster to deploy pixie to.  Deploying Pixie will take about 5 minutes.\n Check if the Pixie components have been deployed:\nkubectl get pods -n pl You should see output similar to below. All pods should be ready and available before proceeding.\nNAME READY STATUS RESTARTS AGE kelvin-7749f484bc-9vmrd 1/1 Running 0 2m14s nats-operator-7d686798f4-tkjn4 1/1 Running 0 2m28s pl-nats-1 1/1 Running 0 2m21s vizier-certmgr-5bc8d5dcc8-grrqt 1/1 Running 0 2m14s vizier-cloud-connector-5cdf4ccb94-crz8p 1/1 Running 0 2m14s vizier-metadata-0 1/1 Running 0 2m13s vizier-pem-5fs5d 1/1 Running 0 2m13s vizier-pem-5kxm5 1/1 Running 0 2m13s vizier-pem-b79fz 1/1 Running 0 2m13s vizier-pem-c5bh8 1/1 Running 0 2m13s vizier-pem-ddjvw 1/1 Running 0 2m13s vizier-pem-rx7f4 1/1 Running 0 2m13s vizier-proxy-7c75497cdd-4lzm5 1/1 Running 0 2m13s vizier-query-broker-6f7fc849f-wdpcz 1/1 Running 0 2m13s  "
},
{
	"uri": "/intermediate/260_weave_flux/githubsetup/",
	"title": "GitHub Setup",
	"tags": [],
	"description": "",
	"content": "We are going to create 2 GitHub repositories. One will be used for a sample application that will trigger a Docker image build. Another will be used to hold Kubernetes manifests that Weave Flux deploys into the cluster. Note this is a pull based method compared to other continuous deployment tools that push to Kubernetes.\nCreate the sample application repository by clicking here.\nFill in the form with repository name, description, and check initializing the repository with a README as shown below and click Create repository.\nRepeat this process to create the Kubernetes manifests repositories by clicking here. Fill in the form as shown below and click Create repository.\nThe next step is to create a personal access token that will allow CodePipeline to receive callbacks from GitHub.\nOnce created, an access token can be stored in a secure enclave and reused, so this step is only required during the first run or when you need to generate new keys.\nOpen up the New personal access page in GitHub.\nYou may be prompted to enter your GitHub password\n Enter a value for Token description, check the repo permission scope and scroll down and click the Generate token button\nCopy the personal access token and save it in a secure place for the next step\nWe will need to revisit GitHub one more time once we provision Weave Flux to enable Weave to control repositories. However, at this time you can move on.\n"
},
{
	"uri": "/intermediate/290_argocd/install/",
	"title": "Install Argo CD",
	"tags": [],
	"description": "",
	"content": "ArgoCD Architecture ArgoCD is composed of three mains components:\nAPI Server: Exposes the API for the WebUI / CLI / CICD Systems\nRepository Server: Internal service which maintains a local cache of the git repository holding the application manifests\nApplication Controller: Kubernetes controller which controls and monitors applications continuously and compares that current live state with desired target state (specified in the repository). If a OutOfSync is detected, it will take corrective actions.\nInstall Argo CD All those components could be installed using a manifest provided by the Argo Project:\nkubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.0.4/manifests/install.yaml Install Argo CD CLI To interact with the API Server we need to deploy the CLI:\nsudo curl --silent --location -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/v2.0.4/argocd-linux-amd64 sudo chmod +x /usr/local/bin/argocd "
},
{
	"uri": "/intermediate/300_cis_eks_benchmark/intro/",
	"title": "Introduction to CIS Amazon EKS Benchmark and kube-bench",
	"tags": [],
	"description": "",
	"content": "CIS Kubernetes Benchmark The latest version of CIS Kubernetes Benchmark v1.5.1 provides guidance on security configurations for Kubernetes versions v1.15 and onwards. The CIS Kubernetes Benchmark is scoped for implementations managing both the control plane, which includes etcd, API server, controller and scheduler, and the data plane, which is made up of one or more nodes or EC2 instances.\nCIS EKS Kubernetes Benchmark Since Amazon EKS provides a managed control plane, not all of the recommendations from the CIS Kubernetes Benchmark are applicable as customers are not responsible for configuring or managing the control plane.\nCIS Amazon EKS Benchmark v1.0.0 provides guidance for node security configurations for Kubernetes and aligns with CIS Kubernetes Benchmark v1.5.1.\nNote: The CIS committee agreed to remove controls for the appropriate control plane recommendations from the managed Kubernetes benchmarks. The CIS Amazon EKS Benchmark consists of four sections on control plane logging configuration, worker nodes, policies and managed services.\n aquasecurity/kube-bench kube-bench is a popular open source CIS Kubernetes Benchmark assessment tool created by AquaSecurity. kube-bench is a Go application that checks whether Kubernetes is deployed securely by running the checks documented in the CIS Kubernetes Benchmark. Tests are configured with YAML files, and this makes kube-bench easy to update as test specifications evolve. AquaSecurity is an AWS Advanced Technology Partner.\n"
},
{
	"uri": "/intermediate/310_opa_gatekeeper/intro/",
	"title": "Introduction to Open Policy Agent Gatekeeper",
	"tags": [],
	"description": "",
	"content": "Open Policy Agent Gatekeeper OPA decouples policy decision-making from policy enforcement. When your software needs to make policy decisions it queries OPA and supplies structured data (e.g., JSON) as input. OPA accepts arbitrary structured data as input.\nOPA generates policy decisions by evaluating the query input and against policies and data. OPA and Rego are domain-agnostic so you can describe almost any kind of invariant in your policies. For example:\n Which users can access which resources. Which subnets egress traffic is allowed to. Which clusters a workload must be deployed to. Which registries binaries can be downloaded from. Which OS capabilities a container can execute with. Which times of day the system can be accessed at.  Policy decisions are not limited to simple yes/no or allow/deny answers. Like query inputs, your policies can generate arbitrary structured data as output.\n"
},
{
	"uri": "/intermediate/245_x-ray/role/",
	"title": "Modify IAM Role",
	"tags": [],
	"description": "",
	"content": "In order for the X-Ray daemon to communicate with the service, we need to create a Kubernetes service account and attach an AWS Identity and Access Management (IAM) role and policy with sufficient permissions.\nIf you have not completed the IAM Roles for Service Accounts lab, please complete the Create an OIDC identity provider step now. You do not need to complete any other sections of that lab.\n Create the service account for X-Ray.\neksctl create iamserviceaccount --name xray-daemon --namespace default --cluster eksworkshop-eksctl --attach-policy-arn arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess --approve --override-existing-serviceaccounts Apply a label to the service account\nkubectl label serviceaccount xray-daemon app=xray-daemon "
},
{
	"uri": "/intermediate/265_spinnaker_eks/overview/",
	"title": "Spinnaker Overview",
	"tags": [],
	"description": "",
	"content": "Spinnaker Architecture You can also see the detailed architecture of spinnaker at Armory docs.\nImage source:https://docs.armory.io/docs/overview/architecture/\nDeck: Browser-based UI for Spinnaker.\nGate: API callers and Spinnaker UI communicate to Spinnaker server via this API gateway called Gate.\nOrca: Pipelines and other ad-hoc operations are managed by this orchestration engine called Orca.\nClouddriver: Indexing and Caching of deployed resources are taken care by Clouddriver. It also facilitates calls to cloud providers like AWS, GCE, and Azure.\nEcho: It is responsible for sending notifications, it also acts as incoming webhook.\nIgor: It is used to trigger pipelines via continuous integration jobs in systems like Jenkins and Travis CI, and it allows Jenkins/Travis stages to be used in pipelines.\nFront50: It\u0026rsquo;s the metadata store of Spinnaker. It persists metadata for all resources which include pipelines, projects, applications and notifications.\nRosco: Rosco bakes machine images (AWS AMIs, Azure VM images, GCE images).\nRush: It is Spinnaker\u0026rsquo;s script excution engine.\nSpinnaker Concepts Spinnaker provides two core sets of features:\n  Application management (a.k.a. infrastructure management) You use Spinnaker‚Äôs application management features to view and manage your cloud resources.\n  Application deployment You use Spinnaker‚Äôs application deployment features to construct and manage continuous delivery workflows.\n  In this workshop, we are only focussing on Application Deployment so lets deep dive into this feature. More details on Spinnaker Nomenclature and Naming Conventions can be found at Armory docs.\n  Pipeline: The pipeline is the key deployment management construct in Spinnaker. It consists of a sequence of actions, known as stages. You can pass parameters from stage to stage along the pipeline. Image source:https://spinnaker.io/concepts/pipelines.png\n  Stage: A Stage in Spinnaker is a collection of sequential Tasks and composed Stages that describe a higher-level action the Pipeline will perform either linearly or in parallel.\n  Task: A Task in Spinnaker is an automatic function to perform.\n  Deployment Strategies: Spinnaker treats cloud-native deployment strategies as first class constructs, handling the underlying orchestration such as verifying health checks, disabling old server groups and enabling new server groups. Spinnaker supports the red/black (a.k.a. blue/green) strategy, with rolling red/black and canary strategies in active development.\n  "
},
{
	"uri": "/beginner/200_secrets/deploying-secrets-volumes/",
	"title": "Creating and Deploying Secrets (cont.)",
	"tags": [],
	"description": "",
	"content": "Exposing Secrets as Volumes Secrets can be also mounted as data volumes on to a Pod and you can control the paths within the volume where the Secret keys are projected using a Pod manifest as shown below: apiVersion: v1 kind: Pod metadata: name: someName namespace: someNamespace spec: containers: - name: someContainer image: someImage volumeMounts: - name: secret-volume mountPath: \u0026#34;/etc/data\u0026#34; readOnly: true volumes: - name: secret-volume secret: secretName: database-credentials items: - key: username path: DATABASE_USER - key: password path: DATABASE_PASSWORD  With the above configuration, what will happen is:\n value for the username key in the database-credentials Secret is stored in the file /etc/data/DATABASE_USER within the Pod value for the password key is stored in the file /etc/data/DATABASE_PASSWORD  Run the following set of commands to deploy a pod that mounts the database-credentials Secret as a volume.\nwget https://eksworkshop.com/beginner/200_secrets/secrets.files/pod-volume.yaml kubectl apply -f pod-volume.yaml kubectl get pod -n octank View the output logs from the pod to verfiy that the files /etc/data/DATABASE_USER and /etc/data/DATABASE_PASSWORD within the Pod have been loaded with the expected literal values\nkubectl logs pod-volume -n octank The output should look as follows: cat /etc/data/DATABASE_USER admin cat /etc/data/DATABASE_PASSWORD Tru5tN0!  "
},
{
	"uri": "/intermediate/201_resource_management/advanced-pod-limits/",
	"title": "Advanced Pod CPU and Memory Management",
	"tags": [],
	"description": "",
	"content": "In the previous section, we created CPU and Memory constraints at the pod level. LimitRange are used to constraint compute, storage or enforce ratio between Request and Limit in a Namespace. In this section, we will separate the compute workloads by low-usage, high-usage and unrestricted-usage.\nWe will create three Namespaces:\nmkdir ~/environment/resource-management kubectl create namespace low-usage kubectl create namespace high-usage kubectl create namespace unrestricted-usage Create Limit Ranges Create LimitRange specification for low-usage and high-usage namespace level. The unrestricted-usage will not have any limits enforced.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/resource-management/low-usage-limit-range.yml apiVersion: v1 kind: LimitRange metadata: name: low-usage-range spec: limits: - max: cpu: 1 memory: 300M min: cpu: 0.5 memory: 100M type: Container EoF kubectl apply -f ~/environment/resource-management/low-usage-limit-range.yml --namespace low-usage cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/resource-management/high-usage-limit-range.yml apiVersion: v1 kind: LimitRange metadata: name: high-usage-range spec: limits: - max: cpu: 2 memory: 2G min: cpu: 1 memory: 1G type: Container EoF kubectl apply -f ~/environment/resource-management/high-usage-limit-range.yml --namespace high-usage Deploy Pods Next we will deploy the pods to the nodes .\nFailed Attempts Creating a pod with values outside what is defined in the LimitRange in the namespace will cause an errors\n# Error due to higher memory request than defined in low-usage namespace: wanted 1g memory above max of 300m kubectl run --namespace low-usage --requests=memory=1G,cpu=0.5 --image hande007/stress-ng basic-request-pod --restart=Never -- --vm-keep --vm-bytes 2g --timeout 600s --vm 1 --oomable --verbose # Error due to lower cpu request than defined in high-usage namespace: wanted 0.5 below min of 1 kubectl run --namespace high-usage --requests=memory=1G,cpu=0.5 --image hande007/stress-ng basic-request-pod --restart=Never -- --vm-keep --vm-bytes 2g --timeout 600s --vm 1 --oomable --verbose Successful Attempts Create pods without specifying Requests or Limits will inherit LimitRange values.\nkubectl run --namespace low-usage --image hande007/stress-ng low-usage-pod --restart=Never -- --vm-keep --vm-bytes 200m --timeout 600s --vm 2 --oomable --verbose kubectl run --namespace high-usage --image hande007/stress-ng high-usage-pod --restart=Never -- --vm-keep --vm-bytes 200m --timeout 600s --vm 2 --oomable --verbose kubectl run --namespace unrestricted-usage --image hande007/stress-ng unrestricted-usage-pod --restart=Never -- --vm-keep --vm-bytes 200m --timeout 600s --vm 2 --oomable --verbose Verify Pod Limits Next we will verify that LimitRange values are being inherited by the pods in each namespace.\neval 'kubectl -n='{low-usage,high-usage,unrestricted-usage}' get pod -o=custom-columns='Name:spec.containers[*].name','Namespace:metadata.namespace','Limits:spec.containers[*].resources.limits';' Output: Name Namespace Limits low-usage-pod low-usage map[cpu:1 memory:300M] Name Namespace Limits high-usage-pod high-usage map[cpu:2 memory:2G] Name Namespace Limits unrestricted-usage-pod unrestricted-usage \u0026lt;none\u0026gt;  Cleanup Clean up before moving on to free up resources\nkubectl delete namespace low-usage kubectl delete namespace high-usage kubectl delete namespace unrestricted-usage "
},
{
	"uri": "/beginner/191_secrets/consume-a-secret/",
	"title": "Access the Secret from a Pod",
	"tags": [],
	"description": "",
	"content": "Deploy a Pod to Consume the Secret Create a YAML file (podconsumingsecret.yaml) with the following pod definition:\ncat \u0026lt;\u0026lt; EOF \u0026gt; podconsumingsecret.yaml --- apiVersion: v1 kind: Pod metadata: name: consumesecret spec: containers: - name: shell image: amazonlinux:2018.03 command: - \u0026#34;bin/bash\u0026#34; - \u0026#34;-c\u0026#34; - \u0026#34;cat /tmp/test-creds \u0026amp;\u0026amp; sleep 10000\u0026#34; volumeMounts: - name: sec mountPath: \u0026#34;/tmp\u0026#34; readOnly: true volumes: - name: sec secret: secretName: test-creds EOF Deploy the pod on your EKS cluster:\nkubectl --namespace secretslab \\  apply -f podconsumingsecret.yaml Output:\npod/consumesecret created  Attach to the pod and attempt to access the secret:\nkubectl --namespace secretslab exec -it consumesecret -- cat /tmp/test-creds Output:\nam i safe?  Let\u0026rsquo;s see if the CloudTrail event for our secret retrieval is now visible. If you go to CloudTrail you should see a record available if you search for the Event type Decrypt with output similar to the following screenshot. If the event hasn\u0026rsquo;t shown up yet, wait a few minutes and try again.\nOn the next screen, you will perform the cleanup operations for this lab.\n"
},
{
	"uri": "/beginner/130_exposing-service/accessing/",
	"title": "Accessing the Service",
	"tags": [],
	"description": "",
	"content": "Accessing the Service Kubernetes supports 2 primary modes of finding a Service:\n environment variables DNS.  The former works out of the box while the latter requires the CoreDNS cluster add-on (automatically installed when creating the EKS cluster).\nEnvironment Variables When a Pod runs on a Node, the kubelet adds a set of environment variables for each active Service. This introduces an ordering problem. To see why, inspect the environment of your running nginx Pods (your Pod name will be different): Let\u0026rsquo;s view the pods again:\nkubectl -n my-nginx get pods -l run=my-nginx -o wide Output: NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE my-nginx-756f645cd7-gsl4g 1/1 Running 0 22m 192.168.59.188 ip-192-168-38-150.us-west-2.compute.internal \u0026lt;none\u0026gt; my-nginx-756f645cd7-t8b6w 1/1 Running 0 22m 192.168.79.210 ip-192-168-92-222.us-west-2.compute.internal \u0026lt;none\u0026gt;  Now let\u0026rsquo;s inspect the environment of one of your running nginx Pods:\nexport mypod=$(kubectl -n my-nginx get pods -l run=my-nginx -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) kubectl -n my-nginx exec ${mypod} -- printenv | grep SERVICE KUBERNETES_SERVICE_HOST=10.100.0.1 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_PORT_HTTPS=443  Note there‚Äôs no mention of your Service. This is because you created the replicas before the Service.\nAnother disadvantage of doing this is that the scheduler might put both Pods on the same machine, which will take your entire Service down if it dies. We can do this the right way by killing the 2 Pods and waiting for the Deployment to recreate them. This time around the Service exists before the replicas. This will give you scheduler-level Service spreading of your Pods (provided all your nodes have equal capacity), as well as the right environment variables:\nkubectl -n my-nginx rollout restart deployment my-nginx kubectl -n my-nginx get pods -l run=my-nginx -o wide Output just in the moment of change: NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE my-nginx-756f645cd7-9tgkw 1/1 Running 0 6s 192.168.14.67 ip-192-168-15-64.us-west-2.compute.internal \u0026lt;none\u0026gt; my-nginx-756f645cd7-gsl4g 0/1 Terminating 0 25m 192.168.59.188 ip-192-168-38-150.us-west-2.compute.internal \u0026lt;none\u0026gt; my-nginx-756f645cd7-ljjgq 1/1 Running 0 6s 192.168.63.80 ip-192-168-38-150.us-west-2.compute.internal \u0026lt;none\u0026gt; my-nginx-756f645cd7-t8b6w 0/1 Terminating 0 25m 192.168.79.210 ip-192-168-92-222.us-west-2.compute.internal \u0026lt;none\u0026gt;  You may notice that the pods have different names, since they are destroyed and recreated.\n Now let‚Äôs inspect the environment of one of your running nginx Pods one more time:\nexport mypod=$(kubectl -n my-nginx get pods -l run=my-nginx -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) kubectl -n my-nginx exec ${mypod} -- printenv | grep SERVICE KUBERNETES_SERVICE_HOST=10.100.0.1 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_PORT_HTTPS=443 MY_NGINX_SERVICE_HOST=10.100.225.196 MY_NGINX_SERVICE_PORT=80  We now have an environment variable referencing the nginx Service IP called MY_NGINX_SERVICE_HOST.\nDNS Kubernetes offers a DNS cluster add-on Service that automatically assigns dns names to other Services. You can check if it‚Äôs running on your cluster:\nTo check if your cluster is already running CoreDNS, use the following command.\nkubectl get service -n kube-system -l k8s-app=kube-dns  The service for CoreDNS is still called kube-dns for backward compatibility.\n NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.0.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 8m  If it isn‚Äôt running, you can enable it. The rest of this section will assume you have a Service with a long lived IP (my-nginx), and a DNS server that has assigned a name to that IP (the CoreDNS cluster add-on), so you can talk to the Service from any pod in your cluster using standard methods (e.g. gethostbyname). Let‚Äôs run another curl application to test this:\nkubectl -n my-nginx run curl --image=radial/busyboxplus:curl -i --tty Then, hit enter and run.\nnslookup my-nginx Output: Server: 10.100.0.10 Address 1: 10.100.0.10 kube-dns.kube-system.svc.cluster.local Name: my-nginx Address 1: 10.100.225.196 my-nginx.my-nginx.svc.cluster.local  Type exit to log out of the container.\nexit "
},
{
	"uri": "/beginner/140_assigning_pods/affinity/",
	"title": "Affinity and anti-affinity",
	"tags": [],
	"description": "",
	"content": "Affinity and anti-affinity nodeSelector provides a very simple way to constrain pods to nodes with particular labels. The affinity/anti-affinity feature greatly extends the types of constraints you can express.\nThe key enhancements are:\n The language is more expressive (not just ‚ÄúAND of exact match‚Äù) You can indicate that the rule is ‚Äúsoft‚Äù/‚Äúpreference‚Äù rather than a hard requirement, so if the scheduler can‚Äôt satisfy it, the pod will still be scheduled You can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located  The affinity feature consists of two types of affinity, ‚Äúnode affinity‚Äù and ‚Äúinter-pod affinity/anti-affinity. Node affinity is like the existing nodeSelector (but with the first two benefits listed above), while inter-pod affinity/anti-affinity constrains against pod labels rather than node labels, as described in the third item listed above, in addition to having the first and second properties listed above.\nNode affinity Node affinity was introduced as alpha in Kubernetes 1.2. Node affinity is conceptually similar to nodeSelector ‚Äì it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.\nThere are currently two types of node affinity, called requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution.\nYou can think of them as ‚Äúhard‚Äù and ‚Äúsoft‚Äù respectively, in the sense that the former specifies rules that must be met for a pod to be scheduled onto a node (just like nodeSelector but using a more expressive syntax), while the latter specifies preferences that the scheduler will try to enforce but will not guarantee. The ‚ÄúIgnoredDuringExecution‚Äù part of the names means that, similar to how nodeSelector works, if labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod will still continue to run on the node.\nThus an example of requiredDuringSchedulingIgnoredDuringExecution would be ‚Äúonly run the pod on nodes with Intel CPUs‚Äù and an example preferredDuringSchedulingIgnoredDuringExecution would be ‚Äútry to run this set of pods in availability zone XYZ, but if it‚Äôs not possible, then allow some to run elsewhere‚Äù.\nNode affinity is specified as field nodeAffinity of field affinity in the PodSpec.\nLet\u0026rsquo;s see an example of a pod that uses node affinity:\nWe are going to create another label on the same node as in the last example.\n# export the first node name as a variable export FIRST_NODE_NAME=$(kubectl get nodes -o json | jq -r \u0026#39;.items[0].metadata.name\u0026#39;) kubectl label nodes ${FIRST_NODE_NAME} azname=az1 And create an affinity:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/pod-with-node-affinity.yaml apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: azname operator: In values: - az1 - az2 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value containers: - name: with-node-affinity image: us.gcr.io/k8s-artifacts-prod/pause:2.0 EoF This node affinity rule says the pod can only be placed on a node with a label whose key is azname and whose value is either az1 or az2. In addition, among nodes that meet that criteria, nodes with a label whose key is another-node-label-key and whose value is another-node-label-value should be preferred.\nLet\u0026rsquo;s apply this:\nkubectl apply -f ~/environment/pod-with-node-affinity.yaml And check if it worked with:\nkubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 93s 192.168.155.38 ip-192-168-155-36.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; with-node-affinity 1/1 Running 0 6s 192.168.156.139 ip-192-168-155-36.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  We are going to apply the label to a different node. So first, let\u0026rsquo;s clean the label and delete the Pod.\nkubectl delete -f ~/environment/pod-with-node-affinity.yaml kubectl label nodes ${FIRST_NODE_NAME} azname- We are applying the label to second node now:\nexport SECOND_NODE_NAME=$(kubectl get nodes -o json | jq -r \u0026#39;.items[1].metadata.name\u0026#39;) kubectl label nodes ${SECOND_NODE_NAME} azname=az1 kubectl apply -f ~/environment/pod-with-node-affinity.yaml And check if it works with:\nkubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 2m14s 192.168.155.38 ip-192-168-155-36.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; with-node-affinity 1/1 Running 0 11s 192.168.166.141 ip-192-168-168-110.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  You can see the operator In being used in the example.\nThe new node affinity syntax supports the following operators: In, NotIn, Exists, DoesNotExist, Gt, Lt. You can use NotIn and DoesNotExist to achieve node anti-affinity behavior.\nIf you specify both nodeSelector and nodeAffinity, both must be satisfied for the pod to be scheduled onto a candidate node.\nIf you specify multiple nodeSelectorTerms associated with nodeAffinity types, then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.\nIf you specify multiple matchExpressions associated with nodeSelectorTerms, then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.\nIf you remove or change the label of the node where the pod is scheduled, the pod won‚Äôt be removed. In other words, the affinity selection works only at the time of scheduling the pod.\nThe weight field in preferredDuringSchedulingIgnoredDuringExecution is in the range 1-100. For each node that meets all of the scheduling requirements (resource requests, RequiredDuringScheduling affinity expressions, etc.), the scheduler will compute a sum by iterating through the elements of this field and adding ‚Äúweight‚Äù to the sum if the node matches the corresponding MatchExpressions. This score is then combined with the scores of other priority functions for the node. The node(s) with the highest total score are the most preferred.\nWe are now ready to delete both pods\nkubectl delete -f ~/environment/pod-nginx.yaml kubectl delete -f ~/environment/pod-with-node-affinity.yaml "
},
{
	"uri": "/beginner/190_ocean/launch_ocean/",
	"title": "Connect Ocean to your EKS Cluster",
	"tags": [],
	"description": "",
	"content": "In this section we will create a new Ocean cluster, associated with your existing EKS cluster.\nStep 1: Create A New Cluster  To get started with the Ocean Creation Wizard, select \u0026ldquo;Cloud Clusters\u0026rdquo; from the side menu, under \u0026ldquo;Ocean\u0026rdquo;, and click the \u0026ldquo;Create Cluster\u0026rdquo; button on the top right. On the Use Cases page, select \u0026ldquo;Migrate Worker Nodes' Configuration\u0026rdquo; under \u0026ldquo;Join an Existing Cluster‚Äù:   Step 2: General Settings  Enter a Cluster Name and Identifier and select the Region of your EKS cluster. The Cluster Identifier for your Ocean cluster should be unique within the account, and defaults to the Ocean Cluster Name.\n  Select an EKS Auto Scaling Group (or alternatively, an Instance which should be an existing worker node) to import the cluster configuration from. Click on Next. Ocean will now import the configuration of your EKS cluster. When importing a cluster, Ocean will clone your cluster and node pools configuration. New instances will then be launched and registered directly to your cluster, and will not be visible via your node pools. Your existing instances and applications will remain unchanged.\n   Step 3: Compute Settings  Confirm or change the settings imported by the Ocean Creation Wizard. By default, Ocean will use as wide a selection of instance types as possible, in order to ensure optimal pricing and availabilty for your worker nodes by tapping into many EC2 Spot capacity pools. If you wish, you can exclude certain types from the pool of instances used by the cluster, by clicking on \u0026ldquo;Customize\u0026rdquo; under \u0026ldquo;Machine Types\u0026rdquo;.\n   Step 4: Connectivity Configuration  Create a token with the \u0026ldquo;Generate Token\u0026rdquo; link, or use an existing one. Install the Controller pod. Learn more about the Controller Pod and Ocean‚Äôs anatomy here. Click Test Connectivity to ensure the controller\u0026rsquo;s functionality. Once the connectivity test is successful, click Next to proceed to the Review screen.   Step 5: Review And Create  On this page you can review your configuration, and check it out in JSON or Terraform formats. When you are satisfied with all settings, click Create.   Step 6: Migrating Existing Nodes In order to fully migrate any existing workloads to Ocean, the original EKS Auto Scaling group/s should be gradually drained and scaled down, while replacement nodes should be launched by Ocean. In order to make this process automatic and safe, and gradually migrate all workloads while maintaining high availablity for the application, Ocean has the \u0026ldquo;Workload Migration\u0026rdquo; feature. You can read about it here, or watch the video tutorial here.\nIn the interest of stability, the Workload Migration process is very gradual, and therefore takes a while (up to half an hour), even for small workloads. So for the purposes of this workshop we will assume that our workloads can tolerate a more aggesive rescheduling. Therefore, proceed with the following steps:\n If you have installed \u0026ldquo;Cluster Autoscaler\u0026rdquo; or set up any scaling poilicies for the orginal ASG managed by your EKS cluster, go ahead and disable them. Ocean\u0026rsquo;e Autoscaler will take their place. Find the ASG associated with your EKS cluster in the EC2 console, right click it and select \u0026ldquo;edit\u0026rdquo;. Set the Desired Capacity, Min and Max values to 0. If you have any pods running, Ocean\u0026rsquo;s autoscaler will pick them up and scale up appropriately.  If you have several node groups configured, with different sets of labels, taints or launch specifications, before scaling them down make sure to configure matching \u0026ldquo;Launch Specifications\u0026rdquo; in Ocean. Have a look at the next page in the workshop to see how.\n You‚Äôre all set! Ocean will now ensure your EKS cluster worker nodes are running on the most cost-effective, optimally sized instances possible.\n"
},
{
	"uri": "/intermediate/270_custom_resource_definition/creating_co/",
	"title": "Create Custom Objects",
	"tags": [],
	"description": "",
	"content": "After the CustomResourceDefinition object has been created, you can create custom objects. Custom objects can contain custom fields. These fields can contain arbitrary JSON. In the following example, the cronSpec and image custom fields are set in a custom object of kind CronTab. The kind CronTab comes from the spec of the CustomResourceDefinition object you created above.\nIf you save the following YAML to my-crontab.yaml:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/my-crontab.yaml apiVersion: \u0026quot;stable.example.com/v1\u0026quot; kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: \u0026quot;* * * * */5\u0026quot; image: my-awesome-cron-image EoF and create it:\nkubectl apply -f my-crontab.yaml You can then manage your CronTab objects using kubectl. For example:\nkubectl get crontab Should print a list like this:\nNAME AGE my-new-cron-object 6s Resource names are not case-sensitive when using kubectl, and you can use either the singular or plural forms defined in the CRD, as well as any short names.\nYou can also view the raw YAML data:\nkubectl get ct -o yaml You should see that it contains the custom cronSpec and image fields from the yaml you used to create it:\napiVersion: v1 items: - apiVersion: stable.example.com/v1 kind: CronTab metadata: creationTimestamp: 2017-05-31T12:56:35Z generation: 1 name: my-new-cron-object namespace: default resourceVersion: \u0026quot;285\u0026quot; selfLink: /apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object uid: 9423255b-4600-11e7-af6a-28d2447dc82b spec: cronSpec: '* * * * */5' image: my-awesome-cron-image kind: List metadata: resourceVersion: \u0026quot;\u0026quot; selfLink: \u0026quot;\u0026quot; We can also describe the custom object with kubectl:\nkubectl describe crontab The output being something like this:\nName: my-new-cron-object Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026quot;apiVersion\u0026quot;:\u0026quot;stable.example.com/v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;CronTab\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;my-new-cron-object\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;cronSpe... API Version: stable.example.com/v1 Kind: CronTab Metadata: Creation Timestamp: 2019-05-09T18:10:35Z Generation: 1 Resource Version: 3274450 Self Link: /apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object UID: bdc71d84-7285-11e9-a54d-0615623ca50e Spec: Cron Spec: * * * * */5 Image: my-awesome-cron-image Events: \u0026lt;none\u0026gt; Or we can check the resource directly from the Kubernetes API. First, we start the proxy in one tab of the Cloud9 environment:\nkubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true And in another tab we check the existance of the Custom Resource\ncurl -i 127.0.0.1:8080/apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object With the output:\nHTTP/1.1 200 OK Audit-Id: 04c5ce6e-5a45-4064-8139-6c2b848bc467 Content-Length: 707 Content-Type: application/json Date: Thu, 09 May 2019 18:18:21 GMT {\u0026quot;apiVersion\u0026quot;:\u0026quot;stable.example.com/v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;CronTab\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{\u0026quot;kubectl.kubernetes.io/last-applied-configuration\u0026quot;:\u0026quot;{\\\u0026quot;apiVersion\\\u0026quot;:\\\u0026quot;stable.example.com/v1\\\u0026quot;,\\\u0026quot;kind\\\u0026quot;:\\\u0026quot;CronTab\\\u0026quot;,\\\u0026quot;metadata\\\u0026quot;:{\\\u0026quot;annotations\\\u0026quot;:{},\\\u0026quot;name\\\u0026quot;:\\\u0026quot;my-new-cron-object\\\u0026quot;,\\\u0026quot;namespace\\\u0026quot;:\\\u0026quot;default\\\u0026quot;},\\\u0026quot;spec\\\u0026quot;:{\\\u0026quot;cronSpec\\\u0026quot;:\\\u0026quot;* * * * */5\\\u0026quot;,\\\u0026quot;image\\\u0026quot;:\\\u0026quot;my-awesome-cron-image\\\u0026quot;}}\\n\u0026quot;},\u0026quot;creationTimestamp\u0026quot;:\u0026quot;2019-05-09T18:10:35Z\u0026quot;,\u0026quot;generation\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;my-new-cron-object\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;resourceVersion\u0026quot;:\u0026quot;3274450\u0026quot;,\u0026quot;selfLink\u0026quot;:\u0026quot;/apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object\u0026quot;,\u0026quot;uid\u0026quot;:\u0026quot;bdc71d84-7285-11e9-a54d-0615623ca50e\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;cronSpec\u0026quot;:\u0026quot;* * * * */5\u0026quot;,\u0026quot;image\u0026quot;:\u0026quot;my-awesome-cron-image\u0026quot;}} "
},
{
	"uri": "/beginner/180_fargate/creating-profile/",
	"title": "Creating a Fargate Profile",
	"tags": [],
	"description": "",
	"content": "The Fargate profile allows an administrator to declare which pods run on Fargate. Each profile can have up to five selectors that contain a namespace and optional labels. You must define a namespace for every selector. The label field consists of multiple optional key-value pairs. Pods that match a selector (by matching a namespace for the selector and all of the labels specified in the selector) are scheduled on Fargate.\nIt is generally a good practice to deploy user application workloads into namespaces other than kube-system or default so that you have more fine-grained capabilities to manage the interaction between your pods deployed on to EKS. You will now create a new Fargate profile named applications that targets all pods destined for the fargate namespace.\nCreate a Fargate profile eksctl create fargateprofile \\  --cluster eksworkshop-eksctl \\  --name game-2048 \\  --namespace game-2048  Fargate profiles are immutable. However, you can create a new updated profile to replace an existing profile and then delete the original after the updated profile has finished creating\n When your EKS cluster schedules pods on Fargate, the pods will need to make calls to AWS APIs on your behalf to do things like pull container images from Amazon ECR. The Fargate Pod Execution Role provides the IAM permissions to do this. This IAM role is automatically created for you by the above command.\nCreation of a Fargate profile can take up to several minutes. Execute the following command after the profile creation is completed and you should see output similar to what is shown below.\neksctl get fargateprofile \\  --cluster eksworkshop-eksctl \\  -o yaml Output: - name: game-2048 podExecutionRoleARN: arn:aws:iam::197520326489:role/eksctl-eksworkshop-eksctl-FargatePodExecutionRole-1NOQE05JKQEED selectors: - namespace: game-2048 subnets: - subnet-02783ce3799e77b0b - subnet-0aa755ffdf08aa58f - subnet-0c6a156cf3d523597  Notice that the profile includes the private subnets in your EKS cluster. Pods running on Fargate are not assigned public IP addresses, so only private subnets (with no direct route to an Internet Gateway) are supported when you create a Fargate profile. Hence, while provisioning an EKS cluster, you must make sure that the VPC that you create contains one or more private subnets. When you create an EKS cluster with eksctl utility, under the hoods it creates a VPC that meets these requirements.\n"
},
{
	"uri": "/beginner/090_rbac/install_test_pods/",
	"title": "Install Test Pods",
	"tags": [],
	"description": "",
	"content": "In this tutorial, we\u0026rsquo;re going to demonstrate how to provide limited access to pods running in the rbac-test namespace for a user named rbac-user.\nTo do that, let\u0026rsquo;s first create the rbac-test namespace, and then install nginx into it:\nkubectl create namespace rbac-test kubectl create deploy nginx --image=nginx -n rbac-test To verify the test pods were properly installed, run:\nkubectl get all -n rbac-test Output should be similar to:\nNAME READY STATUS RESTARTS AGE pod/nginx-5c7588df-8mvxx 1/1 Running 0 48s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx 1/1 1 1 48s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-5c7588df 1 1 1 48s  "
},
{
	"uri": "/020_prerequisites/self_paced/",
	"title": "...on your own",
	"tags": [],
	"description": "",
	"content": "Running the workshop on your own Only complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Kubecon, Immersion Day, etc), go to Start the workshop at an AWS event.\n  Create an AWS account   "
},
{
	"uri": "/intermediate/245_x-ray/x-ray-daemon/",
	"title": "Deploy X-Ray DaemonSet",
	"tags": [],
	"description": "",
	"content": "Now that we have created a service account for X-Ray, we are going to deploy the X-Ray DaemonSet to the EKS cluster. The X-Ray daemon will be deployed to each worker node in the EKS cluster. For reference, see the example implementation used in this module.\nThe AWS X-Ray SDKs are used to instrument your microservices. When using the DaemonSet in the example implementation, you need to configure it to point to xray-service.default:2000.\nThe following showcases how to configure the X-Ray SDK for Go. This is merely an example and not a required step in the workshop.\nfunc init() { xray.Configure(xray.Config{ DaemonAddr: \u0026quot;xray-service.default:2000\u0026quot;, LogLevel: \u0026quot;info\u0026quot;, }) } To deploy the X-Ray DaemonSet:\nkubectl create -f https://eksworkshop.com/intermediate/245_x-ray/daemonset.files/xray-k8s-daemonset.yaml To see the status of the X-Ray DaemonSet:\nkubectl describe daemonset xray-daemon The folllowing is an example of the command output:\nTo view the logs for all of the X-Ray daemon pods run the following\n kubectl logs -l app=xray-daemon "
},
{
	"uri": "/intermediate/220_codepipeline/configmap/",
	"title": "Modify aws-auth ConfigMap",
	"tags": [],
	"description": "",
	"content": "Now that we have the IAM role created, we are going to add the role to the aws-auth ConfigMap for the EKS cluster.\nOnce the ConfigMap includes this new role, kubectl in the CodeBuild stage of the pipeline will be able to interact with the EKS cluster via the IAM role.\nROLE=\u0026quot; - rolearn: arn:aws:iam::${ACCOUNT_ID}:role/EksWorkshopCodeBuildKubectlRole\\n username: build\\n groups:\\n - system:masters\u0026quot; kubectl get -n kube-system configmap/aws-auth -o yaml | awk \u0026quot;/mapRoles: \\|/{print;print \\\u0026quot;$ROLE\\\u0026quot;;next}1\u0026quot; \u0026gt; /tmp/aws-auth-patch.yml kubectl patch configmap/aws-auth -n kube-system --patch \u0026quot;$(cat /tmp/aws-auth-patch.yml)\u0026quot;  If you would like to edit the aws-auth ConfigMap manually, you can run: $ kubectl edit -n kube-system configmap/aws-auth\n "
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/cwalarms/",
	"title": "Using CloudWatch Alarms",
	"tags": [],
	"description": "",
	"content": "You can use the CloudWatch metrics to generate various alarms for your EKS Cluster based on assigned metrics.\nIn CloudWatch Container Insights we‚Äôre going to drill down to create an alarm using CloudWatch for CPU Utilization of the Wordpress service.\nTo do so:\n Click on the three vertical dots in the upper right of the CPU Utilization box. Select View in Metrics.  This will isolate us to a single pane view of CPU Utilization for the eksworkshop-eksctl cluster.\nFrom this window we can create alarms for the understood-zebu-wordpress service so we know when it‚Äôs under heavy load.\nFor this lab we‚Äôre going to set the threshold low so we can guarantee to set it off with the load test.\n To create an alarm, click on the small bell icon in line with the Wordpress service. This will take you to the metrics alarm configuration screen.\nAs we can see from the screen we peaked CPU at over 6 % so we‚Äôre going to set our metric to 3% to assure it sets off an alarm. Set your alarm to 50% of whatever you max was during the load test on the graph.\nClick next on the bottom and continue to Configure Actions.\nWe‚Äôre going to create a configuration to send an SNS alert to your email address when CPU gets above your threshold.\nOn the Configure Action screen:\n Leave default of in Alarm. Select Create new topic under Select and SNS Topic. In Create new topic\u0026hellip; name it wordpress-CPU-alert. In Email Endpoints enter your email address. Click create topic.  Once those items are set, you can click Next at the bottom of the screen.\nOn the next screen we‚Äôll add a unique name for our alert, and press Next.\nThe next screen will show your metric and the conditions. Make sure to click create alarm.\nAfter creating your new SNS topic you will need to verify your subscription in your email. Testing your alarm For the last step of this lab, we‚Äôre going to run one more load test on our site to verify our alarm triggers. Go back to your Cloud9 terminal and run the same commands we can previously to load up our Wordpress site.\ni.e.\nexport WP_ELB=$(kubectl -n wordpress-cwi get svc understood-zebu-wordpress -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[].hostname}\u0026#34;) siege -q -t 15S -c 200 -i ${WP_ELB} In a minute or two, you should receive and email about your CPU being in alert. If you don‚Äôt verify your SNS topic configuration and that you‚Äôve accepted the subscription to the topic.\n"
},
{
	"uri": "/beginner/200_secrets/sealed-secrets/",
	"title": "Sealed Secrets for Kubernetes",
	"tags": [],
	"description": "",
	"content": "How it Works Sealed Secrets is composed of two parts:\n A cluster-side controller A client-side utility called kubeseal  Upon startup, the controller looks for a cluster-wide private/public key pair, and generates a new 4096 bit RSA key pair if not found. The private key is persisted in a Secret object in the same namespace as that of the controller. The public key portion of this is made publicly available to anyone wanting to use SealedSecrets with this cluster.\nDuring encryption, each value in the original Secret is symmetrically encrypted using AES-256 with a randomly-generated session key. The session key is then asymmetrically encrypted with the controller\u0026rsquo;s public key using SHA256 and the original Secret\u0026rsquo;s namespace/name as the input parameter. The output of the encryption process is a string that is constructed as follows:\nlength (2 bytes) of encrypted session key + encrypted session key + encrypted Secret\nWhen a SealedSecret custom resource is deployed to the Kubernetes cluster, the controller will pick it up, unseal it using the private key and create a Secret resource. During decryption, the SealedSecret\u0026rsquo;s namespace/name is used again as the input parameter. This ensures that the SealedSecret and Secret are strictly tied to the same namespace and name.\nThe companion CLI tool kubeseal is used for creating a SealedSecret custom resource definition (CRD) from a Secret resource definition using the public key. kubeseal can communicate with the controller through the Kubernetes API server and retrieve the public key needed for encrypting a Secret at run-time. The public key may also be downloaded from the controller and saved locally to be used offline.\n"
},
{
	"uri": "/beginner/190_efs/efs-csi-driver/",
	"title": "EFS Provisioner for EKS with CSI Driver",
	"tags": [],
	"description": "",
	"content": "About the Amazon EFS CSI Driver On Amazon EKS, the open-source EFS Container Storage Interface (CSI) driver is used to manage the attachment of Amazon EFS volumes to Kubernetes Pods.\nDeploy EFS CSI Driver We are going to deploy the driver using the stable release:\nkubectl apply -k \u0026quot;github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.0\u0026quot; Verify pods have been deployed:\nkubectl get pods -n kube-system Should return new pods with csi driver:\nOutput: NAME READY STATUS RESTARTS AGE efs-csi-node-8hgpt 3/3 Running 0 6h11m efs-csi-node-d7r47 3/3 Running 0 6h11m efs-csi-node-fs49j 3/3 Running 0 6h11m  Create Persistent Volume Next we will deploy a persistent volume using the EFS created.\nmkdir ~/environment/efs cd ~/environment/efs wget https://eksworkshop.com/beginner/190_efs/efs.files/efs-pvc.yaml We need to update this manifest with the EFS ID created:\nsed -i \u0026quot;s/EFS_VOLUME_ID/$FILE_SYSTEM_ID/g\u0026quot; efs-pvc.yaml And then apply:\nkubectl apply -f efs-pvc.yaml Next, check if a PVC resource was created. The output from the command should look similar to what is shown below, with the STATUS field set to Bound.\nkubectl get pvc -n storage Output: NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE efs-storage-claim Bound efs-pvc 5Gi RWX efs-sc 4s  A PV corresponding to the above PVC is dynamically created. Check its status with the following command.\nkubectl get pv Output: NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE efs-pvc 5Gi RWX Retain Bound storage/efs-storage-claim efs-sc 20s  "
},
{
	"uri": "/intermediate/201_resource_management/resource-quota/",
	"title": "Resource Quotas",
	"tags": [],
	"description": "",
	"content": "ResourceQuotas are used to limit resources like cpu,memory, storage, and services. In this section we will set up ResourceQuotas between two teams blue and red.\n# Create different namespaces kubectl create namespace blue kubectl create namespace red Create Resource Quota In this example environment we have two teams are sharing the same resources. The Red team is limited on number of Load Balancers provisioned and Blue team is restricted on memory/cpu usage.\nkubectl create quota blue-team --hard=limits.cpu=1,limits.memory=1G --namespace blue kubectl create quota red-team --hard=services.loadbalancers=1 --namespace red  A list of objects that quotas can be applied to can be found here\n Create Pods In next steps we will evaluate failed and successful attempts at creating resources.\nFailed Attempts Errors will occur when creating pods outside of the ResourceQuota specifications.\n# Error when creating a resource without defined limit kubectl run --namespace blue --image hande007/stress-ng blue-cpu-pod --restart=Never -- --vm-keep --vm-bytes 512m --timeout 600s --vm 2 --oomable --verbose # Error when creating a deployment without specifying limits (Replicaset has errors) kubectl create --namespace blue deployment blue-cpu-deploy --image hande007/stress-ng kubectl describe --namespace blue replicaset -l app=blue-cpu-deploy # Error when creating more than one AWS Load Balancer kubectl run --namespace red --image nginx:latest red-nginx-pod --restart=Never --limits=cpu=0.1,memory=100M kubectl expose --namespace red pod red-nginx-pod --port 80 --type=LoadBalancer --name red-nginx-service-1 kubectl expose --namespace red pod red-nginx-pod --port 80 --type=LoadBalancer --name red-nginx-service-2 Successful Attempts We create resources in blue namespace to use 75% of allocated resources\n# Create Pod kubectl run --namespace blue --limits=cpu=0.25,memory=250M --image nginx blue-nginx-pod-1 --restart=Never --restart=Never kubectl run --namespace blue --limits=cpu=0.25,memory=250M --image nginx blue-nginx-pod-2 --restart=Never --restart=Never kubectl run --namespace blue --limits=cpu=0.25,memory=250M --image nginx blue-nginx-pod-3 --restart=Never --restart=Never Verify Current Resource Quota Usage We can query ResourceQuota to see current utilization.\nkubectl describe quota blue-team --namespace blue kubectl describe quota red-team --namespace red Clean Up Clean up the pods before moving on to free up resources\nkubectl delete namespace red kubectl delete namespace blue "
},
{
	"uri": "/beginner/191_secrets/cleanup/",
	"title": "Cleanup The Lab",
	"tags": [],
	"description": "",
	"content": "Remove The Namespace Let\u0026rsquo;s delete the namespace for this exercise:\nrm -f test-creds rm -f podconsumingsecret.yaml kubectl delete ns secretslab Output:\nnamespace \u0026#34;secretslab\u0026#34; deleted  This cleans up the secret and pod we deployed for this lab.\n"
},
{
	"uri": "/beginner/130_exposing-service/exposing/",
	"title": "Exposing the Service",
	"tags": [],
	"description": "",
	"content": "Exposing the Service For some parts of your applications you may want to expose a Service onto an external IP address. Kubernetes supports two ways of doing this: NodePort and LoadBalancer.\nkubectl -n my-nginx get svc my-nginx Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx ClusterIP 10.100.225.196 \u0026lt;none\u0026gt; 80/TCP 33m  Currently the Service does not have an External IP, so let‚Äôs now patch the Service to use a cloud load balancer, by updating the type of the my-nginx Service from ClusterIP to LoadBalancer:\nkubectl -n my-nginx patch svc my-nginx -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; We can check for the changes:\nkubectl -n my-nginx get svc my-nginx Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx LoadBalancer 10.100.225.196 aca434079a4cb0a9961170c1-23367063.us-west-2.elb.amazonaws.com 80:30470/TCP 39m  The Load Balancer can take a couple of minutes in being available on the DNS.\n Now, let\u0026rsquo;s try if it\u0026rsquo;s accessible.\nexport loadbalancer=$(kubectl -n my-nginx get svc my-nginx -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[*].hostname}\u0026#39;) curl -k -s http://${loadbalancer} | grep title Output \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt;  If the Load Balancer name is too long to fit in the standard kubectl get svc output, you‚Äôll need to do kubectl describe service my-nginx to see it. You‚Äôll see something like this:\nkubectl -n my-nginx describe service my-nginx | grep Ingress Output LoadBalancer Ingress: a320587ffd19711e5a37606cf4a74574-1142138393.us-east-1.elb.amazonaws.com  "
},
{
	"uri": "/beginner/140_assigning_pods/affinity_usecases/",
	"title": "More Practical use-cases",
	"tags": [],
	"description": "",
	"content": "AntiAffinity can be even more useful when they are used with higher level collections such as ReplicaSets, StatefulSets, Deployments, etc. One can easily configure a set of workloads to be co-located in the same defined topology, eg., the same node.\nAlways co-located in the same node In a three-node cluster, a web application has an in-memory cache such as redis. We want the web-servers to be co-located with the cache whenever possible.\nHere is the YAML snippet of a simple redis deployment with three replicas and selector label app=store. The deployment has PodAntiAffinity configured to ensure the scheduler does not co-locate replicas on a single node.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/redis-with-node-affinity.yaml apiVersion: apps/v1 kind: Deployment metadata: name: redis-cache spec: selector: matchLabels: app: store replicas: 3 template: metadata: labels: app: store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; containers: - name: redis-server image: redis:3.2-alpine EoF The below YAML snippet of the web-server deployment has podAntiAffinity and podAffinity configured. This informs the scheduler that all its replicas are to be co-located with pods that have selector label app=store. This will also ensure that each web-server replica does not co-locate on a single node.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/web-with-node-affinity.yaml apiVersion: apps/v1 kind: Deployment metadata: name: web-server spec: selector: matchLabels: app: web-store replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; containers: - name: web-app image: nginx:1.12-alpine EoF Let\u0026rsquo;s apply these Deployments:\nkubectl apply -f ~/environment/redis-with-node-affinity.yaml kubectl apply -f ~/environment/web-with-node-affinity.yaml When we create the above two deployments, our three node cluster should look like:\nnode-1 - webserver-1 - cache-1\nnode-2 - webserver-2 - cache-2\nnode-3 - webserver-3 - cache-3\nAs you can see, all 3 web-server pod replicas are automatically co-located with the redis cache pods as expected.\n# We will use --sort-by to filter by nodes name kubectl get pods -o wide --sort-by=\u0026#39;.spec.nodeName\u0026#39; NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-cache-d5f6b6855-67w4d 1/1 Running 0 9s 192.168.121.92 ip-192-168-97-12.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; web-server-7886dfdc59-nx489 1/1 Running 0 8s 192.168.125.78 ip-192-168-97-12.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; redis-cache-d5f6b6855-z79wr 1/1 Running 0 9s 192.168.156.139 ip-192-168-155-36.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; web-server-7886dfdc59-zswwd 1/1 Running 0 8s 192.168.141.93 ip-192-168-155-36.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; redis-cache-d5f6b6855-snzwb 1/1 Running 0 9s 192.168.173.143 ip-192-168-168-110.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; web-server-7886dfdc59-ljwsk 1/1 Running 0 8s 192.168.166.141 ip-192-168-168-110.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  "
},
{
	"uri": "/beginner/180_fargate/prerequisites-for-alb/",
	"title": "Setting up the LB controller",
	"tags": [],
	"description": "",
	"content": "AWS Load Balancer Controller The AWS ALB Ingress Controller has been rebranded to AWS Load Balancer Controller.\n\u0026ldquo;AWS Load Balancer Controller\u0026rdquo; is a controller to help manage Elastic Load Balancers for a Kubernetes cluster.\n It satisfies Kubernetes Ingress resources by provisioning Application Load Balancers. It satisfies Kubernetes Service resources by provisioning Network Load Balancers.  Helm We will use Helm to install the ALB Ingress Controller.\nCheck to see if helm is installed:\nhelm version  If Helm is not found, see installing helm for instructions.\n Create IAM OIDC provider First, we will have to set up an OIDC provider with the cluster.\nThis step is required to give IAM permissions to a Fargate pod running in the cluster using the IAM for Service Accounts feature.\nLearn more about IAM Roles for Service Accounts in the Amazon EKS documentation.\n eksctl utils associate-iam-oidc-provider \\  --region ${AWS_REGION} \\  --cluster eksworkshop-eksctl \\  --approve Create an IAM policy The next step is to create the IAM policy that will be used by the AWS Load Balancer Controller.\nThis policy will be later associated to the Kubernetes Service Account and will allow the controller pods to create and manage the ELB‚Äôs resources in your AWS account for you.\naws iam create-policy \\  --policy-name AWSLoadBalancerControllerIAMPolicy \\  --policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json Create a IAM role and ServiceAccount for the Load Balancer controller Next, create a Kubernetes Service Account by executing the following command\neksctl create iamserviceaccount \\  --cluster eksworkshop-eksctl \\  --namespace kube-system \\  --name aws-load-balancer-controller \\  --attach-policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy \\  --override-existing-serviceaccounts \\  --approve The above command deploys a CloudFormation template that creates an IAM role and attaches the IAM policy to it.\nThe IAM role gets associated with a Kubernetes Service Account. You can see details of the service account created with the following command.\nkubectl get sa aws-load-balancer-controller -n kube-system -o yaml Output\napiVersion: v1 kind: ServiceAccount metadata: annotations: eks.amazonaws.com/role-arn: arn:aws:iam::\u0026lt;AWS_ACCOUNT_ID\u0026gt;:role/eksctl-eksworkshop-eksctl-addon-iamserviceac-Role1-1MMJRJ4LWWHD8 creationTimestamp: \u0026#34;2020-12-04T19:31:57Z\u0026#34; name: aws-load-balancer-controller namespace: kube-system resourceVersion: \u0026#34;3094\u0026#34; selfLink: /api/v1/namespaces/kube-system/serviceaccounts/aws-load-balancer-controller uid: aa940b27-796e-4cda-bbba-fe6ca8207c00 secrets: - name: aws-load-balancer-controller-token-8pnww  For more information on IAM Roles for Service Accounts follow this link.\n Install the TargetGroupBinding CRDs kubectl apply -k github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master Deploy the Helm chart from the Amazon EKS charts repo Fist, We will verify if the AWS Load Balancer Controller version has beed set\nif [ ! -x ${LBC_VERSION} ] then tput setaf 2; echo \u0026#39;${LBC_VERSION} has been set.\u0026#39; else tput setaf 1;echo \u0026#39;${LBC_VERSION} has NOT been set.\u0026#39; fi  If the result is ${LBC_VERSION} has NOT been set., click here for the instructions.\n helm repo add eks https://aws.github.io/eks-charts export VPC_ID=$(aws eks describe-cluster \\  --name eksworkshop-eksctl \\  --query \u0026#34;cluster.resourcesVpcConfig.vpcId\u0026#34; \\  --output text) helm upgrade -i aws-load-balancer-controller \\  eks/aws-load-balancer-controller \\  -n kube-system \\  --set clusterName=eksworkshop-eksctl \\  --set serviceAccount.create=false \\  --set serviceAccount.name=aws-load-balancer-controller \\  --set image.tag=\u0026#34;${LBC_VERSION}\u0026#34; \\  --set region=${AWS_REGION} \\  --set vpcId=${VPC_ID} You can check if the deployment has completed\nkubectl -n kube-system rollout status deployment aws-load-balancer-controller "
},
{
	"uri": "/020_prerequisites/aws_event/",
	"title": "...at an AWS event",
	"tags": [],
	"description": "",
	"content": "Running the workshop at an AWS Event Only complete this section if you are at an AWS hosted event (such as re:Invent, Kubecon, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to:\nStart the workshop on your own.\n  AWS Workshop Portal   "
},
{
	"uri": "/intermediate/245_x-ray/microservices/",
	"title": "Deploy Example Microservices",
	"tags": [],
	"description": "",
	"content": "We now have the foundation in place to deploy microservices, which are instrumented with X-Ray SDKs, to the EKS cluster.\nIn this step, we are going to deploy example front-end and back-end microservices to the cluster. The example services are already instrumented using the X-Ray SDK for Go. Currently, X-Ray has SDKs for Go, Python, Node.js, Ruby, .NET and Java.\nkubectl apply -f https://eksworkshop.com/intermediate/245_x-ray/sample-front.files/x-ray-sample-front-k8s.yml kubectl apply -f https://eksworkshop.com/intermediate/245_x-ray/sample-back.files/x-ray-sample-back-k8s.yml To review the status of the deployments, you can run:\nkubectl describe deployments x-ray-sample-front-k8s x-ray-sample-back-k8s For the status of the services, run the following command:\nkubectl describe services x-ray-sample-front-k8s x-ray-sample-back-k8s Once the front-end service is deployed, run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser.\nkubectl get service x-ray-sample-front-k8s -o wide After your ELB is deployed and available, open up the endpoint returned by the previous command in your browser and allow it to remain open. The front-end application makes a new request to the /api endpoint once per second, which in turn calls the back-end service. The JSON document displayed in the browser is the result of the request made to the back-end service.\nThis service was configured with a LoadBalancer so, an AWS Elastic Load Balancer (ELB) is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n When the front-end service is first deployed, it can take up to several minutes for the ELB to be created and DNS updated.\n "
},
{
	"uri": "/intermediate/220_codepipeline/forksample/",
	"title": "Fork Sample Repository",
	"tags": [],
	"description": "",
	"content": "We are now going to fork the sample Kubernetes service so that we will be able modify the repository and trigger builds.\nLogin to GitHub and fork the sample service to your own account:\nhttps://github.com/rnzsgh/eks-workshop-sample-api-service-go\nOnce the repo is forked, you can view it in your your GitHub repositories.\nThe forked repo will look like:\n"
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/wraup/",
	"title": "Wrapping Up",
	"tags": [],
	"description": "",
	"content": "Wrapping Up As you can see it‚Äôs fairly easy to get CloudWatch Container Insights to work, and set alarms for CPU and other metrics.\nWith CloudWatch Container Insights we remove the need to manage and update your own monitoring infrastructure and allow you to use native AWS solutions that you don‚Äôt have to manage the platform for.\n Cleanup your Environment Let\u0026rsquo;s clean up Wordpress so it\u0026rsquo;s not running in your cluster any longer.\nhelm -n wordpress-cwi uninstall understood-zebu kubectl delete namespace wordpress-cwi Run the following command to delete Container Insights from your cluster.\ncurl -s https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed \u0026#34;s/{{cluster_name}}/eksworkshop-eksctl/;s/{{region_name}}/${AWS_REGION}/\u0026#34; | kubectl delete -f - Delete the SNS topic and the subscription.\n# Delete the SNS Topic aws sns delete-topic \\  --topic-arn arn:aws:sns:${AWS_REGION}:${ACCOUNT_ID}:wordpress-CPU-Alert # Delete the subscription aws sns unsubscribe \\  --subscription-arn $(aws sns list-subscriptions | jq -r \u0026#39;.Subscriptions[].SubscriptionArn\u0026#39;) Finally we will remove the CloudWatchAgentServerPolicy policy from the Nodes IAM Role\naws iam detach-role-policy \\  --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \\  --role-name ${ROLE_NAME} Thank you for using CloudWatch Container Insights! There is a lot more to learn about our Observability features using Amazon CloudWatch and AWS X-Ray. Take a look at our One Observability Workshop\n "
},
{
	"uri": "/beginner/200_secrets/installing-sealed-secrets/",
	"title": "Installing Sealed Secrets",
	"tags": [],
	"description": "",
	"content": "Installing the kubeseal Client For Linux x86_64 systems, the client-tool may be installed into /usr/local/bin with the following command:\nwget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.16.0/kubeseal-linux-amd64 -O kubeseal sudo install -m 755 kubeseal /usr/local/bin/kubeseal For MacOS systems, the client-tool is installed as follows:\nbrew install kubeseal Installing the Custom Controller and CRD for SealedSecret Install the SealedSecret CRD, controller and RBAC artifacts on your EKS cluster as follows:\nwget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.16.0/controller.yaml kubectl apply -f controller.yaml Check the status of the controller pod.\nkubectl get pods -n kube-system | grep sealed-secrets-controller Output: sealed-secrets-controller-7bdbc75d47-5wxvf 1/1 Running 0 60s  The logs printed by the controller reveal the name of the Secret that it created in its namespace, kube-system, and which contais the private key pair used by the controller for unsealing SealedSecrets deployed to the cluster. Note that the name of the controller pod will be different in your cluster.\nkubectl logs sealed-secrets-controller-84fcdcd5fd-9qb5j -n kube-system Output: controller version: v0.16.0 2021/07/14 21:26:59 Starting sealed-secrets controller version: v0.16.0 2021/07/14 21:26:59 Searching for existing private keys 2021/07/14 21:27:01 New key written to kube-system/sealed-secrets-keydw62x 2021/07/14 21:27:01 Certificate is -----BEGIN CERTIFICATE----- MIIErTCCApWgAwIBAgIQR5dpRFfh\u0026#43;\u0026#43;CnGZuOc5bfGjANBgkqhkiG9w0BAQsFADAA MB4XDTIxMDcxNDIxMjcwMVoXDTMxMDcxMjIxMjcwMVowADCCAiIwDQYJKoZIhvcN (...) vqXZrlmfM7ScQRMSnD5QiqaT3I2F2vpZgTyCvto8rcG62lmUAhKqPXqopBRJx\u0026#43;Of K4MhPVDg6t0YdZFYH6\u0026#43;oKW7OGLR2rp4KBoIYfO/KPZMCYVayNiGPQT6kAr2C/pFu Lg== -----END CERTIFICATE----- 2021/07/14 21:27:01 HTTP server serving on :8080  As seen from the logs of the controller, it searches for a Secret with the label sealedsecrets.bitnami.com/sealed-secrets-key in its namespace. If it does not find one, it creates a new one in its namespace and prints the public key portion of the key pair to its output logs. View the contents of the Secret which contais the public/private key pair in YAML format as follows:\nkubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml Output: apiVersion: v1 kind: List metadata: resourceVersion: \u0026#34;\u0026#34; selfLink: \u0026#34;\u0026#34; items: - apiVersion: v1 kind: Secret type: kubernetes.io/tls metadata: creationTimestamp: \u0026#34;2021-07-14T21:27:01Z\u0026#34; generateName: sealed-secrets-key labels: sealedsecrets.bitnami.com/sealed-secrets-key: active name: sealed-secrets-keydw62x namespace: kube-system resourceVersion: \u0026#34;1968\u0026#34; uid: 65cb8421-3b2b-4e64-9499-1e61536bdbc4 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSU(...)S0tCg== tls.key: LS0tLS1CRUdJTiBSU0EgUF(...)S0tLS0K  "
},
{
	"uri": "/intermediate/201_resource_management/pod-priority/",
	"title": "Pod Priority and Preemption",
	"tags": [],
	"description": "",
	"content": "Pod Priority is used to apply importance of a pod relative to other pods. In this section we will create two PriorityClasses and watch the interaction of pods.\nCreate PriorityClass We will create two PriorityClass, low-priority and high-priority.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/resource-management/high-priority-class.yml apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 100 globalDefault: false description: \u0026quot;High-priority Pods\u0026quot; EoF kubectl apply -f ~/environment/resource-management/high-priority-class.yml cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/resource-management/low-priority-class.yml apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: low-priority value: 50 globalDefault: false description: \u0026quot;Low-priority Pods\u0026quot; EoF kubectl apply -f ~/environment/resource-management/low-priority-class.yml  Pods with without a PriorityClass are 0. A global PriorityClass can be assigned. Additional details can be found here\n Deploy low-priority Pods Next we will deploy low-priority pods to use up resources on the nodes. The goal is to saturate the nodes with as many pods as possible.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/resource-management/low-priority-deployment.yml apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-deployment name: nginx-deployment spec: replicas: 50 selector: matchLabels: app: nginx-deployment template: metadata: labels: app: nginx-deployment spec: priorityClassName: \u0026quot;low-priority\u0026quot; containers: - image: nginx name: nginx-deployment resources: limits: memory: 1G EoF kubectl apply -f ~/environment/resource-management/low-priority-deployment.yml Watch the number of available pods in the Deployment until the available stabilizes around a number. This exercise does not require all pods in the deployment to be in Available state. We want to ensure the nodes are completely filled with pods. It may take up to 2 minutes to stabilize.\nkubectl get deployment nginx-deployment --watch Output: NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 5/50 50 5 5s nginx-deployment 6/50 50 6 6s ... nginx-deployment 21/50 50 21 20s nginx-deployment 21/50 50 21 6m  Deploy High Priority Pod In a new terminal watch Deployment using the command below\nkubectl get deployment --watch Next deploy high-priority Deployment to see the how Kubernetes handles PriorityClass.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/resource-management/high-priority-deployment.yml apiVersion: apps/v1 kind: Deployment metadata: labels: app: high-nginx-deployment name: high-nginx-deployment spec: replicas: 5 selector: matchLabels: app: high-nginx-deployment template: metadata: labels: app: high-nginx-deployment spec: priorityClassName: \u0026quot;high-priority\u0026quot; containers: - image: nginx name: high-nginx-deployment resources: limits: memory: 1G EoF kubectl apply -f ~/environment/resource-management/high-priority-deployment.yml What changes did you see?   Expand for output   When the higher-priority deployment is created it started to remove lower-priority pods on the nodes.\n \n"
},
{
	"uri": "/beginner/140_assigning_pods/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Cleaning up To delete the resources used in this chapter\nkubectl delete -f ~/environment/redis-with-node-affinity.yaml kubectl delete -f ~/environment/web-with-node-affinity.yaml kubectl label nodes --all azname- kubectl label nodes --all disktype- "
},
{
	"uri": "/beginner/190_ocean/deploying_apps/",
	"title": "Deploying Applications With Ocean",
	"tags": [],
	"description": "",
	"content": "In this section we will launch a test deployment and see how Ocean handles different node configurations via the \u0026ldquo;Launch Specifications\u0026rdquo; feature.\nEasily Run Multiple Workload Types In One Cluster The challenge of running multiple workload types (separate applications, dev/test environmets, node groups requiring a GPU AMI, etc\u0026hellip;) on the same Kubernetes cluster is applying a unique configuration to each one of the workloads in a heterogeneous environment. When your worker nodes are managed in a standard EKS cluster, usually every workload type is managed separately in a different Auto-scaling group.\nWith Ocean, you can define custom \u0026ldquo;launch specifications\u0026rdquo; which allow you to configure multiple workload types on the same Ocean Cluster. As part of those launch specs, you can configure different sets of labels and taints to go along with a custom AMI, User Data script, Instance Profile, Security Group, Root Volume size and tags which will be used for the nodes that serve your matching pods. This feature ensures the ability to run any type of workload on the same Ocean Cluster.\nLet\u0026rsquo;s see how this works:\n  Navigate to your Ocean Cluster within the Spot.io Console, then click on the Actions menu on the top right and select \u0026ldquo;Launch Specifications\u0026rdquo;.   Here you can see the \u0026ldquo;Default Launch Specification\u0026rdquo; which represents the initial configuration that the Ocean cluster was created with. To add a new configuration, click the \u0026ldquo;Add Launch Specification\u0026rdquo; button on the top right.   Configure the new Launch Specification as follows:\n Set Name to Dev Environment. Under Node Labels, set Key to env, Value to dev and click \u0026ldquo;Add\u0026rdquo;.     Add another Launch Specification by clicking the \u0026ldquo;Add Launch Specification\u0026rdquo; button again, and configure it as follows:\n Set Name to Test Environment. Under Node Labels, set Key to env, Value to test and click \u0026ldquo;Add\u0026rdquo;.     Once you\u0026rsquo;re finished (make sure you have 3 Launch Specifications), click \u0026ldquo;Update\u0026rdquo; at the bottom right of the page.\n  Running a test deployment Now we will run a deployment that will show us how Ocean scales up and automatically launches nodes from the right Launch Specification.\nBelow is an example yaml with 3 test desployments.\nThe first test deployment, named od uses a selector for the env: dev label, and will require On-Demand instances via the spotinst.io/node-lifecycle: od label. You can read more about using built in labels here. The second deployment, named dev will also require the env: dev label, while the third one, named test should run on instances labeled env: test.\ncat \u0026lt;\u0026lt;EoF \u0026gt; test_deployments.yaml apiVersion: apps/v1 kind: Deployment metadata: name: od spec: selector: matchLabels: env: dev replicas: 2 template: metadata: labels: env: dev spec: containers: - name: nginx-od image: nginx resources: requests: memory: \u0026quot;700Mi\u0026quot; cpu: \u0026quot;256m\u0026quot; nodeSelector: spotinst.io/node-lifecycle: od --- apiVersion: apps/v1 kind: Deployment metadata: name: dev spec: selector: matchLabels: env: dev replicas: 3 template: metadata: labels: env: dev spec: containers: - name: nginx-dev image: nginx resources: requests: memory: \u0026quot;800Mi\u0026quot; cpu: \u0026quot;800m\u0026quot; limits: memory: \u0026quot;1700Mi\u0026quot; cpu: \u0026quot;1700m\u0026quot; nodeSelector: env: dev --- apiVersion: apps/v1 kind: Deployment metadata: name: test spec: selector: matchLabels: env: test replicas: 3 template: metadata: labels: env: test spec: containers: - name: nginx-dev image: nginx resources: requests: memory: \u0026quot;1700Mi\u0026quot; cpu: \u0026quot;500m\u0026quot; limits: memory: \u0026quot;1700Gi\u0026quot; cpu: \u0026quot;1700m\u0026quot; nodeSelector: env: test EoF Let\u0026rsquo;s apply these Deployments and watch Ocean\u0026rsquo;s Autoscaler in action:\nkubectl apply -f test_deployments.yaml At this point Ocean will scale up to meet the demands of the deployments. You will notice that autoscaling happens fast, and instance sizes will be optimized for efficient bin packing of resources. We expect to see at least 3 instances:\n Two instances from the Dev Environment launch specification, On-Demand and Spot. One Spot instance from the Test Environment launch specification.  You can display your nodes with:\n kubectl get nodes The output should look like: NAME STATUS ROLES AGE VERSION ip-192-168-15-64.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 7m58s v1.14.8-eks-b8860f ip-192-168-38-150.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 8m24s v1.14.8-eks-b8860f ip-192-168-86-147.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 8m28s v1.14.8-eks-b8860f ip-192-168-92-222.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 8m9s v1.14.8-eks-b8860f  In addition, the scale up activity should be logged in the Ocean Cluster\u0026rsquo;s log tab: Clicking on \u0026ldquo;view details\u0026rdquo; will open up a window with additional information about the scaling activity: In the next slides, we will preview some additional features and benefits of Ocean for EKS.\n"
},
{
	"uri": "/beginner/180_fargate/deploying-fargate/",
	"title": "Deploying Pods to Fargate",
	"tags": [],
	"description": "",
	"content": "Deploy the sample application Deploy the game 2048 as a sample application to verify that the AWS Load Balancer Controller creates an Application Load Balancer as a result of the Ingress object.\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/2048/2048_full.yaml You can check if the deployment has completed\nkubectl -n game-2048 rollout status deployment deployment-2048 Output: Waiting for deployment \u0026#34;deployment-2048\u0026#34; rollout to finish: 0 of 5 updated replicas are available... Waiting for deployment \u0026#34;deployment-2048\u0026#34; rollout to finish: 1 of 5 updated replicas are available... Waiting for deployment \u0026#34;deployment-2048\u0026#34; rollout to finish: 2 of 5 updated replicas are available... Waiting for deployment \u0026#34;deployment-2048\u0026#34; rollout to finish: 3 of 5 updated replicas are available... Waiting for deployment \u0026#34;deployment-2048\u0026#34; rollout to finish: 4 of 5 updated replicas are available... deployment \u0026#34;deployment-2048\u0026#34; successfully rolled out  Next, run the following command to list all the nodes in the EKS cluster and you should see output as follows:\nkubectl get nodes Output: NAME STATUS ROLES AGE VERSION fargate-ip-192-168-110-35.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 47s v1.17.9-eks-a84824 fargate-ip-192-168-142-4.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 47s v1.17.9-eks-a84824 fargate-ip-192-168-169-29.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 55s v1.17.9-eks-a84824 fargate-ip-192-168-174-79.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 39s v1.17.9-eks-a84824 fargate-ip-192-168-179-197.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 50s v1.17.9-eks-a84824 ip-192-168-20-197.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 16h v1.17.11-eks-cfdc40 ip-192-168-33-161.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 16h v1.17.11-eks-cfdc40 ip-192-168-68-228.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 16h v1.17.11-eks-cfdc40  If your cluster has any worker nodes, they will be listed with a name starting wit the ip- prefix.\nIn addition to the worker nodes, if any, there will now be five additional fargate- nodes listed. These are merely kubelets from the microVMs in which your sample app pods are running under Fargate, posing as nodes to the EKS Control Plane. This is how the EKS Control Plane stays aware of the Fargate infrastructure under which the pods it orchestrates are running. There will be a ‚Äúfargate‚Äù node added to the cluster for each pod deployed on Fargate.\n"
},
{
	"uri": "/beginner/130_exposing-service/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "",
	"content": "What is Ingress? Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.\nHere is a simple example where an Ingress sends all its traffic to one Service: An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL/TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.\nAn Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of Service.Type=NodePort or Service.Type=LoadBalancer.\nYou must have an ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect.\nYou may need to deploy an Ingress controller such as AWS Load Balancer Controller. You can choose from a number of Ingress controllers.\nIdeally, all Ingress controllers should fit the reference specification. In reality, the various Ingress controllers operate slightly differently.\nThe Ingress Resource A minimal ingress resource example for ingress-nginx:\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /testpath backend: serviceName: test servicePort: 80  As with all other Kubernetes resources, an Ingress needs apiVersion, kind, and metadata fields. The name of an Ingress object must be a valid DNS subdomain name. For general information about working with config files, see deploying applications, configuring containers, managing resources. Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which is the rewrite-target annotation. Different Ingress controller support different annotations. Review the documentation for your choice of Ingress controller to learn which annotations are supported.\nThe Ingress spec has all the information needed to configure a load balancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Ingress resource only supports rules for directing HTTP traffic.\nIngress rules Each http rule contains the following information:\n An optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. If a host is provided (for example, foo.bar.com), the rules apply to that host. A list of paths (for example, /testpath), each of which has an associated backend defined with a serviceName and servicePort. Both the host and path must match the content of an incoming request before the load balancer will direct traffic to the referenced service. A backend is a combination of service and port names as described in the Services doc. HTTP (and HTTPS) requests to the Ingress matching the host and path of the rule will be sent to the listed backend.  A default backend is often configured in an Ingress controller that will service any requests that do not match a path in the spec.\nDefault Backend An Ingress with no rules sends all traffic to a single default backend. The default backend is typically a configuration option of the Ingress controller and is not specified in your Ingress resources.\nIf none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.\nClick here to read more on that topic.\n "
},
{
	"uri": "/intermediate/220_codepipeline/githubcredentials/",
	"title": "GitHub Access Token",
	"tags": [],
	"description": "",
	"content": "In order for CodePipeline to receive callbacks from GitHub, we need to generate a personal access token.\nOnce created, an access token can be stored in a secure enclave and reused, so this step is only required during the first run or when you need to generate new keys.\nOpen up the New personal access page in GitHub.\nYou may be prompted to enter your GitHub password\n Enter a value for Token description, check the repo permission scope and scroll down and click the Generate token button\nCopy the personal access token and save it in a secure place for the next step\n"
},
{
	"uri": "/intermediate/245_x-ray/x-ray/",
	"title": "X-Ray Console",
	"tags": [],
	"description": "",
	"content": "We now have the example microservices deployed, so we are going to investigate our Service Graph and Traces in X-Ray section of the AWS Management Console.\nThe Service map in the console provides a visual representation of the steps identified by X-Ray for a particular trace. Each resource that sends data to X-Ray within the same context appears as a service in the graph. In the example below, we can see that the x-ray-sample-front-k8s service is processing 39 transactions per minute with an average latency of 0.99ms per operation. Additionally, the x-ray-sample-back-k8s is showing an average latency of 0.08ms per transaction.\nNext, go to the traces section in the AWS Management Console to view the execution times for the segments in the requests. At the top of the page, we can see the URL for the ELB endpoint and the corresponding traces below.\nIf you click on the link on the left in the Trace list section you will see the overall execution time for the request (0.5ms for the x-ray-sample-front-k8s which wraps other segments and subsegments), as well as a breakdown of the individual segments in the request. In this visualization, you can see the front-end and back-end segments and a subsegment named x-ray-sample-back-k8s-gen In the back-end service source code, we instrumented a subsegment that surrounds a random number generator.\nIn the Go example, the main segment is initialized in the xray.Handler helper, which in turn sets all necessary information in the http.Request context struct, so that it can be used when initializing the subsegment.\nClick on the image to zoom\n "
},
{
	"uri": "/beginner/190_fsx_lustre/deploying-services/",
	"title": "Deploying the Stateful Services",
	"tags": [],
	"description": "",
	"content": "Deploy a Kubernetes storage class, persistent volume claim, and sample application to verify that the CSI driver is working This procedure uses the Dynamic volume provisioning for Amazon S3 from the Amazon FSx for Lustre Container Storage Interface (CSI) driver GitHub repository to consume a dynamically-provisioned Amazon FSx for Lustre volume.\n  Create an Amazon S3 bucket and a folder within it named export by creating and copying a file to the bucket.\naws s3 mb s3://$S3_LOGS_BUCKET echo test-file \u0026gt;\u0026gt; testfile aws s3 cp testfile s3://$S3_LOGS_BUCKET/export/testfile   Create the storageclass definition that should interpolate variables from your shell variables that we defined in on the previous page.\ncat \u0026lt;\u0026lt; EOF \u0026gt; storageclass.yaml --- kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fsx-sc provisioner: fsx.csi.aws.com parameters: subnetId: ${SUBNET_ID} securityGroupIds: ${SECURITY_GROUP_ID} s3ImportPath: s3://${S3_LOGS_BUCKET} s3ExportPath: s3://${S3_LOGS_BUCKET}/export deploymentType: SCRATCH_2 mountOptions: - flock EOF Explanation of the settings:\n  subnetId ‚Äì The subnet ID that the Amazon FSx for Lustre file system should be created in. Amazon FSx for Lustre is not supported in all Availability Zones. Open the Amazon FSx for Lustre console at https://console.aws.amazon.com/fsx/ to confirm that the subnet that you want to use is in a supported Availability Zone. The subnet can include your nodes, or can be a different subnet or VPC. If the subnet that you specify is not the same subnet that you have nodes in, then your VPCs must be connected, and you must ensure that you have the necessary ports open in your security groups.\n  securityGroupIds ‚Äì The security group ID for your nodes.\n  s3ImportPath ‚Äì The Amazon Simple Storage Service data repository that you want to copy data from to the persistent volume.\n  s3ExportPath ‚Äì The Amazon S3 data repository that you want to export new or modified files to.\n  deploymentType ‚Äì The file system deployment type. Valid values are SCRATCH_1, SCRATCH_2, and PERSISTENT_1. For more information about deployment types, see Create your Amazon FSx for Lustre file system.\nNote\nThe Amazon S3 bucket for s3ImportPath and s3ExportPath must be the same, otherwise the driver cannot create the Amazon FSx for Lustre file system. The s3ImportPath can stand alone. A random path will be created automatically like s3://ml-training-data-000/FSxLustre20190308T012310Z. The s3ExportPath cannot be used without specifying a value for S3ImportPath.\n    Create the storageclass.\nkubectl apply -f storageclass.yaml   Download the persistent volume claim manifest.\ncurl -o claim.yaml https://raw.githubusercontent.com/kubernetes-sigs/aws-fsx-csi-driver/master/examples/kubernetes/dynamic_provisioning_s3/specs/claim.yaml (Optional) Edit the claim.yaml file. Change the following to one of the increment values listed below, based on your storage requirements and the deploymentType that you selected in a previous step.\nstorage: \u0026lt;1200Gi\u0026gt;  - SCRATCH_2 and PERSISTENT ‚Äì 1.2 TiB, 2.4 TiB, or increments of 2.4 TiB over 2.4 TiB. - SCRATCH_1 ‚Äì 1.2 TiB, 2.4 TiB, 3.6 TiB, or increments of 3.6 TiB over 3.6 TiB.    Create the persistent volume claim.\nkubectl apply -f claim.yaml   Confirm that the file system is provisioned.\nkubectl get pvc Output:\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE fsx-claim Bound pvc-15dad3c1-2365-11ea-a836-02468c18769e 1200Gi RWX fsx-sc 7m37s The STATUS may show as Pending for 5-10 minutes, before changing to Bound. Don\u0026rsquo;t continue with the next step until the STATUS is Bound.\n  Deploy the sample application.\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-fsx-csi-driver/master/examples/kubernetes/dynamic_provisioning_s3/specs/pod.yaml   Verify that the sample application is running.\n kubectl get pods  **Output:**   NAME READY STATUS RESTARTS AGE fsx-app 1/1 Running 0 8s   Access Amazon S3 files from the Amazon FSx for Lustre file system   If you only want to import data and read it without any modification and creation, then you don\u0026rsquo;t need a value for s3ExportPath in your storageclass.yaml file. Verify that data was written to the Amazon FSx for Lustre file system by the sample app.\nkubectl exec fsx-app -- ls /data Output:\nThe sample app wrote the out.txt file to the file system.\nexport out.txt   Archive files to the s3ExportPath. For new files and modified files, you can use the Lustre user space tool to archive the data back to Amazon S3 using the value that you specified for s3ExportPath.\nExport the file back to Amazon S3.\nkubectl exec -ti fsx-app -- lfs hsm_archive /data/out.txt New files aren\u0026rsquo;t synced back to Amazon S3 automatically. In order to sync files to the s3ExportPath, you need to install the Lustre client in your container image and manually run the lfs hsm_archive command. The container should run in privileged mode with the CAP_SYS_ADMIN capability. This example uses a lifecycle hook to install the Lustre client for demonstration purpose. A normal approach is building a container image with the Lustre client.\n  Confirm that the out.txt file was written to the s3ExportPath folder in Amazon S3.\naws s3 ls s3://$S3_LOGS_BUCKET/export/ Output:\n2021-08-10 12:11:35 4553 out.txt 2021-08-10 11:41:21 10 testfile   "
},
{
	"uri": "/beginner/200_secrets/sealing-secrets/",
	"title": "Sealing Your Secrets",
	"tags": [],
	"description": "",
	"content": "First, let\u0026rsquo;s delete the database-credentials Secret and Pod resources that was created earlier in this module and deployed to the octank namespace in the cluster. After this operation, the only Secret that should exist in that namespace will be that of the token generated by Kubernetes for the default service account associated with the octank namespace.\nkubectl delete pod pod-variable pod-volume -n octank kubectl delete secret database-credentials -n octank kubectl get secret -n octank Output: NAME TYPE DATA AGE default-token-gv8nr kubernetes.io/service-account-token 3 2d23h  Now, let\u0026rsquo;s reuse the Secret created previously to create SealedSecret YAML manifests with kubeseal.\ncd ~/environment/secrets kubeseal --format=yaml \u0026lt; secret.yaml \u0026gt; sealed-secret.yaml An alternative approach is to fetch the public key from the controller and use it offline to seal your Secrets\nkubeseal --fetch-cert \u0026gt; public-key-cert.pem kubeseal --cert=public-key-cert.pem --format=yaml \u0026lt; secret.yaml \u0026gt; sealed-secret.yaml View the contents of the regular Secret and the corresponding SealedSecret with the following commands:\ncat secret.yaml cat sealed-secret.yaml Output of secret.yaml : apiVersion: v1 data: password: VHJ1NXROMCE= username: YWRtaW4= kind: Secret metadata: name: database-credentials namespace: octank type: Opaque  Output of sealed-secret.yaml: apiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: creationTimestamp: null name: database-credentials namespace: octank spec: encryptedData: password: AgBR2S0Orv6YKLgEboBbPBsV6PX9gP9gUUTrISv(...)daQtadX6XnFqoYwOQ== username: AgBtkiM4Q7w3kesVDbZgKOpiVQ\u0026#43;9XMmlqr9xABf(...)B9quqLBF80QFSsZpQ== template: data: null metadata: creationTimestamp: null name: database-credentials namespace: octank type: Opaque  Note that the keys in the original Secret, namely, username and password, are not encrypted in the SealedSecret; only their values. You may change the names of these keys, if necessary, in the SealedSecret YAML file and still be able to deploy it successfully to the cluster. However, you cannot change the name and namespace of the SealedSecret. The SealedSecret and Secret must have the same namespace and name.\nNow, deploy the SealedSecret to your cluster:\nkubectl apply -f sealed-secret.yaml Looking at the logs of the contoller, you can see that it picks up the SealedSecret custom resource that was just deployed, unseals it to create a regular Secret.\nkubectl logs -n kube-system sealed-secrets-controller-7bdbc75d47-5wxvf Output: (...) 2021/07/14 21:27:01 HTTP server serving on :8080 2021/07/15 13:22:20 Updating octank/database-credentials 2021/07/15 13:22:20 Event(v1.ObjectReference{Kind:\u0026#34;SealedSecret\u0026#34;, Namespace:\u0026#34;octank\u0026#34;, Name:\u0026#34;database-credentials\u0026#34;, UID:\u0026#34;abc9b6ab-fe69-453a-8654-c9593de935c7\u0026#34;, APIVersion:\u0026#34;bitnami.com/v1alpha1\u0026#34;, ResourceVersion:\u0026#34;104915\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;Unsealed\u0026#39; SealedSecret unsealed successfully  Verfiy that the database-credentials Secret unsealed from the SealedSecret was deployed by the controller to the octank namespace.\nkubectl get secret -n octank database-credentials Output: NAME TYPE DATA AGE database-credentials Opaque 2 90s  Redeploy the pod that reads from the above Secret and verify that the keys have been exposed as environment variables with the correct literal values.\nkubectl apply -f pod-variable.yaml kubectl wait -n octank pod/pod-variable --for=condition=Ready kubectl logs -n octank pod-variable Output: pod/pod-variable created pod/pod-variable condition met DATABASE_USER = admin DATABASE_PASSWROD = Tru5tN0!  The YAML file, sealed-secret.yaml, that pertains to the SealedSecret is safe to be stored in a Git repository along with YAML manifests pertaining to other Kubernetes resources such as DaemonSets, Deployments, ConfigMaps etc. deployed in the cluster. You can then use a GitOps workflow to manage the deployment of these resources to your cluster. The YAML file, secret.yaml, that pertains to the Secret may be deleted because it is never used in any subsequent workflows.\n"
},
{
	"uri": "/intermediate/270_custom_resource_definition/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Cleaning up To delete the Custom Resource Definitions:\nkubectl delete -f resourcedefinition.yaml kubectl get crontabs "
},
{
	"uri": "/beginner/190_efs/deploying-services/",
	"title": "Deploying the Stateful Services",
	"tags": [],
	"description": "",
	"content": "Next, launch a set of two pods with the following commands.\ncd ~/environment/efs wget https://eksworkshop.com/beginner/190_efs/efs.files/efs-writer.yaml wget https://eksworkshop.com/beginner/190_efs/efs.files/efs-reader.yaml kubectl apply -f efs-writer.yaml kubectl apply -f efs-reader.yaml Each one of these pods references the PVC resource named efs-storage-claim created earlier and mounts the backing PV to a local directory named /shared.\nVerify that the efs-writer pod is successfully writing data to the shared persistent volume.\nkubectl exec -it efs-writer -n storage -- tail /shared/out.txt The output from the above command will look as follows: efs-writer.storage - Thu Mar 5 20:52:19 UTC 2020 efs-writer.storage - Thu Mar 5 20:52:24 UTC 2020 efs-writer.storage - Thu Mar 5 20:52:29 UTC 2020 efs-writer.storage - Thu Mar 5 20:52:34 UTC 2020  Verify that the efs-reader pod is able to successfully read the same data from the shared persistent volume.\nkubectl exec -it efs-reader -n storage -- tail /shared/out.txt The output from the above command will be the same as the one from the efs-writer pod. efs-writer.storage - Thu Mar 5 20:52:19 UTC 2020 efs-writer.storage - Thu Mar 5 20:52:24 UTC 2020 efs-writer.storage - Thu Mar 5 20:52:29 UTC 2020 efs-writer.storage - Thu Mar 5 20:52:34 UTC 2020 efs-writer.storage - Thu Mar 5 20:52:39 UTC 2020 efs-writer.storage - Thu Mar 5 20:52:44 UTC 2020  "
},
{
	"uri": "/beginner/190_ocean/headroom/",
	"title": "Headroom - A Buffer For Faster Scale Out",
	"tags": [],
	"description": "",
	"content": "Keeping Scale-Up Proactive One of the key features of the Ocean Autoscaler, is the ability to maintain a dynamic buffer of spare capacity called Headroom. The buffer is algorithmically tailored to meet the actual requirements of the incoming containerized workloads which enables immediate pod scheduling. This is perfect for workloads that make use of Horizontal Pod Autoscaler, and helps prevent pending pods which scale scale-out of worker nodes a proactive process, instead of the reactive nature of Kubernetes Cluster-Autoscaler.\nHeadroom is configurable via the Ocean Cluster\u0026rsquo;s \u0026ldquo;Actions\u0026rdquo; menu, under \u0026ldquo;Customize Scaling\u0026rdquo;: The Headroom can be configured Automatically, or Manually. The automatic option would derive the size and number of headroom units from the most common pods in the cluster. The total percentage of the cluster capacity (in terms of CPU and Memory) that will be reserved for Headroom, is configurable. The default value, recommended for most use cases is 5%.\nThe manual option allows you to tailor the headroom units to your own specification, but note that the defined headroom capacity will end up being static (as opposed to the automatic configuration, which scales with the cluster).\n"
},
{
	"uri": "/beginner/130_exposing-service/ingress_controller_alb/",
	"title": "Ingress Controller",
	"tags": [],
	"description": "",
	"content": "Ingress Controllers In order for the Ingress resource to work, the cluster must have an ingress controller running.\nUnlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllers are not started automatically with a cluster.\nAWS Load Balancer Controller The AWS ALB Ingress Controller has been rebranded to AWS Load Balancer Controller.\n AWS Load Balancer Controller is a controller to help manage Elastic Load Balancers for a Kubernetes cluster.\n It satisfies Kubernetes Ingress resources by provisioning Application Load Balancers. It satisfies Kubernetes Service resources by provisioning Network Load Balancers.  In this chapter we will focus on the Application Load Balancer.\nAWS Elastic Load Balancing Application Load Balancer (ALB) is a popular AWS service that load balances incoming traffic at the application layer (layer 7) across multiple targets, such as Amazon EC2 instances, in multiple Availability Zones.\nALB supports multiple features including:\n host or path based routing TLS (Transport Layer Security) termination, WebSockets HTTP/2 AWS WAF (Web Application Firewall) integration integrated access logs, and health checks  Deploy the AWS Load Balancer Controller Prerequisites We will verify if the AWS Load Balancer Controller version has been set\nif [ ! -x ${LBC_VERSION} ] then tput setaf 2; echo \u0026#39;${LBC_VERSION} has been set.\u0026#39; else tput setaf 1;echo \u0026#39;${LBC_VERSION} has NOT been set.\u0026#39; fi  If the result is ${LBC_VERSION} has NOT been set., click here for the instructions.\n We will use Helm to install the ALB Ingress Controller.\nCheck to see if helm is installed:\nhelm version --short  If Helm is not found, click installing Helm CLI for instructions.\n Create IAM OIDC provider eksctl utils associate-iam-oidc-provider \\  --region ${AWS_REGION} \\  --cluster eksworkshop-eksctl \\  --approve  Learn more about IAM Roles for Service Accounts in the Amazon EKS documentation.\n Create an IAM policy called Create a policy called AWSLoadBalancerControllerIAMPolicy\naws iam create-policy \\  --policy-name AWSLoadBalancerControllerIAMPolicy \\  --policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json Create a IAM role and ServiceAccount eksctl create iamserviceaccount \\  --cluster eksworkshop-eksctl \\  --namespace kube-system \\  --name aws-load-balancer-controller \\  --attach-policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy \\  --override-existing-serviceaccounts \\  --approve Install the TargetGroupBinding CRDs kubectl apply -k github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master kubectl get crd Deploy the Helm chart The helm chart will deploy from the eks repo\nhelm repo add eks https://aws.github.io/eks-charts helm upgrade -i aws-load-balancer-controller \\  eks/aws-load-balancer-controller \\  -n kube-system \\  --set clusterName=eksworkshop-eksctl \\  --set serviceAccount.create=false \\  --set serviceAccount.name=aws-load-balancer-controller \\  --set image.tag=\u0026#34;${LBC_VERSION}\u0026#34; kubectl -n kube-system rollout status deployment aws-load-balancer-controller Deploy Sample Application Now let‚Äôs deploy a sample 2048 game into our Kubernetes cluster and use the Ingress resource to expose it to traffic:\nDeploy 2048 game resources:\nexport EKS_CLUSTER_VERSION=$(aws eks describe-cluster --name eksworkshop-eksctl --query cluster.version --output text) if [ \u0026#34;`echo \u0026#34;${EKS_CLUSTER_VERSION} \u0026lt; 1.19\u0026#34; | bc`\u0026#34; -eq 1 ]; then curl -s https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/2048/2048_full.yaml \\  | sed \u0026#39;s=alb.ingress.kubernetes.io/target-type: ip=alb.ingress.kubernetes.io/target-type: instance=g\u0026#39; \\  | kubectl apply -f - fi if [ \u0026#34;`echo \u0026#34;${EKS_CLUSTER_VERSION} \u0026gt;= 1.19\u0026#34; | bc`\u0026#34; -eq 1 ]; then curl -s https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/2048/2048_full_latest.yaml \\  | sed \u0026#39;s=alb.ingress.kubernetes.io/target-type: ip=alb.ingress.kubernetes.io/target-type: instance=g\u0026#39; \\  | kubectl apply -f - fi After few seconds, verify that the Ingress resource is enabled:\nkubectl get ingress/ingress-2048 -n game-2048 You should be able to see the following output\nNAME HOSTS ADDRESS PORTS AGE ingress-2048 * k8s-game2048-ingress2-8ae3738fd5-251279030.us-east-2.elb.amazonaws.com 80 6m20s  You can find more information on the ingress with this command:\nexport GAME_INGRESS_NAME=$(kubectl -n game-2048 get targetgroupbindings -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n game-2048 get targetgroupbindings ${GAME_INGRESS_NAME} -o yaml output\napiVersion: elbv2.k8s.aws/v1beta1 kind: TargetGroupBinding metadata: creationTimestamp: \u0026#34;2020-10-24T20:16:37Z\u0026#34; finalizers: - elbv2.k8s.aws/resources generation: 1 labels: ingress.k8s.aws/stack-name: ingress-2048 ingress.k8s.aws/stack-namespace: game-2048 name: k8s-game2048-service2-0e5fd48cc4 namespace: game-2048 resourceVersion: \u0026#34;292608\u0026#34; selfLink: /apis/elbv2.k8s.aws/v1beta1/namespaces/game-2048/targetgroupbindings/k8s-game2048-service2-0e5fd48cc4 uid: a1e3567e-429d-4f3c-b1fc-1131775cb74b spec: networking: ingress: - from: - securityGroup: groupID: sg-0f2bf9481b203d45a ports: - protocol: TCP serviceRef: name: service-2048 port: 80 targetGroupARN: arn:aws:elasticloadbalancing:us-east-2:197520326489:targetgroup/k8s-game2048-service2-0e5fd48cc4/4e0de699a21473e2 targetType: instance status: observedGeneration: 1  Finally, you access your newly deployed 2048 game by clicking the URL generated with these commands\nIt could take 2 or 3 minutes for the ALB to be ready.\n export GAME_2048=$(kubectl get ingress/ingress-2048 -n game-2048 -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) echo http://${GAME_2048} "
},
{
	"uri": "/intermediate/245_x-ray/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the Tracing with X-Ray module.\nThe content for this module was based on the Application Tracing on Kubernetes with AWS X-Ray blog post.\nThis module is not used in subsequent steps, so you can remove the resources now or at the end of the workshop.\nDelete the Kubernetes example microservices deployed:\nkubectl delete deployments x-ray-sample-front-k8s x-ray-sample-back-k8s kubectl delete services x-ray-sample-front-k8s x-ray-sample-back-k8s Delete the X-Ray DaemonSet:\nkubectl delete -f https://eksworkshop.com/intermediate/245_x-ray/daemonset.files/xray-k8s-daemonset.yaml Delete the IAM Service Account:\neksctl delete iamserviceaccount --name xray-daemon --cluster eksworkshop-eksctl "
},
{
	"uri": "/intermediate/220_codepipeline/codepipeline/",
	"title": "CodePipeline Setup",
	"tags": [],
	"description": "",
	"content": "Now we are going to create the AWS CodePipeline using AWS CloudFormation.\nCloudFormation is an infrastructure as code (IaC) tool which provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts.\nEach EKS deployment/service should have its own CodePipeline and be located in an isolated source repository.\nYou can modify the CloudFormation templates provided with this workshop to meet your system requirements to easily onboard new services to your EKS cluster. For each new service the following steps can be repeated.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       CodePipeline \u0026amp; EKS  Launch    Download      After the console is open, enter your GitHub username, personal access token (created in previous step), check the acknowledge box and then click the \u0026ldquo;Create stack\u0026rdquo; button located at the bottom of the page.\nWait for the status to change from \u0026ldquo;CREATE_IN_PROGRESS\u0026rdquo; to CREATE_COMPLETE before moving on to the next step.\nOpen CodePipeline in the Management Console. You will see a CodePipeline that starts with eks-workshop-codepipeline. Click this link to view the details.\nIf you receive a permissions error similar to User x is not authorized to perform: codepipeline:ListPipelines\u0026hellip; upon clicking the above link, the CodePipeline console may have opened up in the wrong region. To correct this, from the Region dropdown in the console, choose the region you provisioned the workshop in. Select Oregon (us-west-2) if you provisioned the workshow per the \u0026ldquo;Start the workshop at an AWS event\u0026rdquo; instructions.\n Once you are on the detail page for the specific CodePipeline, you can see the status along with links to the change and build details.\nIf you click on the \u0026ldquo;details\u0026rdquo; link in the build/deploy stage, you can see the output from the CodeBuild process.\n To review the status of the deployment, you can run:\nkubectl describe deployment hello-k8s For the status of the service, run the following command:\nkubectl describe service hello-k8s Challenge: How can we view our exposed service?\nHINT: Which kubectl command will get you the Elastic Load Balancer (ELB) endpoint for this app?\n  Expand here to see the solution   Once the service is built and delivered, we can run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser. If the message is not updated immediately, give Kubernetes some time to deploy the change.\nkubectl get services hello-k8s -o wide   "
},
{
	"uri": "/020_prerequisites/workspace/",
	"title": "Create a Workspace",
	"tags": [],
	"description": "",
	"content": " The Cloud9 workspace should be built by an IAM user with Administrator privileges, not the root account user. Please ensure you are logged in as an IAM user, not the root account user.\n A list of supported browsers for AWS Cloud9 is found here.\n Ad blockers, javascript disablers, and tracking blockers should be disabled for the cloud9 domain, or connecting to the workspace might be impacted. Cloud9 requires third-party-cookies. You can whitelist the specific domains.\n Launch Cloud9 in your closest region:  Oregon Ireland Ohio Singapore  Create a Cloud9 Environment: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n Create a Cloud9 Environment: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n Create a Cloud9 Environment: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n Create a Cloud9 Environment: https://ap-southeast-1.console.aws.amazon.com/cloud9/home?region=ap-southeast-1\n  $(function(){$(\"#region\").tabs();});  Select Create environment Name it eksworkshop, click Next. Choose t3.small for instance type, take all default values and click Create environment  When it comes up, customize the environment by:\n Closing the Welcome tab  Opening a new terminal tab in the main work area  Closing the lower work area  Your workspace should now look like this   If you intend to run all the sections in this workshop, it will be useful to have more storage available for all the repositories and tests.\n Increase the disk size on the Cloud9 instance The following command adds more disk space to the root volume of the EC2 instance that Cloud9 runs on. Once the command completes, we reboot the instance and it could take a minute or two for the IDE to come back online.\n pip3 install --user --upgrade boto3 export instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id) python -c \u0026#34;import boto3 import os from botocore.exceptions import ClientError ec2 = boto3.client(\u0026#39;ec2\u0026#39;) volume_info = ec2.describe_volumes( Filters=[ { \u0026#39;Name\u0026#39;: \u0026#39;attachment.instance-id\u0026#39;, \u0026#39;Values\u0026#39;: [ os.getenv(\u0026#39;instance_id\u0026#39;) ] } ] ) volume_id = volume_info[\u0026#39;Volumes\u0026#39;][0][\u0026#39;VolumeId\u0026#39;] try: resize = ec2.modify_volume( VolumeId=volume_id, Size=30 ) print(resize) except ClientError as e: if e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] == \u0026#39;InvalidParameterValue\u0026#39;: print(\u0026#39;ERROR MESSAGE: {}\u0026#39;.format(e))\u0026#34; if [ $? -eq 0 ]; then sudo reboot fi "
},
{
	"uri": "/beginner/200_secrets/managing-sealing-key/",
	"title": "Managing the Sealing Key",
	"tags": [],
	"description": "",
	"content": "Without the private key that is managed by the controller, there is no way to decrypt the encrypted data within a SealedSecret. In the event that you are trying to restore the original state of a cluster after a disaster or you want to leverage GitOps workflow to deploy the Kubernetes resources, including SealedSecrets, from a Git repository and stand up a separate instance of an EKS cluster, the controller deployed in the new cluster must use the same private key to be able to unseal the SealedSecrets.\nRun the following command in order to retrieve the private key from the cluster. In a production environment, you will typically make use of Kubernetes RBAC to grant the permissions required to perform this operation to restricted set of clients.\nkubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml \u0026gt; master.yaml To test how this works, let\u0026rsquo;s first delete the installation of the controller, the Secret that it created which contains the private key, the SealedSecret resource named database-credentials as well as the Secret that was unsealed from it.\nkubectl delete secret database-credentials -n octank kubectl delete sealedsecret database-credentials -n octank kubectl delete secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key kubectl delete -f controller.yaml Now, put the Secret containing the private key back into the cluster using the master.yaml file.\nkubectl apply -f master.yaml kubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key Output: NAME TYPE DATA AGE sealed-secrets-keydw62x kubernetes.io/tls 2 13s  Next, redeploy the SealedSecret CRD, controller and RBAC artifacts on your EKS.\nkubectl apply -f controller.yaml kubectl get pods -n kube-system | grep sealed-secrets-controller Output: sealed-secrets-controller-84fcdcd5fd-gznc2 0/1 ContainerCreating 0 2s  View the logs of the newly launched controller pod. Note that the name of the controller pod will be different in your cluster.\nkubectl logs sealed-secrets-controller-84fcdcd5fd-ds5t6 -n kube-system Output: 2021/07/15 13:55:56 Starting sealed-secrets controller version: v0.16.0 2021/07/15 13:55:56 Searching for existing private keys controller version: v0.16.0 2021/07/15 13:55:56 ----- sealed-secrets-keydw62x 2021/07/15 13:55:56 HTTP server serving on :8080  As you can see from the logs, the controller was able to find the existing Secret sealed-secrets-keyvk4pr in the kube-system namespace and therefore does not create a new key pair. Now, let\u0026rsquo;s redeploy the SealedSecret and verify that the controller is able to successfully unseal it.\nkubectl apply -f sealed-secret.yaml kubectl logs sealed-secrets-controller-84fcdcd5fd-ds5t6 -n kube-system Output: (...) 2021/07/15 13:55:56 HTTP server serving on :8080 2021/07/15 13:57:42 Updating octank/database-credentials 2021/07/15 13:57:42 Event(v1.ObjectReference{Kind:\u0026#34;SealedSecret\u0026#34;, Namespace:\u0026#34;octank\u0026#34;, Name:\u0026#34;database-credentials\u0026#34;, UID:\u0026#34;7b1a3942-df08-4df0-9957-1cfa133fc9b5\u0026#34;, APIVersion:\u0026#34;bitnami.com/v1alpha1\u0026#34;, ResourceVersion:\u0026#34;108846\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;Unsealed\u0026#39; SealedSecret unsealed successfully  If the file master.yaml which contains the public/private key pair generated by the controller is compromised, then all the SealedSecret manifests can be unsealed and the encrypted sensitive information they store revealed. Hence, this file must be guarded by granting least privilege access. For additional guidance on sealing key renewal, manual sealing key management etc., please consult the documentation.\nOne option to secure the private key is to store the master.yaml file contents as a SecureString parameter in AWS Systems Manager Parameter Store. The parameter could be secured using a KMS Customer managed key (CMK) and you can use the Key policy to restrict the set of IAM principals who can use this key in order to retrieve the parameter. Additionally, you may also enable automatic rotation of this CMK in KMS. Note that Standard tier parameters support a maximum parameter value of 4096 characters. Hence, given the size of the master.yaml file, you will have to store it as a parameter in the Advanced tier.\n "
},
{
	"uri": "/beginner/130_exposing-service/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Cleaning up To delete the resources used in this chapter:\nkubectl delete -f ~/environment/run-my-nginx.yaml kubectl delete ns my-nginx rm ~/environment/run-my-nginx.yaml export EKS_CLUSTER_VERSION=$(aws eks describe-cluster --name eksworkshop-eksctl --query cluster.version --output text) if [ \u0026#34;`echo \u0026#34;${EKS_CLUSTER_VERSION} \u0026lt; 1.19\u0026#34; | bc`\u0026#34; -eq 1 ]; then curl -s https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/2048/2048_full.yaml \\  | sed \u0026#39;s=alb.ingress.kubernetes.io/target-type: ip=alb.ingress.kubernetes.io/target-type: instance=g\u0026#39; \\  | kubectl delete -f - fi if [ \u0026#34;`echo \u0026#34;${EKS_CLUSTER_VERSION} \u0026gt;= 1.19\u0026#34; | bc`\u0026#34; -eq 1 ]; then curl -s https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/2048/2048_full_latest.yaml \\  | sed \u0026#39;s=alb.ingress.kubernetes.io/target-type: ip=alb.ingress.kubernetes.io/target-type: instance=g\u0026#39; \\  | kubectl delete -f - fi unset EKS_CLUSTER_VERSION helm uninstall aws-load-balancer-controller \\  -n kube-system kubectl delete -k github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master eksctl delete iamserviceaccount \\  --cluster eksworkshop-eksctl \\  --name aws-load-balancer-controller \\  --namespace kube-system \\  --wait aws iam delete-policy \\  --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy "
},
{
	"uri": "/beginner/190_efs/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Cleaning up Delete the Kubernetes resources deployed to the EKS cluster.\ncd ~/environment/efs kubectl delete -f efs-reader.yaml kubectl delete -f efs-writer.yaml kubectl delete -f efs-pvc.yaml Delete the efs-csi-node daemonset from the kube-system namespace\nkubectl delete ds efs-csi-node -n kube-system Delete the mount targets associated with the EFS file system\nFILE_SYSTEM_ID=$(aws efs describe-file-systems | jq --raw-output '.FileSystems[].FileSystemId') targets=$(aws efs describe-mount-targets --file-system-id $FILE_SYSTEM_ID | jq --raw-output '.MountTargets[].MountTargetId') for target in ${targets[@]} do echo \u0026quot;deleting mount target \u0026quot; $target aws efs delete-mount-target --mount-target-id $target done Check the status of EFS file system to find out if the mount targets have all been deleted.\naws efs describe-file-systems --file-system-id $FILE_SYSTEM_ID When the NumberOfMountTargets field in the JSON output reads 0, run the following command to delete the EFS file system.\naws efs delete-file-system --file-system-id $FILE_SYSTEM_ID Delete the security group that is associated with the EFS file system\naws ec2 delete-security-group --group-id $MOUNT_TARGET_GROUP_ID "
},
{
	"uri": "/beginner/190_fsx_lustre/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Cleaning up Once we are done, let\u0026rsquo;s cleanup the resources specific to this module:\nkubectl delete -f \u0026quot;https://raw.githubusercontent.com/kubernetes-sigs/aws-fsx-csi-driver/master/examples/kubernetes/dynamic_provisioning_s3/specs/pod.yaml\u0026quot; kubectl delete -f claim.yaml kubectl delete -f storageclass.yaml kubectl delete -k \u0026quot;github.com/kubernetes-sigs/aws-fsx-csi-driver/deploy/kubernetes/overlays/stable/?ref=master\u0026quot; aws s3 rm --recursive s3://$S3_LOGS_BUCKET aws s3 rb s3://$S3_LOGS_BUCKET eksctl delete iamserviceaccount \\ --region $AWS_REGION \\ --name fsx-csi-controller-sa \\ --namespace kube-system \\ --cluster $CLUSTER_NAME aws iam delete-policy \\ --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/Amazon_FSx_Lustre_CSI_Driver "
},
{
	"uri": "/beginner/180_fargate/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "",
	"content": "Ingress After few seconds, verify that the Ingress resource is enabled:\nkubectl get ingress/ingress-2048 -n game-2048 You should be able to see the following output\nNAME HOSTS ADDRESS PORTS AGE ingress-2048 * k8s-game2048-ingress2-8ae3738fd5-1566954439.us-east-2.elb.amazonaws.com 80 14m  It could take 2 or 3 minutes for the ALB to be ready.\n From your AWS Management Console, if you navigate to the EC2 dashboard and the select Load Balancers from the menu on the left-pane, you should see the details of the ALB instance similar to the following. From the left-pane, if you select Target Groups and look at the registered targets under the Targets tab, you will see the IP addresses and ports of the sample app pods listed. Notice that the pods have been directly registered with the load balancer whereas when we worked with worker nodes in an earlier lab, the IP address of the worker nodes and the NodePort were registered as targets. The latter case is the Instance Mode where Ingress traffic starts at the ALB and reaches the Kubernetes worker nodes through each service\u0026rsquo;s NodePort and subsequently reaches the pods through the service‚Äôs ClusterIP. While running under Fargate, ALB operates in IP Mode, where Ingress traffic starts at the ALB and reaches the Kubernetes pods directly.\nIllustration of request routing from an AWS Application Load Balancer to Pods on worker nodes in Instance mode: Illustration of request routing from an AWS Application Load Balancer to Fargate Pods in IP mode: At this point, your deployment is complete and you should be able to reach the game-2048 service from a browser using the DNS name of the ALB. You may get the DNS name of the load balancer either from the AWS Management Console or from the output of the following command.\nexport FARGATE_GAME_2048=$(kubectl get ingress/ingress-2048 -n game-2048 -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) echo \u0026#34;http://${FARGATE_GAME_2048}\u0026#34; Output should look like this http://3e100955-2048game-2048ingr-6fa0-1056911976.us-east-2.elb.amazonaws.com  "
},
{
	"uri": "/beginner/190_ocean/showback/",
	"title": "Showback - Cost Allocation",
	"tags": [],
	"description": "",
	"content": "Kubernetes Workload Cost Showback Ocean provides you with advanced Cost Analysis for your EKS cluster, with detailed showback which delivers visibility down to the application level.\nFound under Cost Analysis tab of the Ocean Console, the data can be aggregated in several ways:\n Namespace: With namespaces being used to create logical groupings within a cluster, this is a very straightforward way to identify which environment, team or other grouping is responsible for the associated cost. This cost analysis option will show you both the aggregated spend per namespace as well as a breakdown of spend by deployments and other K8s objects within that namespace. Namespace/Resource Label: This option shows a spend breakdown of namespaces or K8s resrources that have been given a specific label key. Namespace/Resource Annotaion: Similar to Label filtering, Ocean can aggregate cost analysis for namespaces and resources that have been annotated with a particular key.  In addition to the aggregation options described above, if you wish to only look at a subset of your cluster, either based on labels or annotations, you can create a custom filter via the \u0026ldquo;Add Filter\u0026rdquo; button. For example, if you wish to view only the costs incurred by your K8s application manager, Helm, select the relevant type (label/annotation), the key, an operator (e.g. equals, exists, etc.) and the desired key value.\n"
},
{
	"uri": "/intermediate/240_monitoring/deploy-grafana/",
	"title": "Deploy Grafana",
	"tags": [],
	"description": "",
	"content": "We are now going to install Grafana. For this example, we are primarily using the Grafana defaults, but we are overriding several parameters. As with Prometheus, we are setting the storage class to gp2, admin password, configuring the datasource to point to Prometheus and creating an external load balancer for the service.\nCreate YAML file called grafana.yaml with following commands:\nmkdir ${HOME}/environment/grafana cat \u0026lt;\u0026lt; EoF \u0026gt; ${HOME}/environment/grafana/grafana.yaml datasources: datasources.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://prometheus-server.prometheus.svc.cluster.local access: proxy isDefault: true EoF kubectl create namespace grafana helm install grafana grafana/grafana \\  --namespace grafana \\  --set persistence.storageClassName=\u0026#34;gp2\u0026#34; \\  --set persistence.enabled=true \\  --set adminPassword=\u0026#39;EKS!sAWSome\u0026#39; \\  --values ${HOME}/environment/grafana/grafana.yaml \\  --set service.type=LoadBalancer Run the following command to check if Grafana is deployed properly:\nkubectl get all -n grafana You should see similar results. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/grafana-f64dbbcf4-794rk 1/1 Running 0 55s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana LoadBalancer 10.100.60.167 aa0fa7322d86e408786cdd21ebcc461c-1708627185.us-east-2.elb.amazonaws.com 80:31929/TCP 55s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/grafana 1/1 1 1 55s NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-f64dbbcf4 1 1 1 55s  It can take several minutes before the ELB is up, DNS is propagated and the nodes are registered.\n You can get Grafana ELB URL using this command. Copy \u0026amp; Paste the value into browser to access Grafana web UI.\nexport ELB=$(kubectl get svc -n grafana grafana -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) echo \u0026#34;http://$ELB\u0026#34; When logging in, use the username admin and get the password hash by running the following:\nkubectl get secret --namespace grafana grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo "
},
{
	"uri": "/beginner/170_statefulset/services/",
	"title": "Create Services",
	"tags": [],
	"description": "",
	"content": "Introduction Kubernetes Service defines a logical set of Pods and a policy by which to access them.\nService can be exposed in different ways by specifying a type in the serviceSpec. StatefulSet currently requires a Headless Service to control the domain of its Pods, directly reach each Pod with stable DNS entries.\nBy specifying \u0026ldquo;None\u0026rdquo; for the clusterIP, you can create Headless Service.\nCreate Services Copy/Paste the following commands into your Cloud9 Terminal.\ncat \u0026lt;\u0026lt; EoF \u0026gt; ${HOME}/environment/ebs_statefulset/mysql-services.yaml # Headless service for stable DNS entries of StatefulSet members. apiVersion: v1 kind: Service metadata: namespace: mysql name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- # Client service for connecting to any MySQL instance for reads. # For writes, you must instead connect to the leader: mysql-0.mysql. apiVersion: v1 kind: Service metadata: namespace: mysql name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql EoF You can see the mysql service is for DNS resolution so that when pods are placed by StatefulSet controller, pods can be resolved using pod-name.mysql. mysql-read is a client service that does load balancing for all followers.\nCreate service mysql and mysql-read by following command\nkubectl create -f ${HOME}/environment/ebs_statefulset/mysql-services.yaml "
},
{
	"uri": "/020_prerequisites/k8stools/",
	"title": "Install Kubernetes Tools",
	"tags": [],
	"description": "",
	"content": "Amazon EKS clusters require kubectl and kubelet binaries and the aws-cli or aws-iam-authenticator binary to allow IAM authentication for your Kubernetes cluster.\nIn this workshop we will give you the commands to download the Linux binaries. If you are running Mac OSX / Windows, please see the official EKS docs for the download links.\n Install kubectl sudo curl --silent --location -o /usr/local/bin/kubectl \\  https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl sudo chmod +x /usr/local/bin/kubectl Update awscli Upgrade AWS CLI according to guidance in AWS documentation.\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Install jq, envsubst (from GNU gettext utilities) and bash-completion sudo yum -y install jq gettext bash-completion moreutils Install yq for yaml processing echo \u0026#39;yq() { docker run --rm -i -v \u0026#34;${PWD}\u0026#34;:/workdir mikefarah/yq \u0026#34;$@\u0026#34; }\u0026#39; | tee -a ~/.bashrc \u0026amp;\u0026amp; source ~/.bashrc Verify the binaries are in the path and executable for command in kubectl jq envsubst aws do which $command \u0026amp;\u0026gt;/dev/null \u0026amp;\u0026amp; echo \u0026#34;$commandin path\u0026#34; || echo \u0026#34;$commandNOT FOUND\u0026#34; done Enable kubectl bash_completion kubectl completion bash \u0026gt;\u0026gt; ~/.bash_completion . /etc/profile.d/bash_completion.sh . ~/.bash_completion set the AWS Load Balancer Controller version echo \u0026#39;export LBC_VERSION=\u0026#34;v2.2.0\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile . ~/.bash_profile "
},
{
	"uri": "/intermediate/260_weave_flux/installweaveflux/",
	"title": "Install Weave Flux",
	"tags": [],
	"description": "",
	"content": "Now we will use Helm to install Weave Flux into our cluster and enable it to interact with our Kubernetes configuration GitHub repo.\nFirst, install the Flux Custom Resource Definition:\nkubectl apply -f https://raw.githubusercontent.com/fluxcd/helm-operator/master/deploy/crds.yaml Check that Helm is installed.\nhelm list This command should either return a list of helm charts that have already been deployed or nothing.\nIf you get an error message, see installing helm for instructions.\n  In the following steps, your Git user name will be required. Without this information, the resulting pipeline will not function as expected. Set this as an environment variable to reuse in the next commands:\n YOURUSER=yourgitusername First, create the flux Kubernetes namespace\nkubectl create namespace flux Next, add the Flux chart repository to Helm and install Flux.\nUpdate the Git URL below to match your user name and Kubernetes configuration manifest repository.\n helm repo add fluxcd https://charts.fluxcd.io helm upgrade -i flux fluxcd/flux \\ --set git.url=git@github.com:${YOURUSER}/k8s-config \\ --set git.branch=main \\ --namespace flux helm upgrade -i helm-operator fluxcd/helm-operator \\ --set helm.versions=v3 \\ --set git.ssh.secretName=flux-git-deploy \\ --set git.branch=main \\ --namespace flux Watch the install and confirm everything starts. There should be 3 pods.\nkubectl get pods -n flux Install fluxctl in order to get the SSH key to allow GitHub write access. This allows Flux to keep the configuration in GitHub in sync with the configuration deployed in the cluster.\nsudo wget -O /usr/local/bin/fluxctl $(curl https://api.github.com/repos/fluxcd/flux/releases/latest | jq -r \u0026quot;.assets[] | select(.name | test(\\\u0026quot;linux_amd64\\\u0026quot;)) | .browser_download_url\u0026quot;) sudo chmod 755 /usr/local/bin/fluxctl fluxctl version fluxctl identity --k8s-fwd-ns flux Copy the provided key and add that as a deploy key in the GitHub repository.\n In GitHub, select your k8s-config GitHub repo. Go to Settings and click Deploy Keys. Alternatively, you can go by direct URL by replacing your user name in this URL: github.com/YOURUSER/k8s-config/settings/keys. Click on Add Deploy Key Name: Flux Deploy Key Paste the key output from fluxctl Click Allow Write Access. This allows Flux to keep the repo in sync with the real state of the cluster Click Add Key  Now Flux is configured and should be ready to pull configuration.\n"
},
{
	"uri": "/intermediate/300_cis_eks_benchmark/ssh-into-node/",
	"title": "Module 1: Install kube-bench in node",
	"tags": [],
	"description": "",
	"content": "In this module, we will install kube-bench in one of the nodes and run the CIS Amazon EKS Benchmark node assessment against eks-1.0 node controls.\nList Amazon EKS cluster nodes kubectl get nodes -o wide Output NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ip-192-168-17-56.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 24h v1.16.12-eks-904af05 192.168.17.56 34.220.140.125 Amazon Linux 2 4.14.181-142.260.amzn2.x86_64 docker://19.3.6 ip-192-168-45-110.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 24h v1.16.12-eks-904af05 192.168.45.110 34.220.227.8 Amazon Linux 2 4.14.181-142.260.amzn2.x86_64 docker://19.3.6 ip-192-168-84-9.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 24h v1.16.12-eks-904af05 192.168.84.9 34.210.27.208 Amazon Linux 2 4.14.181-142.260.amzn2.x86_64 docker://19.3.6 SSH into nodes  Ssh (using SSM) via the AWS Console by clicking \u0026lsquo;Connect\u0026rsquo;-\u0026gt;\u0026lsquo;Session Manager`  Install kube-bench Install kube-bench using the commands below.\n Set latest version  KUBEBENCH_URL=$(curl -s https://api.github.com/repos/aquasecurity/kube-bench/releases/latest | jq -r '.assets[] | select(.name | contains(\u0026quot;amd64.rpm\u0026quot;)) | .browser_download_url')  Download and install kube-bench using yum  sudo yum install -y $KUBEBENCH_URL Run assessment against eks-1.0 Run the assessment against eks-1.0 controls based on CIS Amazon EKS Benchmark node assessments.\nkube-bench --benchmark eks-1.0 Output [INFO] 3 Worker Node Security Configuration [INFO] 3.1 Worker Node Configuration Files [PASS] 3.1.1 Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Scored) [PASS] 3.1.2 Ensure that the proxy kubeconfig file ownership is set to root:root (Scored) [PASS] 3.1.3 Ensure that the kubelet configuration file has permissions set to 644 or more restrictive (Scored) [PASS] 3.1.4 Ensure that the kubelet configuration file ownership is set to root:root (Scored) [INFO] 3.2 Kubelet [PASS] 3.2.1 Ensure that the --anonymous-auth argument is set to false (Scored) [PASS] 3.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored) [PASS] 3.2.3 Ensure that the --client-ca-file argument is set as appropriate (Scored) [PASS] 3.2.4 Ensure that the --read-only-port argument is set to 0 (Scored) [PASS] 3.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Scored) [PASS] 3.2.6 Ensure that the --protect-kernel-defaults argument is set to true (Scored) [PASS] 3.2.7 Ensure that the --make-iptables-util-chains argument is set to true (Scored) [PASS] 3.2.8 Ensure that the --hostname-override argument is not set (Scored) [WARN] 3.2.9 Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture (Scored) [PASS] 3.2.10 Ensure that the --rotate-certificates argument is not set to false (Scored) [PASS] 3.2.11 Ensure that the RotateKubeletServerCertificate argument is set to true (Scored) == Remediations == 3.2.9 If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service == Summary == 14 checks PASS 0 checks FAIL 1 checks WARN 0 checks INFO Clean-up  Uninstall kube-bench  sudo yum remove kube-bench -y  Exit out of the node  exit "
},
{
	"uri": "/intermediate/310_opa_gatekeeper/setup/",
	"title": "OPA Gatekeeper setup in EKS",
	"tags": [],
	"description": "",
	"content": "In this section, we will setup OPA Gatekeeper within the cluster.\n1. Deploy OPA Gatekeeper using Prebuilt docker images kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.1/deploy/gatekeeper.yaml 2. Check the pods in gatekeeper-system namespace kubectl get pods -n gatekeeper-system The output will be similar to:\nNAME READY STATUS RESTARTS AGE gatekeeper-audit-5bc9b59c57-9d9hc 1/1 Running 0 25s gatekeeper-controller-manager-744cdc8556-hxf2n 1/1 Running 0 25s gatekeeper-controller-manager-744cdc8556-jn42m 1/1 Running 0 25s gatekeeper-controller-manager-744cdc8556-wwrb6 1/1 Running 0 25s 3. Observe OPA Gatekeeper Component logs once operational You can follow the OPA logs to see the webhook requests being issued by the Kubernetes API server:\nkubectl logs -l control-plane=audit-controller -n gatekeeper-system kubectl logs -l control-plane=controller-manager -n gatekeeper-system This completes the OPA Gatekeeper setup on Amazon EKS cluster. To order to define and enforce the policy, OPA Gatekeeper uses a framework OPA Constraint Framework\n"
},
{
	"uri": "/intermediate/220_codepipeline/change/",
	"title": "Trigger New Release",
	"tags": [],
	"description": "",
	"content": "Update Our Application So far we have walked through setting up CI/CD for EKS using AWS CodePipeline and now we are going to make a change to the GitHub repository so that we can see a new release built and delivered.\nOpen GitHub and select the forked repository with the name eks-workshop-sample-api-service-go.\nClick on main.go file and then click on the edit button, which looks like a pencil.\nChange the text where it says \u0026ldquo;Hello World\u0026rdquo;, add a commit message and then click the \u0026ldquo;Commit changes\u0026rdquo; button.\nYou should leave the master branch selected.\nThe main.go application needs to be compiled, so please ensure that you don\u0026rsquo;t accidentally break the build :)\n After you modify and commit your change in GitHub, in approximately one minute you will see a new build triggered in the AWS Management Console Confirm the Change If you still have the ELB URL open in your browser, refresh to confirm the update. If you need to retrieve the URL again, use kubectl get services hello-k8s -o wide\n"
},
{
	"uri": "/beginner/200_secrets/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Delete all the resources created in this module.\ncd ~/environment/secrets kubectl delete Secret --all -n octank kubectl delete SealedSecret --all -n octank kubectl delete pod --all -n octank kubectl delete -f controller.yaml kubectl delete namespace octank "
},
{
	"uri": "/beginner/180_fargate/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Cleaning up To delete the resources used in this chapter:\nkubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/2048/2048_full.yaml helm uninstall aws-load-balancer-controller \\  -n kube-system eksctl delete iamserviceaccount \\  --cluster eksworkshop-eksctl \\  --name aws-load-balancer-controller \\  --namespace kube-system \\  --wait aws iam delete-policy \\  --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy kubectl delete -k github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master eksctl delete fargateprofile \\  --name game-2048 \\  --cluster eksworkshop-eksctl "
},
{
	"uri": "/intermediate/201_resource_management/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Ensure all the resources created in this module are cleaned up.\n# Basic Pod CPU and Memory Management kubectl delete pod basic-request-pod kubectl delete pod basic-limit-memory-pod kubectl delete pod basic-limit-cpu-pod kubectl delete pod basic-restricted-pod # Advanced Pod CPU and Memory Management kubectl delete namespace low-usage kubectl delete namespace high-usage kubectl delete namespace unrestricted-usage # Resource Quotas kubectl delete namespace red kubectl delete namespace blue # Pod Priority and Preemption kubectl delete deployment high-nginx-deployment kubectl delete deployment nginx-deployment kubectl delete priorityclass high-priority kubectl delete priorityclass low-priority # Prerequisites rm -r ~/environment/resource-management/ helm uninstall metrics-server --namespace metrics kubectl delete namespace metrics-server "
},
{
	"uri": "/beginner/190_ocean/rightsizing/",
	"title": "Rightsizing Applications",
	"tags": [],
	"description": "",
	"content": "Right Size Pod Requirements For Optimal Resource Allocation One of the challenging tasks of managing containerized clusters is estimating your Pods‚Äô resource requirements in terms of vCPU and memory. Even if development teams manage to achieve an accurate estimate of their application‚Äôs resource consumption, chances are that these measurements will vary in a production environment.\nOcean‚Äôs Right Sizing feature compares the CPU \u0026amp; Memory requests of your Pods to their actual consumption in production. After analyzing the difference, Ocean provides resizing recommendations to improve the resource configuration of the Deployments.\nApplying accurate resource requests and limits to Deployments can help prevent over-provisioning of extra resources which leads to underutilization and higher cluster costs, or under-provisioning of fewer resources than required, which may lead to various errors such as OOM events.\nThe Right Sizing feature requires having the Metric Server installed in your EKS cluster.\nOnce the Metric Server is installed, Ocean requires up to 4 days metric collection, in order to dispaly suggestions.\n Once active, top reccomendations can be found aggregated under the Cluster\u0026rsquo;s \u0026ldquo;Right Sizing\u0026rdquo; tab, as seen in the screenshot above, beneath the cluster-wide CPU and Memory graphs which show the cluster\u0026rsquo;s total requested resources vs Ocean\u0026rsquo;s recommended values.\nViewing a particular Deployment will display collected metrics compared to it\u0026rsquo;s resource requests and limits, as well as any resizing recommendations.\nExample 1 Below you can see a recommendation that suggests reducing the requested values for both CPU and Memory of a certain Deployment. Considering a large number of pods, applying such a recommendation can significantly reduce overprovisioning. The recommendation can be easily applied via kubectl:\nkubectl set resources DeploymentName --requests=cpu=3575,memory=3001\nExample 2 Below you can see a recommendation that suggests increasing the requested values for CPU, but reducing Memory values. Overall, applying recommendations leads to improved cluster performance. You can also use the Right Sizing feature via API.\n"
},
{
	"uri": "/intermediate/220_codepipeline/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the CI/CD with CodePipeline module.\nThis module is not used in subsequent steps, so you can remove the resources now, or at the end of the workshop.\nFirst we need to delete the Kubernetes deployment and service:\nkubectl delete deployments hello-k8s kubectl delete services hello-k8s Next, we are going to delete the CloudFormation stack created. Open CloudFormation the AWS Management Console.\nCheck the box next to the eksws-codepipeline stack, select the Actions dropdown menu and then click Delete stack:\nNow we are going to delete the ECR repository:\nEmpty and then delete the S3 bucket used by CodeBuild for build artifacts (bucket name starts with eksws-codepipeline). First, select the bucket, then empty the bucket and finally delete the bucket:\n"
},
{
	"uri": "/020_prerequisites/iamrole/",
	"title": "Create an IAM role for your Workspace",
	"tags": [],
	"description": "",
	"content": " Follow this deep link to create an IAM role with Administrator access. Confirm that AWS service and EC2 are selected, then click Next: Permissions to view permissions. Confirm that AdministratorAccess is checked, then click Next: Tags to assign tags. Take the defaults, and click Next: Review to review. Enter eksworkshop-admin for the Name, and click Create role.   "
},
{
	"uri": "/beginner/190_ocean/cluster_roll/",
	"title": "Deploy Infrastructure Changes With Ease",
	"tags": [],
	"description": "",
	"content": "Applying Infrastructure Configuration Changes Ocean\u0026rsquo;s Cluster Roll allows you to apply changes to instance configurations (modifying AMI, User-Data, Security-Group, Private IP, etc.) and roll your cluster in a single click. This replaces the running instances in a blue-green manner, in batches of a configurable size.\nOcean\u0026rsquo;s Cluster Roll takes into consideration the actual pods currently running in the cluster and is aware of any new workload entering the cluster. It launches compute capacity to match the workload‚Äôs requirements, and freezes any auto-scaling related activity, until the roll is completed. As a result, once the Roll is completed, no further re-adjustment via scale-up or scale-down is required.\nCreating A Cluster Roll On the top right corner of your Ocean cluster\u0026rsquo;s console, select ‚ÄúActions‚Äù and then choose \u0026ldquo;Cluster Roll‚Äù. In the pop-up window, configure the batch size - a percentage of the cluster‚Äôs target capacity (the total amount of nodes in the cluster) to roll at once. This will determine the number of batches in the Roll. You can optionally add a comment, specifying why the Roll was made.\nFinally, from the \u0026ldquo;Cluster Rolls\u0026rdquo; tab of the Ocean console you can monitor the progress of the Cluster Roll you have created, stop it, or view any previous Rolls.\n"
},
{
	"uri": "/020_prerequisites/ec2instance/",
	"title": "Attach the IAM role to your Workspace",
	"tags": [],
	"description": "",
	"content": " Click the grey circle button (in top right corner) and select Manage EC2 Instance.  Select the instance, then choose Actions / Security / Modify IAM Role  Choose eksworkshop-admin from the IAM Role drop down, and select Save   "
},
{
	"uri": "/beginner/190_ocean/logs/",
	"title": "Cluster Logs and Scaling Decisions",
	"tags": [],
	"description": "",
	"content": "Cluster Logs And Autoscaling Visibility The final tab on the Ocean cluster dashboard is the log tab, which contains your cluster logs. Here you can look back at the various events occurring in the Cluster and filter by time, severity and resource ID.\nMost importantly, you can gain deep visibility into the decisions made by Ocean\u0026rsquo;s Autoscaler, and see the reason for each scaling activity by clicking on ‚ÄúView Details‚Äù. The details view will show you pre and post scale resource allocation, the reason for the scaling, and the affected resources.\n"
},
{
	"uri": "/020_prerequisites/workspaceiam/",
	"title": "Update IAM settings for your Workspace",
	"tags": [],
	"description": "",
	"content": " Cloud9 normally manages IAM credentials dynamically. This isn\u0026rsquo;t currently compatible with the EKS IAM authentication, so we will disable it and rely on the IAM role instead.\n  Return to your Cloud9 workspace and click the gear icon (in top right corner) Select AWS SETTINGS Turn off AWS managed temporary credentials Close the Preferences tab   To ensure temporary credentials aren\u0026rsquo;t already in place we will also remove any existing credentials file:\nrm -vf ${HOME}/.aws/credentials We should configure our aws cli with our current region as default.\nIf you are at an AWS event, ask your instructor which AWS region to use.\n export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) export AZS=($(aws ec2 describe-availability-zones --query \u0026#39;AvailabilityZones[].ZoneName\u0026#39; --output text --region $AWS_REGION)) Check if AWS_REGION is set to desired region\ntest -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set Let\u0026rsquo;s save these into bash_profile\necho \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AZS=(${AZS[@]})\u0026#34; | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region Validate the IAM role Use the GetCallerIdentity CLI command to validate that the Cloud9 IDE is using the correct IAM role.\naws sts get-caller-identity --query Arn | grep eksworkshop-admin -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; If the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\n"
},
{
	"uri": "/beginner/110_irsa/oidc-provider/",
	"title": "Create an OIDC identity provider",
	"tags": [],
	"description": "",
	"content": "To use IAM roles for service accounts in your cluster, you must create an IAM OIDC Identity Provider. This can be done using the AWS Console, AWS CLIs and eksctl. For the sake of this workshop, we will use the last.\nCheck your eksctl version that your eksctl version is at least 0.57.0 eksctl version 0.57.0  If your eksctl version is lower than 0.57.0, use Installing or Upgrading eksctl in the user guide\n Create your IAM OIDC Identity Provider for your cluster eksctl utils associate-iam-oidc-provider --cluster eksworkshop-eksctl --approve 2021-07-20 17:51:36 [‚Ñπ] eksctl version 0.57.0 2021-07-20 17:51:36 [‚Ñπ] using region us-east-1 2021-07-20 17:51:38 [‚Ñπ] will create IAM Open ID Connect provider for cluster \u0026#34;eksworkshop-eksctl\u0026#34; in \u0026#34;us-east-1\u0026#34; 2021-07-20 17:51:39 [‚úî] created IAM Open ID Connect provider for cluster \u0026#34;eksworkshop-eksctl\u0026#34; in \u0026#34;us-east-1\u0026#34;  If you go to the Identity Providers in IAM Console, you will see OIDC provider has created for your cluster\n"
},
{
	"uri": "/beginner/185_bottlerocket/launchbottlerocket/",
	"title": "Launch Bottlerocket",
	"tags": [],
	"description": "",
	"content": "Add Bottlerocket nodes to an EKS cluster Create an environment variable for the kubernetes version. We will use this in the next step.\nK8S_VERSION=`kubectl version | grep Server | grep -Eo \u0026#39;.\u0026#34;v.{0,4}\u0026#39; | sed -n \u0026#39;s/.*:\u0026#34;v//p\u0026#39;` echo K8S_VERSION: ${K8S_VERSION} Create an eksctl deployment file (eksworkshop_bottlerocket.yaml) use in creating your cluster using the following syntax:\ncat \u0026lt;\u0026lt; EOF \u0026gt; eksworkshop_bottlerocket.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} version: \u0026#34;${K8S_VERSION}\u0026#34; availabilityZones: [\u0026#34;${AZS[0]}\u0026#34;, \u0026#34;${AZS[1]}\u0026#34;, \u0026#34;${AZS[2]}\u0026#34;] nodeGroups: - name: ng-bottlerocket labels: { role: bottlerocket } instanceType: t2.small desiredCapacity: 3 amiFamily: Bottlerocket iam: attachPolicyARNs: - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore bottlerocket: settings: motd: \u0026#34;Hello from eksctl!\u0026#34; # To enable all of the control plane logs, uncomment below: # cloudWatch: # clusterLogging: # enableTypes: [\u0026#34;*\u0026#34;] secretsEncryption: keyARN: ${MASTER_ARN} EOF Next, use the file you created as the input for the eksctl cluster update.\neksctl create nodegroup -f eksworkshop_bottlerocket.yaml  Launching Bottlerocket nodes will take approximately 10 minutes\n Output: 2021-05-26 16:23:34 [‚Ñπ] node \u0026#34;ip-192-168-36-124.us-east-2.compute.internal\u0026#34; is ready 2021-05-26 16:23:34 [‚Ñπ] node \u0026#34;ip-192-168-4-14.us-east-2.compute.internal\u0026#34; is ready 2021-05-26 16:23:34 [‚Ñπ] node \u0026#34;ip-192-168-87-9.us-east-2.compute.internal\u0026#34; is ready  Next, run the following command to list all the nodes in the EKS cluster and you should see output as follows:\nkubectl get nodes Output: NAME STATUS ROLES AGE VERSION ip-192-168-21-9.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 8h v1.17.12-eks-7684af ip-192-168-36-124.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 72s v1.17.17 ip-192-168-4-14.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 71s v1.17.17 ip-192-168-42-0.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 8h v1.17.12-eks-7684af ip-192-168-71-214.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 8h v1.17.12-eks-7684af ip-192-168-87-9.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 71s v1.17.17  Your cluster now has 6 worker nodes, 3 of them are using Bottlerocket in an unmanaged nodegroup.\nUnmanaged nodegroups do not show up in the AWS EKS console(Configutaion/Compute tab), however the nodes show up in the AWS EKS console(Overview tab). You can also use the \u0026ldquo;eksctl get nodegroup\u0026rdquo; command to list both types of nodegroups.\neksctl get nodegroup --cluster=eksworkshop-eksctl Congratulations! You now have a fully working Amazon EKS Cluster with Bottlerocket nodes that is ready to use!\n"
},
{
	"uri": "/advanced/430_emr_on_eks/sample_workload/",
	"title": "Run sample workload",
	"tags": [],
	"description": "",
	"content": "Now let\u0026rsquo;s run a sample workload using one of the inbuilt example scripts that calculates the value of pi.\nFirst get the virtual EMR clusters id and arn of the role that EMR uses for job execution.\nexport VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query \u0026#34;virtualClusters[?state==\u0026#39;RUNNING\u0026#39;].id\u0026#34; --output text) export EMR_ROLE_ARN=$(aws iam get-role --role-name EMRContainers-JobExecutionRole --query Role.Arn --output text) Lets start a sample spark job.\naws emr-containers start-job-run \\  --virtual-cluster-id=$VIRTUAL_CLUSTER_ID \\  --name=pi-2 \\  --execution-role-arn=$EMR_ROLE_ARN \\  --release-label=emr-6.2.0-latest \\  --job-driver=\u0026#39;{ \u0026#34;sparkSubmitJobDriver\u0026#34;: { \u0026#34;entryPoint\u0026#34;: \u0026#34;local:///usr/lib/spark/examples/src/main/python/pi.py\u0026#34;, \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1\u0026#34; } }\u0026#39; You will be able to see the completed job in EMR console. It should look like below:\nIn the next few sections we will cover how to use spark history server to view job history. We will also take a look at how to send logs to s3 and cloudwatch.\n"
},
{
	"uri": "/advanced/340_appmesh_flagger/canary_analysis/",
	"title": "About Canary Analysis",
	"tags": [],
	"description": "",
	"content": "Before we start the canary deployment setup, lets learn more about the canary analysis and how it works in Flagger.\nCanary Resource Flagger can be configured to automate the release process for Kubernetes workloads with a custom resource named canary which we installed in the previous chapter. The canary custom resource defines the release process of an application running on Kubernetes.\nWe have defined the canary release with progressive traffic shifting for the deployment of backend service detail here.\nWhen we deploy a new version of detail backend service, Flagger gradually shifts traffic to the canary, and at the same time, measures the requests success rate as well as the average response duration.\nCanary Target Below is one of the section from Canary Resource for canary target for detail service. For more details, see Flagger documentation here.\nspec: targetRef: apiVersion: apps/v1 kind: Deployment name: detail progressDeadlineSeconds: 60  Based on the above configuration, Flagger generates deployment/detail-primary Kubernetes object during canary analysis. This primary deployment is considered the stable release of our detail service, by default all traffic is routed to this version and the target deployment is scaled to zero. Flagger will detect changes to the target deployment (including secrets and configmaps) and will perform a canary analysis before promoting the new version as primary.\nThe progress deadline represents the maximum time in seconds for the canary deployment to make progress before it is rolled back, defaults to ten minutes.\nCanary Service Below canary service section from Canary Resource dictates how the target workload is exposed inside the cluster. The canary target should expose a TCP port that will be used by Flagger to create the ClusterIP Services. For more details, see Flagger documentation here.\nservice: # container port port: 3000  Based on our canary spec service, Flagger creates the following Kubernetes ClusterIP service\n  detail.flagger.svc.cluster.local\nselector app=detail-primary\n  detail-primary.flagger.svc.cluster.local\nselector app=detail-primary\n  detail-canary.flagger.svc.cluster.local\nselector app=detail\n  This ensures that traffic to detail.flagger:3000 will be routed to the latest stable release of our detail service. The detail-canary.flagger:3000 address is available only during the canary analysis and can be used for conformance testing or load testing.\nCanary Analysis The canary analysis defines:\n the type of deployment strategy  Canary Release A/B Testing Blue/Green Deployment   the metrics used to validate the canary version. Flagger comes with two builtin metric checks:  HTTP request success rate and duration HTTP request success duration   the webhooks used for acceptance test, load testing, etc the alerting settings  Flagger can be configured to send Slack notifications:    The canary analysis runs periodically until it reaches the maximum traffic weight or the number of iterations. On each run, Flagger calls the webhooks, checks the metrics and if the failed checks threshold is reached, stops the analysis and rolls back the canary. For more details, see Flagger documentation here.\nFor the canary analyis of detail service, we have used the below setup.\n We are checking the failed metrics for every iteration of canary analysis. If the request-success-rate metrics is below 99% or if the latency is greater than 500ms (which means the number of failures reach the threshold which is \u0026ldquo;1\u0026rdquo; in our setup), then canary analysis will fail and will rollback. We have pre-rollout webhook for running acceptance-test that are executed before routing traffic to canary. The canary advancement is paused if the pre-rollout hook fails and the canary will rollback. We also have rollout hook for load-test that are executed during the analysis on each iteration before the metric checks. If a rollout hook call fails; the canary advancement is paused and eventfully rolled back.  # define the canary analysis timing and KPIs analysis: # schedule interval (default 60s) interval: 1m # max number of failed metric checks before rollback threshold: 1 # max traffic percentage routed to canary # percentage (0-100) maxWeight: 15 # canary increment step # percentage (0-100) stepWeight: 5 # App Mesh Prometheus checks metrics: - name: request-success-rate # minimum req success rate (non 5xx responses) # percentage (0-100) thresholdRange: min: 99 interval: 1m - name: request-duration # maximum req duration P99 # milliseconds thresholdRange: max: 500 interval: 30s # testing (optional) webhooks: - name: acceptance-test type: pre-rollout url: http://flagger-loadtester.flagger/ timeout: 30s metadata: type: bash cmd: \u0026#39;curl -s http://detail-canary.flagger:3000/ping | grep \u0026#34;Healthy\u0026#34;\u0026#39; - name: \u0026#34;load test\u0026#34; type: rollout url: http://flagger-loadtester.flagger/ timeout: 15s metadata: cmd: \u0026#34;hey -z 1m -q 5 -c 2 http://detail-canary.flagger:3000/ping\u0026#34;  Canary Status Get the current status of canary deployments in our cluster:\nkubectl get canaries --all-namespaces NAMESPACE NAME STATUS WEIGHT LASTTRANSITIONTIME flagger detail Succeeded 0 2021-03-23T05:16:21Z  The status condition reflects the last known state of the canary analysis:\nkubectl -n flagger get canary/detail -oyaml | awk \u0026#39;/status/,0\u0026#39;  status: canaryWeight: 0 conditions: - lastTransitionTime: \u0026#34;2021-03-23T05:16:21Z\u0026#34; lastUpdateTime: \u0026#34;2021-03-23T05:16:21Z\u0026#34; message: Canary analysis completed successfully, promotion finished. reason: Succeeded status: \u0026#34;True\u0026#34; type: Promoted failedChecks: 0 iterations: 0 lastAppliedSpec: 75cf74bc9b lastTransitionTime: \u0026#34;2021-03-23T05:16:21Z\u0026#34; phase: Succeeded trackedConfigs: {}  The Promoted status condition can have one of the following reasons:\n Initialized Waiting Progressing Promoting Finalising Succeeded Failed  A failed canary will have the promoted status set to false, the reason to failed and the last applied spec will be different to the last promoted one.\nstatus: canaryWeight: 0 conditions: - lastTransitionTime: \u0026#34;2021-03-24T01:07:21Z\u0026#34; lastUpdateTime: \u0026#34;2021-03-24T01:07:21Z\u0026#34; message: Canary analysis failed, Deployment scaled to zero. reason: Failed status: \u0026#34;False\u0026#34; type: Promoted failedChecks: 0 iterations: 0 lastAppliedSpec: 6d5749b74f lastTransitionTime: \u0026#34;2021-03-24T01:07:21Z\u0026#34; phase: Failed trackedConfigs: {}  Now lets deploy the app and canary analysis setup and see this in action!\n"
},
{
	"uri": "/beginner/150_spotnodegroups/spotlifecycle/",
	"title": "Spot Configuration and Lifecycle",
	"tags": [],
	"description": "",
	"content": "View the Spot Managed Node Group Configuration Use the AWS Management Console to inspect the Spot managed node group deployed in your Kubernetes cluster. Select Elastic Kubernetes Service, click on Clusters, and then on eksworkshop-eksctl cluster. Select the Configuration tab and Compute sub tab. You can see 2 node groups created - one On-Demand node group and one Spot node groups.\nClick on ng-spot group and you can see the instance types set from the create command.\nClick on the Auto Scaling group name in the Details tab. Scroll to the Purchase options and instance types settings. Note how Spot best practices are applied out of the box:\n Capacity Optimized allocation strategy, which will launch Spot Instances from the most-available spare capacity pools. This results in minimizing the Spot Interruptions. Capacity Rebalance helps EKS managed node groups manage the lifecycle of the Spot Instance by proactively replacing instances that are at higher risk of being interrupted. This results in proactively augmenting your fleet with a new Spot Instance before a running instance is interrupted by EC2  Interruption Handling in Spot Managed Node Groups To handle Spot interruptions, you do not need to install any extra automation tools on the cluster, like, AWS Node Termination Handler. The managed node group handles Spot interruptions for you in the following way: the underlying EC2 Auto Scaling group is opted-in to Capacity Rebalancing, which means that when one of the Spot Instances in your node group is at elevated risk of interruption and gets an EC2 instance rebalance recommendation, it will attempt to launch a replacement instance. The more instance types you configure in the managed node group, the more chances EC2 Auto Scaling has of launching a replacement Spot Instance.\nWhen a Spot node receives a rebalance recommendation\n Amazon EKS automatically attempts to launch a new replacement Spot node and waits until it successfully joins the cluster. When a replacement Spot node is bootstrapped and in the Ready state on Kubernetes, Amazon EKS cordons and drains the Spot node that received the rebalance recommendation. Cordoning the Spot node ensures that the node is marked as \u0026lsquo;unschedulable\u0026rsquo; and hence the service controller doesn\u0026rsquo;t send any new requests to this Spot node. It also removes it from its list of healthy, active Spot nodes. Draining the Spot node ensures that running pods are evicted gracefully. If a Spot two-minute interruption notice arrives before the replacement Spot node is in a Ready state, Amazon EKS starts draining the Spot node that received the rebalance recommendation.  This process avoids waiting for new capacity to be available when there is a termination notice, and instead procures capacity in advance, limiting the time that pods might be left pending.\n"
},
{
	"uri": "/intermediate/246_monitoring_amp_amg/setup_iam/",
	"title": "Setup IAM",
	"tags": [],
	"description": "",
	"content": "Setup IAM for Prometheus Server to send metrics to AMP The shell script shown below can be used to execute the following actions after substituting the placeholder variable YOUR_EKS_CLUSTER_NAME with the name of your Amazon EKS cluster.\n Creates an IAM role with an IAM policy that has permissions to remote-write into an AMP workspace Creates a Kubernetes service account that is annotated with the IAM role Creates a trust relationship between the IAM role and the OIDC provider hosted in your Amazon EKS cluster  ##!/bin/bash CLUSTER_NAME=YOUR_EKS_CLUSTER_NAME AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026quot;Account\u0026quot; --output text) OIDC_PROVIDER=$(aws eks describe-cluster --name $CLUSTER_NAME --query \u0026quot;cluster.identity.oidc.issuer\u0026quot; --output text | sed -e \u0026quot;s/^https:\\/\\///\u0026quot;) PROM_SERVICE_ACCOUNT_NAMESPACE=prometheus GRAFANA_SERVICE_ACCOUNT_NAMESPACE=grafana SERVICE_ACCOUNT_NAME=iamproxy-service-account SERVICE_ACCOUNT_IAM_ROLE=EKS-AMP-ServiceAccount-Role SERVICE_ACCOUNT_IAM_ROLE_DESCRIPTION=\u0026quot;IAM role to be used by a K8s service account with write access to AMP\u0026quot; SERVICE_ACCOUNT_IAM_POLICY=AWSManagedPrometheusWriteAccessPolicy SERVICE_ACCOUNT_IAM_POLICY_ARN=arn:aws:iam::$AWS_ACCOUNT_ID:policy/$SERVICE_ACCOUNT_IAM_POLICY # # Setup a trust policy designed for a specific combination of K8s service account and namespace to sign in from a Kubernetes cluster which hosts the OIDC Idp. # If the IAM role already exists, then add this new trust policy to the existing trust policy # echo \u0026quot;Creating a new trust policy\u0026quot; read -r -d '' NEW_TRUST_RELATIONSHIP \u0026lt;\u0026lt;EOF [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Federated\u0026quot;: \u0026quot;arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRoleWithWebIdentity\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;${OIDC_PROVIDER}:sub\u0026quot;: \u0026quot;system:serviceaccount:${GRAFANA_SERVICE_ACCOUNT_NAMESPACE}:${SERVICE_ACCOUNT_NAME}\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Federated\u0026quot;: \u0026quot;arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRoleWithWebIdentity\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;${OIDC_PROVIDER}:sub\u0026quot;: \u0026quot;system:serviceaccount:${PROM_SERVICE_ACCOUNT_NAMESPACE}:${SERVICE_ACCOUNT_NAME}\u0026quot; } } } ] EOF # # Get the old trust policy, if one exists, and append it to the new trust policy # OLD_TRUST_RELATIONSHIP=$(aws iam get-role --role-name $SERVICE_ACCOUNT_IAM_ROLE --query 'Role.AssumeRolePolicyDocument.Statement[]' --output json) COMBINED_TRUST_RELATIONSHIP=$(echo $OLD_TRUST_RELATIONSHIP $NEW_TRUST_RELATIONSHIP | jq -s add) echo \u0026quot;Appending to the existing trust policy\u0026quot; read -r -d '' TRUST_POLICY \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: ${COMBINED_TRUST_RELATIONSHIP} } EOF echo \u0026quot;${TRUST_POLICY}\u0026quot; \u0026gt; TrustPolicy.json # # Setup the permission policy grants write permissions for all AWS StealFire workspaces # read -r -d '' PERMISSION_POLICY \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[ { \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;aps:RemoteWrite\u0026quot;, \u0026quot;aps:QueryMetrics\u0026quot;, \u0026quot;aps:GetSeries\u0026quot;, \u0026quot;aps:GetLabels\u0026quot;, \u0026quot;aps:GetMetricMetadata\u0026quot; ], \u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot; } ] } EOF echo \u0026quot;${PERMISSION_POLICY}\u0026quot; \u0026gt; PermissionPolicy.json # # Create an IAM permission policy to be associated with the role, if the policy does not already exist # SERVICE_ACCOUNT_IAM_POLICY_ID=$(aws iam get-policy --policy-arn $SERVICE_ACCOUNT_IAM_POLICY_ARN --query 'Policy.PolicyId' --output text) if [ \u0026quot;$SERVICE_ACCOUNT_IAM_POLICY_ID\u0026quot; = \u0026quot;\u0026quot; ]; then echo \u0026quot;Creating a new permission policy $SERVICE_ACCOUNT_IAM_POLICY\u0026quot; aws iam create-policy --policy-name $SERVICE_ACCOUNT_IAM_POLICY --policy-document file://PermissionPolicy.json else echo \u0026quot;Permission policy $SERVICE_ACCOUNT_IAM_POLICY already exists\u0026quot; fi # # If the IAM role already exists, then just update the trust policy. # Otherwise create one using the trust policy and permission policy # SERVICE_ACCOUNT_IAM_ROLE_ARN=$(aws iam get-role --role-name $SERVICE_ACCOUNT_IAM_ROLE --query 'Role.Arn' --output text) if [ \u0026quot;$SERVICE_ACCOUNT_IAM_ROLE_ARN\u0026quot; = \u0026quot;\u0026quot; ]; then echo \u0026quot;$SERVICE_ACCOUNT_IAM_ROLE role does not exist. Creating a new role with a trust and permission policy\u0026quot; # # Create an IAM role for Kubernetes service account # SERVICE_ACCOUNT_IAM_ROLE_ARN=$(aws iam create-role \\ --role-name $SERVICE_ACCOUNT_IAM_ROLE \\ --assume-role-policy-document file://TrustPolicy.json \\ --description \u0026quot;$SERVICE_ACCOUNT_IAM_ROLE_DESCRIPTION\u0026quot; \\ --query \u0026quot;Role.Arn\u0026quot; --output text) # # Attach the trust and permission policies to the role # aws iam attach-role-policy --role-name $SERVICE_ACCOUNT_IAM_ROLE --policy-arn $SERVICE_ACCOUNT_IAM_POLICY_ARN else echo \u0026quot;$SERVICE_ACCOUNT_IAM_ROLE_ARN role already exists. Updating the trust policy\u0026quot; # # Update the IAM role for Kubernetes service account with a with the new trust policy # aws iam update-assume-role-policy --role-name $SERVICE_ACCOUNT_IAM_ROLE --policy-document file://TrustPolicy.json fi echo $SERVICE_ACCOUNT_IAM_ROLE_ARN # EKS cluster hosts an OIDC provider with a public discovery endpoint. # Associate this Idp with AWS IAM so that the latter can validate and accept the OIDC tokens issued by Kubernetes to service accounts. # Doing this with eksctl is the easier and best approach. # eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --approve "
},
{
	"uri": "/beginner/115_sg-per-pod/20_rds/",
	"title": "RDS creation",
	"tags": ["beginner"],
	"description": "",
	"content": "Now that our security groups are ready let\u0026rsquo;s create our Amazon RDS for PostgreSQL database.\nWe first need to create a DB subnet groups. We will use the same subnets as our EKS cluster.\nexport PUBLIC_SUBNETS_ID=$(aws ec2 describe-subnets \\  --filters \u0026#34;Name=vpc-id,Values=$VPC_ID\u0026#34; \u0026#34;Name=tag:Name,Values=eksctl-eksworkshop-eksctl-cluster/SubnetPublic*\u0026#34; \\  --query \u0026#39;Subnets[*].SubnetId\u0026#39; \\  --output json | jq -c .) # create a db subnet group aws rds create-db-subnet-group \\  --db-subnet-group-name rds-eksworkshop \\  --db-subnet-group-description rds-eksworkshop \\  --subnet-ids ${PUBLIC_SUBNETS_ID} We can now create our database.\n# get RDS SG ID export RDS_SG=$(aws ec2 describe-security-groups \\  --filters Name=group-name,Values=RDS_SG Name=vpc-id,Values=${VPC_ID} \\  --query \u0026#34;SecurityGroups[0].GroupId\u0026#34; --output text) # generate a password for RDS export RDS_PASSWORD=\u0026#34;$(date | md5sum |cut -f1 -d\u0026#39; \u0026#39;)\u0026#34; echo ${RDS_PASSWORD} \u0026gt; ~/environment/sg-per-pod/rds_password # create RDS Postgresql instance aws rds create-db-instance \\  --db-instance-identifier rds-eksworkshop \\  --db-name eksworkshop \\  --db-instance-class db.t3.micro \\  --engine postgres \\  --db-subnet-group-name rds-eksworkshop \\  --vpc-security-group-ids $RDS_SG \\  --master-username eksworkshop \\  --publicly-accessible \\  --master-user-password ${RDS_PASSWORD} \\  --backup-retention-period 0 \\  --allocated-storage 20  It will take up to 4 minutes for the database to be created.\n You can verify if it\u0026rsquo;s available using this command.\naws rds describe-db-instances \\  --db-instance-identifier rds-eksworkshop \\  --query \u0026#34;DBInstances[].DBInstanceStatus\u0026#34; \\  --output text Expected output\navailable  Now that the database is available, let\u0026rsquo;s get our database Endpoint.\n# get RDS endpoint export RDS_ENDPOINT=$(aws rds describe-db-instances \\  --db-instance-identifier rds-eksworkshop \\  --query \u0026#39;DBInstances[0].Endpoint.Address\u0026#39; \\  --output text) echo \u0026#34;RDS endpoint: ${RDS_ENDPOINT}\u0026#34; Our last step is to create some content in the database.\nsudo amazon-linux-extras install -y postgresql12 cd sg-per-pod cat \u0026lt;\u0026lt; EoF \u0026gt; ~/environment/sg-per-pod/pgsql.sql CREATE TABLE welcome (column1 TEXT); insert into welcome values (\u0026#39;--------------------------\u0026#39;); insert into welcome values (\u0026#39;Welcome to the eksworkshop\u0026#39;); insert into welcome values (\u0026#39;--------------------------\u0026#39;); EoF export RDS_PASSWORD=$(cat ~/environment/sg-per-pod/rds_password) psql postgresql://eksworkshop:${RDS_PASSWORD}@${RDS_ENDPOINT}:5432/eksworkshop \\  -f ~/environment/sg-per-pod/pgsql.sql "
},
{
	"uri": "/beginner/120_network-policies/calico-enterprise/",
	"title": "Calico Enterprise Usecases",
	"tags": [],
	"description": "",
	"content": "CALICO ENTERPRISE USE CASES In this module, we will use Tigera‚Äôs Calico Enterprise to implement the following 3 Amazon EKS use cases:\n1) Implement Egress Access Controls Establishing service to service connectivity within your Amazon EKS cluster is easy. But how do you enable some of your workloads to securely connect to services like Amazon RDS, ElasticCache, etc. that are external to the cluster.\nWith Calico Enterprise, you can author DNS Policies that implement fine-grained access controls between a workload and the external services it needs to connect to.\n2) Troubleshoot Microservice Connectivity Troubleshooting microservice connectivity issues can be incredibly difficult due to the dynamic nature of container orchestration in Amazon EKS. Connectivity issues can be related to network performance, but often are the result of your Network Policies not being defined properly.\nWith Calico Enterprise, you can explore a visual representation of all your microservices communicating with each other, and filter by connections that were accepted and denied. You can further drill into an individual denied connection to see which Network Policy evaluated and denied the connection, and then fix the policy to resolve the connectivity issue.\n3) Implement Enterprise Security Controls Many applications have compliance requirements such as workload isolation, ensuring dev cannot talk to prod or implementing network zones (e.g. DMZ can communicate with the public internet but not your backend databases). While you can implement these rules using open-source Project Calico, there are a few limitations:\n It is possible for other users to inadvertently override or intentionally tamper with your security controls, and very difficult to detect when that happens. Proof that the policies have been enforced now and in the past is not possible  With Calico Enterprise, you can implement Security Controls at a higher precedent policy tier that cannot be overridden by other users. You will also learn how to provide audit reports that demonstrate compliance now and historically, as well as audit (or alert on) changes to policies.\n"
},
{
	"uri": "/beginner/091_iam-groups/create-iam-roles/",
	"title": "Create IAM Roles",
	"tags": [],
	"description": "",
	"content": "We are going to create 3 roles:\n a k8sAdmin role which will have admin rights in our EKS cluster a k8sDev role which will give access to the developers namespace in our EKS cluster a k8sInteg role which will give access to the integration namespace in our EKS cluster  Create the roles:\nPOLICY=$(echo -n \u0026#39;{\u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;,\u0026#34;Statement\u0026#34;:[{\u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;,\u0026#34;Principal\u0026#34;:{\u0026#34;AWS\u0026#34;:\u0026#34;arn:aws:iam::\u0026#39;; echo -n \u0026#34;$ACCOUNT_ID\u0026#34;; echo -n \u0026#39;:root\u0026#34;},\u0026#34;Action\u0026#34;:\u0026#34;sts:AssumeRole\u0026#34;,\u0026#34;Condition\u0026#34;:{}}]}\u0026#39;) echo ACCOUNT_ID=$ACCOUNT_ID echo POLICY=$POLICY aws iam create-role \\  --role-name k8sAdmin \\  --description \u0026#34;Kubernetes administrator role (for AWS IAM Authenticator for Kubernetes).\u0026#34; \\  --assume-role-policy-document \u0026#34;$POLICY\u0026#34; \\  --output text \\  --query \u0026#39;Role.Arn\u0026#39; aws iam create-role \\  --role-name k8sDev \\  --description \u0026#34;Kubernetes developer role (for AWS IAM Authenticator for Kubernetes).\u0026#34; \\  --assume-role-policy-document \u0026#34;$POLICY\u0026#34; \\  --output text \\  --query \u0026#39;Role.Arn\u0026#39; aws iam create-role \\  --role-name k8sInteg \\  --description \u0026#34;Kubernetes role for integration namespace in quick cluster.\u0026#34; \\  --assume-role-policy-document \u0026#34;$POLICY\u0026#34; \\  --output text \\  --query \u0026#39;Role.Arn\u0026#39;  In this example, the assume-role-policy allows the root account to assume the role. We are going to allow specific groups to also be able to assume those roles. Check the official documentation for more information.\n Because the above roles are only used to authenticate within our EKS cluster, they don\u0026rsquo;t need to have AWS permissions. We will only use them to allow some IAM groups to assume this role in order to have access to our EKS cluster.\n"
},
{
	"uri": "/advanced/420_kubeflow/dashboard/",
	"title": "Kubeflow Dashboard",
	"tags": [],
	"description": "",
	"content": "Kubeflow Dashboard Use port-forward to access Kubeflow dashboard\nkubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 In your Cloud9 environment, click Tools / Preview / Preview Running Application to access dashboard. You can click on Pop out window button to maximize browser into new tab.\nLeave the current terminal running because if you kill the process, you will loose access to the dashboard. Open new Terminal to follow rest of the workshop\nClick on Start Setup\nSpecify the namespace as eksworkshop\nClick on Finish to view the dashboard\n"
},
{
	"uri": "/beginner/190_ocean/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Cleaning up If you are running in your own account, you may keep your Ocean cluster and your Spot.io Account, as it is free of charge for up to 20 Nodes. If you are running in an account that was created for you as part of an AWS event, there is no need to delete the resources used in this chapter as the AWS account will be closed automatically, but you should still proceed with the deletion of the Ocean cluster and closing of the Spot.io account.\n To delete the resources used in this chapter from your EKS cluster:\nkubectl delete -f test_deployments.yaml kubectl delete deployment spotinst-kubernetes-cluster-controller -n kube-system In order to delete the Ocean cluster, on the Ocean dashboard click Actions, and then \u0026ldquo;Delete\u0026rdquo;. Click \u0026ldquo;Delete Cluster\u0026rdquo; in the prompt. Next in order to delete your Spot.io Organization, click the blue \u0026ldquo;Chat\u0026rdquo; button on far right edge of your Console, and ask the 24/7 Spot.io Tech Support team to delete it.\n If your wish to make further use of your Spot.io Organization (to connect your real cloud account later, for example) you may leave your Organization intact, and instead ask the Spot.io Tech Support team to allow you to create a new account, and make that one your default. Once that is done, you can ask for the account you were using during the workshop to be deleted.\n Finally, to detach your AWS cloud account from the platform, you need to delete the CloudFormation stack used to create the IAM Role and Policy during the account connection stage. Browse to the AWS ClouFormation Console, go to Stacks, Search for \u0026ldquo;spotinst-iam\u0026rdquo; and delete the Stack.\n"
},
{
	"uri": "/beginner/160_advanced-networking/secondary_cidr/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Before we configure EKS, we need to enable secondary CIDR blocks in your VPC and make sure they have proper tags and route table configurations\nAdd secondary CIDRs to your VPC There are restrictions on the range of secondary CIDRs you can use to extend your VPC. For more info, see IPv4 CIDR Block Association Restrictions\n You can use below commands to add 100.64.0.0/16 to your EKS cluster VPC. Please note to change the Values parameter to EKS cluster name if you used different name than eksctl-eksworkshop\nVPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=eksctl-eksworkshop* --query 'Vpcs[].VpcId' --output text) aws ec2 associate-vpc-cidr-block --vpc-id $VPC_ID --cidr-block 100.64.0.0/16 Next step is to create subnets. Before we do this step, let\u0026rsquo;s check how many subnets we are consuming. You can run this command to see EC2 instance and AZ details\naws ec2 describe-instances --filters \u0026quot;Name=tag-key,Values=eks:cluster-name\u0026quot; \u0026quot;Name=tag-value,Values=eksworkshop*\u0026quot; --query 'Reservations[*].Instances[*].[PrivateDnsName,Tags[?Key==`eks:nodegroup-name`].Value|[0],Placement.AvailabilityZone,PrivateIpAddress,PublicIpAddress]' --output table ------------------------------------------------------------------------------------------------------------------------------------------ | DescribeInstances | \u0026#43;-----------------------------------------------\u0026#43;---------------------------------------\u0026#43;-------------\u0026#43;-----------------\u0026#43;----------------\u0026#43; | ip-192-168-9-228.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2c | 192.168.9.228 | 18.191.57.131 | | ip-192-168-71-211.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2a | 192.168.71.211 | 18.221.77.249 | | ip-192-168-33-135.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2b | 192.168.33.135 | 13.59.167.90 | \u0026#43;-----------------------------------------------\u0026#43;---------------------------------------\u0026#43;-------------\u0026#43;-----------------\u0026#43;----------------\u0026#43;  I have 3 instances and using 3 subnets in my environment. For simplicity, we will use the same AZ\u0026rsquo;s and create 3 secondary CIDR subnets but you can certainly customize according to your networking requirements. Remember to change the AZ names according to your environment\nexport AZ1=us-east-2a export AZ2=us-east-2b export AZ3=us-east-2c CGNAT_SNET1=$(aws ec2 create-subnet --cidr-block 100.64.0.0/19 --vpc-id $VPC_ID --availability-zone $AZ1 --query 'Subnet.SubnetId' --output text) CGNAT_SNET2=$(aws ec2 create-subnet --cidr-block 100.64.32.0/19 --vpc-id $VPC_ID --availability-zone $AZ2 --query 'Subnet.SubnetId' --output text) CGNAT_SNET3=$(aws ec2 create-subnet --cidr-block 100.64.64.0/19 --vpc-id $VPC_ID --availability-zone $AZ3 --query 'Subnet.SubnetId' --output text) Next step is to add Kubernetes tags on newer Subnets. You can check these tags by querying your current subnets\naws ec2 describe-subnets --filters Name=cidr-block,Values=192.168.0.0/19 --output text Output shows similar to this TAGS aws:cloudformation:logical-id SubnetPublicUSEAST2C TAGS kubernetes.io/role/elb 1 TAGS eksctl.cluster.k8s.io/v1alpha1/cluster-name eksworkshop-eksctl TAGS Name eksctl-eksworkshop-eksctl-cluster/SubnetPublicUSEAST2C TAGS aws:cloudformation:stack-name eksctl-eksworkshop-eksctl-cluster TAGS kubernetes.io/cluster/eksworkshop-eksctl shared TAGS aws:cloudformation:stack-id arn:aws:cloudformation:us-east-2:012345678901:stack/eksctl-eksworkshop-eksctl-cluster/8da51fc0-2b5e-11e9-b535-022c6f51bf82  Here are the commands to add tags to both the subnets\naws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=kubernetes.io/role/elb,Value=1 aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=kubernetes.io/role/elb,Value=1 aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=kubernetes.io/role/elb,Value=1 As next step, we need to associate three new subnets into a route table. Again for simplicity, we chose to add new subnets to the Public route table that has connectivity to Internet Gateway\nSNET1=$(aws ec2 describe-subnets --filters Name=cidr-block,Values=192.168.0.0/19 --query 'Subnets[].SubnetId' --output text) RTASSOC_ID=$(aws ec2 describe-route-tables --filters Name=association.subnet-id,Values=$SNET1 --query 'RouteTables[].RouteTableId' --output text) aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET1 aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET2 aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET3 "
},
{
	"uri": "/advanced/410_batch/jobs/",
	"title": "Kubernetes Jobs",
	"tags": [],
	"description": "",
	"content": "Kubernetes Jobs A job creates one or more pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the job tracks the successful completions. When a specified number of successful completions is reached, the job itself is complete. Deleting a Job will cleanup the pods it created.\nLet\u0026rsquo;s start by creating the job-whalesay.yaml manifest using this command\nmkdir ~/environment/batch_policy/ cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/batch_policy/job-whalesay.yaml apiVersion: batch/v1 kind: Job metadata: name: whalesay spec: template: spec: containers: - name: whalesay image: docker/whalesay command: [\u0026#34;cowsay\u0026#34;, \u0026#34;This is a Kubernetes Job!\u0026#34;] restartPolicy: Never backoffLimit: 4 EoF Run a sample Kubernetes Job using the whalesay image.\nkubectl apply -f ~/environment/batch_policy/job-whalesay.yaml Wait until the job has completed successfully.\nkubectl get job/whalesay NAME COMPLETIONS DURATION AGE whalesay 1/1 3s 21s  Confirm the output.\nkubectl logs -l job-name=whalesay ___________________________ \u0026lt; This is a Kubernetes Job! \u0026gt; --------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  "
},
{
	"uri": "/advanced/310_servicemesh_with_istio/download/",
	"title": "Download and Install Istio CLI",
	"tags": [],
	"description": "",
	"content": "Before we can get started configuring Istio we‚Äôll need to first install the command line tools that you will interact with. To do this run the following.\nWe will use istio version 1.10.0\n echo \u0026#39;export ISTIO_VERSION=\u0026#34;1.10.0\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${HOME}/.bash_profile source ${HOME}/.bash_profile cd ~/environment curl -L https://istio.io/downloadIstio | ISTIO_VERSION=${ISTIO_VERSION} sh - The installation directory contains:\n Installation YAML files for Kubernetes in install/kubernetes Sample applications in samples/ The istioctl client binary in the bin/ directory (istioctl is used when manually injecting Envoy as a sidecar proxy).  cd ${HOME}/environment/istio-${ISTIO_VERSION} sudo cp -v bin/istioctl /usr/local/bin/ We can verify that we have the proper version in our $PATH\nistioctl version --remote=false "
},
{
	"uri": "/intermediate/330_app_mesh/install_app_mesh_controller/",
	"title": "AWS App Mesh Integration",
	"tags": [],
	"description": "",
	"content": "This tutorial guides you through the installation and use of the open source AWS App Mesh Controller for Kubernetes. When using the App Mesh Controller, you manage your App Mesh resources such as Virtual Services and Virtual Nodes through the Kubernetes API the same way you manage native Kubernetes resources such as Services and Deployments.\nThe controller monitors your Kubernetes objects, and when App Mesh resources are created or changed it reflects those changes in AWS App Mesh for you. Specification of your App Mesh resources is done through the use of Custom Resource Definitions (CRDs) provided by the App Mesh Controller project. In this tutorial, you will use the following custom resources:\n meshes virtualservices virtualnodes virtualrouters  The controller will handle routine App Mesh tasks such as creating and injecting Envoy proxy containers into your application pods. Automated sidecar injection is controlled by enabling a webhook on a per-namespace basis. This is optional but recommended as it makes using App Mesh in Kubernetes transparent.\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/deploy_applications/",
	"title": "Deploy Product Catalog app",
	"tags": [],
	"description": "",
	"content": "To understand AWS App Mesh, its best to also understand any applications that run on top of it. In this chapter, we\u0026rsquo;ll walk you through the following parts of application setup and deployment:\n Discuss the Application Architecture Build the Application Services container images Push the container images to Amazon ECR Deploy the application services into the Amazon EKS cluster, initially without AWS App Mesh. Test the connectivity between the Services  "
},
{
	"uri": "/intermediate/240_monitoring/dashboards/",
	"title": "Dashboards",
	"tags": [],
	"description": "",
	"content": "Log in to Grafana Log in to Grafana dashboard using credentials supplied during configuration.\nYou will notice that \u0026lsquo;Install Grafana\u0026rsquo; \u0026amp; \u0026lsquo;create your first data source\u0026rsquo; are already completed. We will import community created dashboard for this tutorial.\nCluster Monitoring Dashboard For creating a dashboard to monitor the cluster:\n Click '+' button on left panel and select \u0026lsquo;Import\u0026rsquo;. Enter 3119 dashboard id under Grafana.com Dashboard. Click \u0026lsquo;Load\u0026rsquo;. Select \u0026lsquo;Prometheus\u0026rsquo; as the endpoint under prometheus data sources drop down. Click \u0026lsquo;Import\u0026rsquo;.  This will show monitoring dashboard for all cluster nodes\nPods Monitoring Dashboard For creating a dashboard to monitor all the pods:\n Click '+' button on left panel and select \u0026lsquo;Import\u0026rsquo;. Enter 6417 dashboard id under Grafana.com Dashboard. Click \u0026lsquo;Load\u0026rsquo;. Enter Kubernetes Pods Monitoring as the Dashboard name. Click change to set the Unique identifier (uid). Select \u0026lsquo;Prometheus\u0026rsquo; as the endpoint under prometheus data sources drop down.s Click \u0026lsquo;Import\u0026rsquo;.  "
},
{
	"uri": "/beginner/090_rbac/create_iam_user/",
	"title": "Create a User",
	"tags": [],
	"description": "",
	"content": " For the sake of simplicity, in this chapter, we will save credentials to a file to make it easy to toggle back and forth between users. Never do this in production or with credentials that have privileged access; It is not a security best practice to store credentials on the filesystem.\n From within the Cloud9 terminal, create a new user called rbac-user, and generate/save credentials for it:\naws iam create-user --user-name rbac-user aws iam create-access-key --user-name rbac-user | tee /tmp/create_output.json By running the previous step, you should get a response similar to:\n{ \u0026#34;AccessKey\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;rbac-user\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2019-07-17T15:37:27Z\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026lt; AWS Secret Access Key \u0026gt; , \u0026#34;AccessKeyId\u0026#34;: \u0026lt; AWS Access Key \u0026gt; } }  To make it easy to switch back and forth between the admin user you created the cluster with, and this new rbac-user, run the following command to create a script that when sourced, sets the active user to be rbac-user:\ncat \u0026lt;\u0026lt; EoF \u0026gt; rbacuser_creds.sh export AWS_SECRET_ACCESS_KEY=$(jq -r .AccessKey.SecretAccessKey /tmp/create_output.json) export AWS_ACCESS_KEY_ID=$(jq -r .AccessKey.AccessKeyId /tmp/create_output.json) EoF "
},
{
	"uri": "/010_introduction/basics/",
	"title": "Kubernetes (k8s) Basics",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n What is Kubernetes   Kubernetes Nodes   K8s Objects Overview   K8s Objects Detail (1/2)   K8s Objects Detail (2/2)   "
},
{
	"uri": "/beginner/050_deploy/deploycrystal/",
	"title": "Deploy Crystal Backend API",
	"tags": [],
	"description": "",
	"content": "Let‚Äôs bring up the Crystal Backend API!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-crystal "
},
{
	"uri": "/920_cleanup/undeploy/",
	"title": "Undeploy the applications",
	"tags": [],
	"description": "",
	"content": "To delete the resources created by the applications, we should delete the application deployments and kubernetes dashboard.\nNote that if you followed the cleanup section of the modules, some of the commands below might fail because there is nothing to delete and its ok.\nUndeploy the applications:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml export DASHBOARD_VERSION=\u0026#34;v2.0.0\u0026#34; kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/${DASHBOARD_VERSION}/src/deploy/recommended/kubernetes-dashboard.yaml "
},
{
	"uri": "/030_eksctl/launcheks/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": " DO NOT PROCEED with this step unless you have validated the IAM role in use by the Cloud9 IDE. You will not be able to run the necessary kubectl commands in the later modules unless the EKS cluster is built using the IAM role.\n Challenge: How do I check the IAM role on the workspace?\n  Expand here to see the solution   Run aws sts get-caller-identity and validate that your Arn contains eksworkshop-adminand an Instance Id.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/eksworkshop-admin/i-01234567890abcdef\u0026quot; } If you do not see the correct role, please go back and validate the IAM role for troubleshooting.\nIf you do see the correct role, proceed to next step to create an EKS cluster.\n  Create an EKS cluster eksctl version must be 0.38.0 or above to deploy EKS 1.19, click here to get the latest version.\n Create an eksctl deployment file (eksworkshop.yaml) use in creating your cluster using the following syntax:\ncat \u0026lt;\u0026lt; EOF \u0026gt; eksworkshop.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} version: \u0026#34;1.19\u0026#34; availabilityZones: [\u0026#34;${AZS[0]}\u0026#34;, \u0026#34;${AZS[1]}\u0026#34;, \u0026#34;${AZS[2]}\u0026#34;] managedNodeGroups: - name: nodegroup desiredCapacity: 3 instanceType: t3.small ssh: enableSsm: true # To enable all of the control plane logs, uncomment below: # cloudWatch: # clusterLogging: # enableTypes: [\u0026#34;*\u0026#34;] secretsEncryption: keyARN: ${MASTER_ARN} EOF Next, use the file you created as the input for the eksctl cluster creation.\nWe are deliberatly launching at least one Kubernetes version behind the latest available on Amazon EKS. This allows you to perform the cluster upgrade lab.\n eksctl create cluster -f eksworkshop.yaml  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/tabs-example/tabs/eksctl/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": "To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/beginner/170_statefulset/statefulset/",
	"title": "Create StatefulSet",
	"tags": [],
	"description": "",
	"content": "Introduction StatefulSet consists of serviceName, replicas, template and volumeClaimTemplates:\n serviceName is \u0026ldquo;mysql\u0026rdquo;, headless service we created in previous section replicas is 3, the desired number of pod template is the configuration of pod volumeClaimTemplates is to claim volume for pod based on storageClassName, mysql-gp2 that we created in the Define Storageclass section.  Percona Xtrabackup is used in the template to clone source MySQL server to its followers.\n Create StatefulSet Copy/Paste the following commands into your Cloud9 Terminal.\ncd ${HOME}/environment/ebs_statefulset wget https://eksworkshop.com/beginner/170_statefulset/statefulset.files/mysql-statefulset.yaml Create the StatefulSet \u0026ldquo;mysql\u0026rdquo; by running the following command.\nkubectl apply -f ${HOME}/environment/ebs_statefulset/mysql-statefulset.yaml Watch StatefulSet Watch the status of StatefulSet.\nkubectl -n mysql rollout status statefulset mysql It will take few minutes for pods to initialize and have StatefulSet created.\nWaiting for 2 pods to be ready... Waiting for 1 pods to be ready... partitioned roll out complete: 2 new pods have been updated...  Open another Cloud9 Terminal and watch the progress of pods creation using the following command.\nkubectl -n mysql get pods -l app=mysql --watch You can see ordered, graceful deployment with a stable, unique name for each pod.\nNAME READY STATUS RESTARTS AGE mysql-0 0/2 Init:0/2 0 16s mysql-0 0/2 Init:1/2 0 17s mysql-0 0/2 PodInitializing 0 18s mysql-0 1/2 Running 0 19s mysql-0 2/2 Running 0 25s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Init:0/2 0 0s mysql-1 0/2 Init:1/2 0 10s mysql-1 0/2 PodInitializing 0 11s mysql-1 1/2 Running 0 12s mysql-1 2/2 Running 0 16s  Press Ctrl+C to stop watching.\n Check the dynamically created PVC by following command.\nkubectl -n mysql get pvc -l app=mysql We can see data-mysql-0, and data-mysql-1 have been created with the STORAGECLASS mysql-gp2.\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mysql-0 Bound pvc-2a9bb222-3fbe-11ea-94be-0aff3e98c5a0 10Gi RWO mysql-gp2 22m data-mysql-1 Bound pvc-47076f1d-3fbe-11ea-94be-0aff3e98c5a0 10Gi RWO mysql-gp2 21m  And now the same information from the EC2 console.\nWe can see the EBS volumes have been automatically encrypted by the AWS Key Management Service (KMS)\n "
},
{
	"uri": "/beginner/060_helm/helm_micro/customize/",
	"title": "Customize Defaults",
	"tags": [],
	"description": "",
	"content": "If you look in the newly created eksdemo directory, you\u0026rsquo;ll see several files and directories.\nThe table below outlines the purpose of each component in the Helm chart structure.\n   File or Directory Description     charts/ Sub-charts that the chart depends on   Chart.yaml Information about your chart   values.yaml The default values for your templates   template/ The template files   template/deployment.yaml Basic manifest for creating Kubernetes Deployment objects   template/_helpers.tpl Used to define Go template helpers   template/hpa.yaml Basic manifest for creating Kubernetes Horizontal Pod Autoscaler objects   template/ingress.yaml Basic manifest for creating Kubernetes Ingress objects   template/NOTES.txt A plain text file to give users detailed information about how to use the newly installed chart   template/serviceaccount.yaml Basic manifest for creating Kubernetes ServiceAccount objects   template/service.yaml Basic manifest for creating Kubernetes Service objects   tests/ Directory of Test files   tests/test-connections.yaml Tests that validate that your chart works as expected when it is installed    We\u0026rsquo;re actually going to create our own files, so we\u0026rsquo;ll delete these boilerplate files\nrm -rf ~/environment/eksdemo/templates/ rm ~/environment/eksdemo/Chart.yaml rm ~/environment/eksdemo/values.yaml Run the following code block to create a new Chart.yaml file which will describe the chart\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/Chart.yaml apiVersion: v2 name: eksdemo description: A Helm chart for EKS Workshop Microservices application version: 0.1.0 appVersion: 1.0 EoF Next we\u0026rsquo;ll copy the manifest files for each of our microservices into the templates directory as servicename.yaml\n#create subfolders for each template type mkdir -p ~/environment/eksdemo/templates/deployment mkdir -p ~/environment/eksdemo/templates/service # Copy and rename frontend manifests cp ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/frontend.yaml cp ~/environment/ecsdemo-frontend/kubernetes/service.yaml ~/environment/eksdemo/templates/service/frontend.yaml # Copy and rename crystal manifests cp ~/environment/ecsdemo-crystal/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/crystal.yaml cp ~/environment/ecsdemo-crystal/kubernetes/service.yaml ~/environment/eksdemo/templates/service/crystal.yaml # Copy and rename nodejs manifests cp ~/environment/ecsdemo-nodejs/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/nodejs.yaml cp ~/environment/ecsdemo-nodejs/kubernetes/service.yaml ~/environment/eksdemo/templates/service/nodejs.yaml All files in the templates directory are sent through the template engine. These are currently plain YAML files that would be sent to Kubernetes as-is.\nReplace hard-coded values with template directives Let\u0026rsquo;s replace some of the values with template directives to enable more customization by removing hard-coded values.\nOpen ~/environment/eksdemo/templates/deployment/frontend.yaml in your Cloud9 editor.\nThe following steps should be completed seperately for frontend.yaml, crystal.yaml, and nodejs.yaml.\n Under spec, find replicas: 1 and replace with the following:\nreplicas: {{ .Values.replicas }} Under spec.template.spec.containers.image, replace the image with the correct template value from the table below:\n   Filename Value     frontend.yaml - image: {{ .Values.frontend.image }}:{{ .Values.version }}   crystal.yaml - image: {{ .Values.crystal.image }}:{{ .Values.version }}   nodejs.yaml - image: {{ .Values.nodejs.image }}:{{ .Values.version }}    Create a values.yaml file with our template defaults Run the following code block to populate our template directives with default values.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/values.yaml # Default values for eksdemo. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Release-wide Values replicas: 3 version: \u0026#39;latest\u0026#39; # Service Specific Values nodejs: image: brentley/ecsdemo-nodejs crystal: image: brentley/ecsdemo-crystal frontend: image: brentley/ecsdemo-frontend EoF "
},
{
	"uri": "/intermediate/210_jenkins/deploy/",
	"title": "Deploy Jenkins",
	"tags": [],
	"description": "",
	"content": "Install Jenkins We\u0026rsquo;ll begin by creating the values.yaml to declare the configuration of our Jenkins installation.\ncat \u0026lt;\u0026lt; EOF \u0026gt; values.yaml --- controller: # Used for label app.kubernetes.io/component componentName: \u0026#34;jenkins-controller\u0026#34; image: \u0026#34;jenkins/jenkins\u0026#34; tag: \u0026#34;2.289.2-lts-jdk11\u0026#34; additionalPlugins: - aws-codecommit-jobs:0.3.0 - aws-java-sdk:1.11.995 - junit:1.51 - ace-editor:1.1 - workflow-support:3.8 - pipeline-model-api:1.8.5 - pipeline-model-definition:1.8.5 - pipeline-model-extensions:1.8.5 - workflow-job:2.41 - credentials-binding:1.26 - aws-credentials:1.29 - credentials:2.5 - lockable-resources:2.11 - branch-api:2.6.4 resources: requests: cpu: \u0026#34;1024m\u0026#34; memory: \u0026#34;4Gi\u0026#34; limits: cpu: \u0026#34;4096m\u0026#34; memory: \u0026#34;8Gi\u0026#34; javaOpts: \u0026#34;-Xms4000m -Xmx4000m\u0026#34; servicePort: 80 serviceType: LoadBalancer agent: Enabled: false rbac: create: true serviceAccount: create: false name: \u0026#34;jenkins\u0026#34; EOF Now we\u0026rsquo;ll use the helm cli to create the Jenkins server as we\u0026rsquo;ve declared it in the values.yaml file.\nhelm install cicd stable/jenkins -f values.yaml The output of this command will give you some additional information such as the admin password and the way to get the host name of the ELB that was provisioned.\nLet\u0026rsquo;s give this some time to provision and while we do let\u0026rsquo;s watch for pods to boot.\nkubectl get pods -w You should see the pods in init, pending or running state.\nOnce this changes to running we can get the load balancer address.\nexport SERVICE_IP=$(kubectl get svc --namespace default cicd-jenkins --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\u0026#34;) echo http://$SERVICE_IP/login  This service was configured with a LoadBalancer so, an AWS Elastic Load Balancer (ELB) is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n When the front-end service is first deployed, it can take up to several minutes for the ELB to be created and DNS updated. During this time the link above may display a \u0026ldquo;site unreachable\u0026rdquo; message. To check if the instances are in service, follow this deep link to the load balancer console. On the load balancer select the instances tab and ensure that the instance status is listed as \u0026ldquo;InService\u0026rdquo; before proceeding to the jenkins login page.\n "
},
{
	"uri": "/intermediate/230_logging/setup_es/",
	"title": "Provision an Elasticsearch Cluster",
	"tags": [],
	"description": "",
	"content": "This example creates an one instance Amazon Elasticsearch cluster named eksworkshop-logging. This cluster will be created in the same region as the EKS Kubernetes cluster.\nThe Elasticsearch cluster will have Fine-Grained Access Control enabled.\nFine-grained access control offers two forms of authentication and authorization:\n A built-in user database, which makes it easy to configure usernames and passwords inside of Elasticsearch. AWS Identity and Access Management (IAM) integration, which lets you map IAM principals to permissions.  We will create a public access domain with fine-grained access control enabled, an access policy that doesn\u0026rsquo;t use IAM principals, and a master user in the internal user database.\nFirst let\u0026rsquo;s create some variables\n# name of our elasticsearch cluster export ES_DOMAIN_NAME=\u0026#34;eksworkshop-logging\u0026#34; # Elasticsearch version export ES_VERSION=\u0026#34;7.4\u0026#34; # kibana admin user export ES_DOMAIN_USER=\u0026#34;eksworkshop\u0026#34; # kibana admin password export ES_DOMAIN_PASSWORD=\u0026#34;$(openssl rand -base64 12)_Ek1$\u0026#34; We are ready to create the Elasticsearch cluster\n# Download and update the template using the variables created previously curl -sS https://www.eksworkshop.com/intermediate/230_logging/deploy.files/es_domain.json \\  | envsubst \u0026gt; ~/environment/logging/es_domain.json # Create the cluster aws es create-elasticsearch-domain \\  --cli-input-json file://~/environment/logging/es_domain.json  It takes a little while for the cluster to be in an active state. The AWS Console should show the following status when the cluster is ready.\n You could also check this via AWS CLI\nif [ $(aws es describe-elasticsearch-domain --domain-name ${ES_DOMAIN_NAME} --query \u0026#39;DomainStatus.Processing\u0026#39;) == \u0026#34;false\u0026#34; ] then tput setaf 2; echo \u0026#34;The Elasticsearch cluster is ready\u0026#34; else tput setaf 1;echo \u0026#34;The Elasticsearch cluster is NOT ready\u0026#34; fi  It is important to wait for the cluster to be available before moving to the next section.\n "
},
{
	"uri": "/beginner/080_scaling/test_hpa/",
	"title": "Scale an Application with HPA",
	"tags": [],
	"description": "",
	"content": "Deploy a Sample App We will deploy an application and expose as a service on TCP port 80.\nThe application is a custom-built image based on the php-apache image. The index.php page performs calculations to generate CPU load. More information can be found here\nkubectl create deployment php-apache --image=us.gcr.io/k8s-artifacts-prod/hpa-example kubectl set resources deploy php-apache --requests=cpu=200m kubectl expose deploy php-apache --port 80 kubectl get pod -l app=php-apache Create an HPA resource This HPA scales up when CPU exceeds 50% of the allocated container resource.\nkubectl autoscale deployment php-apache `#The target average CPU utilization` \\ --cpu-percent=50 \\  --min=1 `#The lower limit for the number of pods that can be set by the autoscaler` \\ --max=10 `#The upper limit for the number of pods that can be set by the autoscaler` View the HPA using kubectl. You probably will see \u0026lt;unknown\u0026gt;/50% for 1-2 minutes and then you should be able to see 0%/50%\nkubectl get hpa Generate load to trigger scaling Open a new terminal in the Cloud9 Environment and run the following command to drop into a shell on a new container\nkubectl --generator=run-pod/v1 run -i --tty load-generator --image=busybox /bin/sh Execute a while loop to continue getting http:///php-apache\nwhile true; do wget -q -O - http://php-apache; done In the previous tab, watch the HPA with the following command\nkubectl get hpa -w You will see HPA scale the pods from 1 up to our configured maximum (10) until the CPU average is below our target (50%)\nYou can now stop (Ctrl + C) load test that was running in the other terminal. You will notice that HPA will slowly bring the replica count to min number based on its configuration. You should also get out of load testing application by pressing Ctrl + D.\n"
},
{
	"uri": "/020_prerequisites/aws_event/portal/",
	"title": "AWS Workshop Portal",
	"tags": [],
	"description": "",
	"content": "Login to AWS Workshop Portal This workshop creates an AWS account and a Cloud9 environment. You will need the Participant Hash provided upon entry, and your email address to track your unique session.\nConnect to the portal by clicking the button or browsing to https://dashboard.eventengine.run/. The following screen shows up.\nEnter the provided hash in the text box. The button on the bottom right corner changes to Accept Terms \u0026amp; Login. Click on that button to continue.\nClick on AWS Console on dashboard.\nTake the defaults and click on Open AWS Console. This will open AWS Console in a new browser tab.\nOnce you have completed the step above, you can head straight to Create a Workspace\n"
},
{
	"uri": "/intermediate/310_opa_gatekeeper/policy-example-1/",
	"title": "Build Policy using Constraint &amp; Constraint Template",
	"tags": [],
	"description": "",
	"content": "1. Build Constraint Templates ConstraintTemplate describes the Rego that enforces the constraint and the schema of the constraint. The schema constraint allows the author of the constraint (cluster admin) to define the contraint behavior.\nIn this example, the cluster admin will force the use of unprivileged containers in the cluseter. The OPA Gatekeeper will look for the securitycontext field and check if privileged=true. If it\u0026rsquo;s the case, then, the request will fail.\ncat \u0026gt; /tmp/constrainttemplate.yaml \u0026lt;\u0026lt;EOF apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8spspprivilegedcontainer spec: crd: spec: names: kind: K8sPSPPrivilegedContainer targets: - target: admission.k8s.gatekeeper.sh rego: | package k8spspprivileged violation[{\u0026#34;msg\u0026#34;: msg, \u0026#34;details\u0026#34;: {}}] { c := input_containers[_] c.securityContext.privileged msg := sprintf(\u0026#34;Privileged container is not allowed: %v, securityContext: %v\u0026#34;, [c.name, c.securityContext]) } input_containers[c] { c := input.review.object.spec.containers[_] } input_containers[c] { c := input.review.object.spec.initContainers[_] } EOF Create it\nkubectl create -f /tmp/constrainttemplate.yaml 2. Build Constraint The cluster admin will use the constraint to inform the OPA Gatekeeper to enforce the policy. For our example, as cluster admin we want to enforce that all the created pod should not be privileged.\ncat \u0026gt; /tmp/constraint.yaml \u0026lt;\u0026lt;EOF apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sPSPPrivilegedContainer metadata: name: psp-privileged-container spec: match: kinds: - apiGroups: [\u0026#34;\u0026#34;] kinds: [\u0026#34;Pod\u0026#34;] EOF Create it\nkubectl create -f /tmp/constraint.yaml 3. Test it First, check for the CRD constraint and constrainttemplate were created.\nkubectl get constraint kubectl get constrainttemplate Second, let\u0026rsquo;s try to deploy a privileged nginx pod:\ncat \u0026gt; /tmp/example.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: bad-nginx labels: app: bad-nginx spec: containers: - name: nginx image: nginx securityContext: privileged: true EOF kubectl create -f /tmp/example.yaml You should now see an error message similar to below:\nError from server ([denied by psp-privileged-container] Privileged container is not allowed: nginx, securityContext: {\u0026#34;privileged\u0026#34;: true}): error when creating \u0026#34;example.yaml\u0026#34;: admission webhook \u0026#34;validation.gatekeeper.sh\u0026#34; denied the request: [denied by psp-privileged-container] Privileged container is not allowed: nginx, securityContext: {\u0026#34;privileged\u0026#34;: true} The request was denied by kubernetes API, because it didn\u0026rsquo;t meet the requirement from the constraint forced by OPA Gatekeeper.\n"
},
{
	"uri": "/intermediate/290_argocd/configure/",
	"title": "Configure ArgoCD",
	"tags": [],
	"description": "",
	"content": "As the Argo CD has been deployed, we now need to configure argocd-server and then login:\nExpose argocd-server By default argocd-server is not publicaly exposed. For the purpose of this workshop, we will use a Load Balancer to make it usable:\nkubectl patch svc argocd-server -n argocd -p '{\u0026quot;spec\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;LoadBalancer\u0026quot;}}' Wait about 2 minutes for the LoadBalancer creation\nexport ARGOCD_SERVER=`kubectl get svc argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'` Login The initial password is autogenerated with the pod name of the ArgoCD API server:\nexport ARGO_PWD=`kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026quot;{.data.password}\u0026quot; | base64 -d` Using admin as login and the autogenerated password:\nargocd login $ARGOCD_SERVER --username admin --password $ARGO_PWD --insecure You should get as an output:\n'admin' logged in successfully "
},
{
	"uri": "/intermediate/260_weave_flux/codepipeline/",
	"title": "Create Image with CodePipeline",
	"tags": [],
	"description": "",
	"content": "Now we are going to create the AWS CodePipeline using AWS CloudFormation. This pipeline will be used to build a Docker image from your GitHub source repo (eks-example). Note that this does not deploy the image. Weave Flux will handle that.\nCloudFormation is an infrastructure as code (IaC) tool which provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts.\nEach EKS deployment/service should have its own CodePipeline and be located in an isolated source repository.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       CodePipeline \u0026amp; EKS  Launch    Download      After the console is open, enter your GitHub username, personal access token (created in previous step) and then click the \u0026ldquo;Create stack\u0026rdquo; button located at the bottom of the page.\nWait for the status to change from \u0026ldquo;CREATE_IN_PROGRESS\u0026rdquo; to CREATE_COMPLETE before moving on to the next step.\nOpen CodePipeline in the Management Console. You will see a CodePipeline that starts with image-codepipeline. Click this link to view the details.\nIf you receive a permissions error similar to User x is not authorized to perform: codepipeline:ListPipelines\u0026hellip; upon clicking the above link, the CodePipeline console may have opened up in the wrong region. To correct this, from the Region dropdown in the console, choose the region you provisioned the workshop in.\n Currently the image build is likely failed because we have no code in our repository. We will add a sample application to our GitHub repo (eks-example). Clone the repo substituting your GitHub user name.\ngit clone https://github.com/${YOURUSER}/eks-example.git cd eks-example Next create a base README file, a source directory, and download a sample nginx configuration (hello.conf), home page (index.html), and Dockerfile.\necho \u0026quot;# eks-example\u0026quot; \u0026gt; README.md mkdir src wget -O src/hello.conf https://raw.githubusercontent.com/aws-samples/eks-workshop/main/content/intermediate/260_weave_flux/app.files/hello.conf wget -O src/index.html https://raw.githubusercontent.com/aws-samples/eks-workshop/main/content/intermediate/260_weave_flux/app.files/index.html wget https://raw.githubusercontent.com/aws-samples/eks-workshop/main/content/intermediate/260_weave_flux/app.files/Dockerfile Now that we have a simple hello world app, commit the changes to start the image build pipeline.\ngit add . git commit -am \u0026quot;Initial commit\u0026quot; git push In the CodePipeline console go to the details page for the specific CodePipeline. You can see status along with links to the change and build details.\nIf you click on the \u0026ldquo;details\u0026rdquo; link in the build/deploy stage, you can see the output from the CodeBuild process.\n To verify the image is built, go to the Amazon ECR console and look for your eks-example image repository.\n"
},
{
	"uri": "/intermediate/200_migrate_to_eks/deploy-counter-app-kind/",
	"title": "Deploy counter app to kind",
	"tags": [],
	"description": "",
	"content": "Once the kind cluster is ready we can check it with\nkubectl --context kind-kind get nodes Deploy our postgres database to the cluster. First create a ConfigMap to initialize an empty database and then create a PersistentVolume on hostPath to store the data.\ncat \u0026lt;\u0026lt;EOF | kubectl --context kind-kind apply -f - --- apiVersion: v1 kind: ConfigMap metadata: name: postgres-config labels: app: postgres data: POSTGRES_PASSWORD: supersecret init: | CREATE TABLE importantdata ( id int4 PRIMARY KEY, count int4 NOT NULL ); INSERT INTO importantdata (id , count) VALUES (1, 0); --- kind: PersistentVolume apiVersion: v1 metadata: name: postgres-pv-volume labels: type: local app: postgres spec: storageClassName: manual capacity: storage: 5Gi accessModes: - ReadWriteMany hostPath: path: \u0026#34;/mnt/data\u0026#34; --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: postgres-pv-claim labels: app: postgres spec: storageClassName: manual accessModes: - ReadWriteMany resources: requests: storage: 5Gi EOF Now deploy a Postgres StatefulSet and service. You can see we mount the ConfigMap and PersistentVolumeClaim\ncat \u0026lt;\u0026lt;EOF | kubectl --context kind-kind apply -f - apiVersion: apps/v1 kind: StatefulSet metadata: name: postgres spec: replicas: 1 serviceName: postgres selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: terminationGracePeriodSeconds: 5 containers: - name: postgres image: postgres:13 imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; ports: - containerPort: 5432 envFrom: - configMapRef: name: postgres-config volumeMounts: - mountPath: /var/lib/postgresql/data name: postgredb - mountPath: /docker-entrypoint-initdb.d name: init resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; volumes: - name: postgredb persistentVolumeClaim: claimName: postgres-pv-claim - name: init configMap: name: postgres-config items: - key: init path: init.sql --- apiVersion: v1 kind: Service metadata: name: postgres labels: app: postgres spec: type: ClusterIP ports: - port: 5432 selector: app: postgres EOF Finally deploy the counter frontend and NodePort service\ncat \u0026lt;\u0026lt;EOF | kubectl --context kind-kind apply -f - --- apiVersion: apps/v1 kind: Deployment metadata: name: counter labels: app: counter spec: replicas: 2 selector: matchLabels: app: counter template: metadata: labels: app: counter spec: containers: - name: counter image: public.ecr.aws/aws-containers/stateful-counter:latest ports: - containerPort: 8000 resources: requests: memory: \u0026#34;16Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; --- apiVersion: v1 kind: Service metadata: name: counter-service spec: type: NodePort selector: app: counter ports: - port: 8000 name: http nodePort: 30000 EOF You can verify that your application and database is running with\nkubectl --context kind-kind get pods "
},
{
	"uri": "/intermediate/265_spinnaker_eks/install_operator/",
	"title": "Install Spinnaker Operator",
	"tags": [],
	"description": "",
	"content": "There are several methods to install open source Spinnaker on EKS/Kubernetes:\n Halyard Operator Kleat intended to replace Halyard (under active development)  In this workshop we will be using Spinnaker Operator, a Kubernetes Operator for managing Spinnaker, built by Armory. The Operator makes managing Spinnaker, which runs in Kubernetes, dramatically simpler and more automated, while introducing new Kubernetes-native features. The current tool (Halyard) involved significant manual processes and requires Spinnaker domain expertise.\nIn contrast, the Operator lets you treat Spinnaker as just another Kubernetes deployment, which makes installing and managing Spinnaker easy and reliable. The Operator unlocks the scalability of a GitOps workflow by defining Spinnaker configurations in a code repository rather than in hal commands.\nMore details on the benefits of Sipnnaker Operator can be found in Armory Docs\nPreRequisite   We assume that we have an existing EKS Cluster eksworkshop-eksctl created from EKS Workshop.\n  We also assume that we have increased the disk size on your Cloud9 instance as we need to build docker images for our application.\n  [Optional] If you want to use AWS Console to navigate and explore resources in Amazon EKS ensure that you have completed Console Credentials to get full access to the EKS Cluster in the EKS console.\n  We also have installed the prequisite for EKS Cluster installation based on the instructions here\n  And we have also validated the IAM role in use by the Cloud9 IDE based on the intructions here\n  Ensure you are getting the IAM role that you have attached to Cloud9 IDE when you execute the below command\n  aws sts get-caller-identity { \u0026#34;Account\u0026#34;: \u0026#34;XXXXXXX\u0026#34;, \u0026#34;UserId\u0026#34;: \u0026#34;YYYYYYYY:i-009b2d423b1386a74\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:sts::XXXXXXX:assumed-role/eksworkshop-admin-s/i-009b2d423b1386a74\u0026#34; }   Check if AWS_REGION and ACCOUNT_ID are set correctly  test -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set test -n \u0026#34;$ACCOUNT_ID\u0026#34; \u0026amp;\u0026amp; echo ACCOUNT_ID is \u0026#34;$ACCOUNT_ID\u0026#34; || echo ACCOUNT_ID is not set If not, export the ACCOUNT_ID and AWS_REGION to ENV export ACCOUNT_ID=\u0026lt;your_account_id\u0026gt; export AWS_REGION=\u0026lt;your_aws_region\u0026gt;  EKS cluster setup We need bigger instance type for installing spinnaker services, hence we are creating new EKS cluster. We are also deleting the existng nodegroup nodegroup that was created as part of cluster creation as we need Spinnaker Operator to create the services in the new nodegroup spinnaker\n cat \u0026lt;\u0026lt; EOF \u0026gt; spinnakerworkshop.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} # https://eksctl.io/usage/eks-managed-nodegroups/ managedNodeGroups: - name: spinnaker minSize: 2 maxSize: 3 desiredCapacity: 3 instanceType: m5.large ssh: enableSsm: true volumeSize: 20 labels: {role: spinnaker} tags: nodegroup-role: spinnaker EOF eksctl create nodegroup -f spinnakerworkshop.yaml eksctl delete nodegroup --cluster=eksworkshop-eksctl --name=nodegroup ..... ..... ..... 2021-05-20 16:45:15 [‚Ñπ] waiting for at least 2 node(s) to become ready in \u0026#34;spinnaker\u0026#34; 2021-05-20 16:45:15 [‚Ñπ] nodegroup \u0026#34;spinnaker\u0026#34; has 3 node(s) 2021-05-20 16:45:15 [‚Ñπ] node \u0026#34;ip-192-y-x-184.${AWS_REGION}.compute.internal\u0026#34; is ready 2021-05-20 16:45:15 [‚Ñπ] node \u0026#34;ip-192-y-x-128.${AWS_REGION}.compute.internal\u0026#34; is ready 2021-05-20 16:45:15 [‚Ñπ] node \u0026#34;ip-192-y-x-105.${AWS_REGION}.compute.internal\u0026#34; is ready 2021-05-20 16:45:15 [‚úî] created 1 managed nodegroup(s) in cluster \u0026#34;eksworkshop-eksctl\u0026#34; 2021-05-20 16:45:16 [‚Ñπ] checking security group configuration for all nodegroups 2021-05-20 16:45:16 [‚Ñπ] all nodegroups have up-to-date configuration  Confirm the setup\nkubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-y-x-184.${AWS_REGION}.compute.internal Ready \u0026lt;none\u0026gt; 4m45s v1.17.12-eks-7684af ip-192-y-x-128.${AWS_REGION}.compute.internal Ready \u0026lt;none\u0026gt; 4m37s v1.17.12-eks-7684af ip-192-y-x-105.${AWS_REGION}.compute.internal Ready \u0026lt;none\u0026gt; 4m47s v1.17.12-eks-7684af√•  Install Spinnaker CRDs Pick a release from https://github.com/armory/spinnaker-operator/releases and export that version. Below we are using the latest release of Spinnaker Operator when this workshop was written,\nexport VERSION=1.2.4 echo $VERSION mkdir -p spinnaker-operator \u0026amp;\u0026amp; cd spinnaker-operator bash -c \u0026quot;curl -L https://github.com/armory/spinnaker-operator/releases/download/v${VERSION}/manifests.tgz | tar -xz\u0026quot; kubectl apply -f deploy/crds/ 1.2.4 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 621 100 621 0 0 2700 0 --:--:-- --:--:-- --:--:-- 2688 100 11225 100 11225 0 0 29080 0 --:--:-- --:--:-- --:--:-- 29080 customresourcedefinition.apiextensions.k8s.io/spinnakeraccounts.spinnaker.io created customresourcedefinition.apiextensions.k8s.io/spinnakerservices.spinnaker.io created  Install Spinnaker Operator Install operator in namespace spinnaker-operator. We have used Cluster mode for the operator that works across namespaces and requires a ClusterRole to perform validation.\nkubectl create ns spinnaker-operator kubectl -n spinnaker-operator apply -f deploy/operator/cluster namespace/spinnaker-operator created deployment.apps/spinnaker-operator created clusterrole.rbac.authorization.k8s.io/spinnaker-operator-role created clusterrolebinding.rbac.authorization.k8s.io/spinnaker-operator-binding created serviceaccount/spinnaker-operator created  Make sure the Spinnaker-Operator pod is running This may take couple of minutes\n kubectl get pod -n spinnaker-operator NAME READY STATUS RESTARTS AGE spinnaker-operator-6d95f9b567-tcq4w 2/2 Running 0 82s  "
},
{
	"uri": "/intermediate/300_cis_eks_benchmark/run-as-job/",
	"title": "Module 2: Run kube-bench as a K8s job",
	"tags": [],
	"description": "",
	"content": "Create a job file Create a job file named job-eks.yaml using the command below.\ncat \u0026lt;\u0026lt; EOF \u0026gt; job-eks.yaml --- apiVersion: batch/v1 kind: Job metadata: name: kube-bench spec: template: spec: hostPID: true containers: - name: kube-bench image: aquasec/kube-bench:latest command: [\u0026#34;kube-bench\u0026#34;, \u0026#34;--benchmark\u0026#34;, \u0026#34;eks-1.0\u0026#34;] volumeMounts: - name: var-lib-kubelet mountPath: /var/lib/kubelet readOnly: true - name: etc-systemd mountPath: /etc/systemd readOnly: true - name: etc-kubernetes mountPath: /etc/kubernetes readOnly: true restartPolicy: Never volumes: - name: var-lib-kubelet hostPath: path: \u0026#34;/var/lib/kubelet\u0026#34; - name: etc-systemd hostPath: path: \u0026#34;/etc/systemd\u0026#34; - name: etc-kubernetes hostPath: path: \u0026#34;/etc/kubernetes\u0026#34; EOF Run the job on your cluster Run the kube-bench job on a pod in your cluster using the command below.\nkubectl apply -f job-eks.yaml View job assessment results Find the pod that was created. It should be in the default namespace.\nkubectl get pods --all-namespaces Retrieve the value of this pod and the output report. Note the pod name will be different for your environment.\nkubectl logs kube-bench-\u0026lt;value\u0026gt; Output [INFO] 3 Worker Node Security Configuration [INFO] 3.1 Worker Node Configuration Files [PASS] 3.1.1 Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Scored) [PASS] 3.1.2 Ensure that the proxy kubeconfig file ownership is set to root:root (Scored) [PASS] 3.1.3 Ensure that the kubelet configuration file has permissions set to 644 or more restrictive (Scored) [PASS] 3.1.4 Ensure that the kubelet configuration file ownership is set to root:root (Scored) [INFO] 3.2 Kubelet [PASS] 3.2.1 Ensure that the --anonymous-auth argument is set to false (Scored) [PASS] 3.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored) [PASS] 3.2.3 Ensure that the --client-ca-file argument is set as appropriate (Scored) [PASS] 3.2.4 Ensure that the --read-only-port argument is set to 0 (Scored) [PASS] 3.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Scored) [PASS] 3.2.6 Ensure that the --protect-kernel-defaults argument is set to true (Scored) [PASS] 3.2.7 Ensure that the --make-iptables-util-chains argument is set to true (Scored) [PASS] 3.2.8 Ensure that the --hostname-override argument is not set (Scored) [WARN] 3.2.9 Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture (Scored) [PASS] 3.2.10 Ensure that the --rotate-certificates argument is not set to false (Scored) [PASS] 3.2.11 Ensure that the RotateKubeletServerCertificate argument is set to true (Scored) == Remediations == 3.2.9 If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service == Summary == 14 checks PASS 0 checks FAIL 1 checks WARN 0 checks INFO Cleanup  Delete the resources  kubectl delete -f job-eks.yaml rm -f job-eks.yaml "
},
{
	"uri": "/020_prerequisites/",
	"title": "Start the workshop...",
	"tags": ["beginner", "kubeflow", "appmesh", "CON203", "CON205", "CON206", "OPN401"],
	"description": "",
	"content": "Getting Started   To start the workshop, follow one of the following depending on whether you are\u0026hellip;\n \u0026hellip;running the workshop on your own (in your own account), or \u0026hellip;attending an AWS hosted event (using AWS provided hashes)  Once you have completed with either setup, continue with Create a Workspace\n"
},
{
	"uri": "/intermediate/241_pixie/using_pixie/",
	"title": "Using Pixie",
	"tags": [],
	"description": "",
	"content": "The Pixie platform uses PxL scripts to both:\n Query telemetry data collected by the Pixie Platform (DNS events, HTTP events, etc). Extend Pixie to collect new data sources (in addition to those collected by default).  This tutorial will demonstrate the use of PxL scripts to query telemetry data automatically collected by Pixie.\nPixie comes with many pre-built PxL scripts. These scripts are executed by the Pixie platform using the web-based Live UI, CLI or API.\nThis tutorial will demonstrate using the Live UI.\n"
},
{
	"uri": "/beginner/091_iam-groups/create-iam-groups/",
	"title": "Create IAM Groups",
	"tags": [],
	"description": "",
	"content": "We want to have different IAM users which will be added to specific IAM groups in order to have different rights in the kubernetes cluster.\nWe will define 3 groups:\n k8sAdmin - users from this group will have admin rights on the kubernetes cluster k8sDev - users from this group will have full access only in the development namespace of the cluster k8sInteg - users from this group will have access to integration namespace.   In fact, users from k8sDev and k8sInteg groups will only have access to namespaces where we will define kubernetes RBAC access for their associated kubernetes role. We\u0026rsquo;ll see this but first, let\u0026rsquo;s creates the groups.\n Create k8sAdmin IAM Group The k8sAdmin Group will be allowed to assume the k8sAdmin IAM Role.\naws iam create-group --group-name k8sAdmin Let\u0026rsquo;s add a Policy on our group which will allow users from this group to assume our k8sAdmin Role:\nADMIN_GROUP_POLICY=$(echo -n \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAssumeOrganizationAccountRole\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::\u0026#39;; echo -n \u0026#34;$ACCOUNT_ID\u0026#34;; echo -n \u0026#39;:role/k8sAdmin\u0026#34; } ] }\u0026#39;) echo ADMIN_GROUP_POLICY=$ADMIN_GROUP_POLICY aws iam put-group-policy \\ --group-name k8sAdmin \\ --policy-name k8sAdmin-policy \\ --policy-document \u0026#34;$ADMIN_GROUP_POLICY\u0026#34; Create k8sDev IAM Group The k8sDev Group will be allowed to assume the k8sDev IAM Role.\naws iam create-group --group-name k8sDev Let\u0026rsquo;s add a Policy on our group which will allow users from this group to assume our k8sDev Role:\nDEV_GROUP_POLICY=$(echo -n \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAssumeOrganizationAccountRole\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::\u0026#39;; echo -n \u0026#34;$ACCOUNT_ID\u0026#34;; echo -n \u0026#39;:role/k8sDev\u0026#34; } ] }\u0026#39;) echo DEV_GROUP_POLICY=$DEV_GROUP_POLICY aws iam put-group-policy \\ --group-name k8sDev \\ --policy-name k8sDev-policy \\ --policy-document \u0026#34;$DEV_GROUP_POLICY\u0026#34; Create k8sInteg IAM Group aws iam create-group --group-name k8sInteg Let\u0026rsquo;s add a Policy on our group which will allow users from this group to assume our k8sInteg Role:\nINTEG_GROUP_POLICY=$(echo -n \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAssumeOrganizationAccountRole\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::\u0026#39;; echo -n \u0026#34;$ACCOUNT_ID\u0026#34;; echo -n \u0026#39;:role/k8sInteg\u0026#34; } ] }\u0026#39;) echo INTEG_GROUP_POLICY=$INTEG_GROUP_POLICY aws iam put-group-policy \\ --group-name k8sInteg \\ --policy-name k8sInteg-policy \\ --policy-document \u0026#34;$INTEG_GROUP_POLICY\u0026#34; You now should have your 3 groups\naws iam list-groups { \u0026#34;Groups\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;GroupName\u0026#34;: \u0026#34;k8sAdmin\u0026#34;, \u0026#34;GroupId\u0026#34;: \u0026#34;AGPAZRV3OHPJZGT2JKVDV\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::xxxxxxxxxx:group/k8sAdmin\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2020-04-07T13:32:52Z\u0026#34; }, { \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;GroupName\u0026#34;: \u0026#34;k8sDev\u0026#34;, \u0026#34;GroupId\u0026#34;: \u0026#34;AGPAZRV3OHPJUOBR375KI\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::xxxxxxxxxx:group/k8sDev\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2020-04-07T13:33:15Z\u0026#34; }, { \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;GroupName\u0026#34;: \u0026#34;k8sInteg\u0026#34;, \u0026#34;GroupId\u0026#34;: \u0026#34;AGPAZRV3OHPJR6GM6PFDG\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::xxxxxxxxxx:group/k8sInteg\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2020-04-07T13:33:25Z\u0026#34; } ] }  "
},
{
	"uri": "/intermediate/241_pixie/using_pixie/run_script/",
	"title": "Run a PxL script",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s use Pixie\u0026rsquo;s Live UI to run a built-in PxL script.\nRun a PxL Script  Open Pixie‚Äôs Live UI. Select your cluster using the cluster drop-down menu.  Select the px/namespace script from the script drop-down menu.  This script has one required argument (as denoted with an *). Enter px-sock-shop for the namespace argument.  After the script executes, you should see a view similar to the following:\nThis script (px/namespace) shows:\n A service graph of HTTP1/2 traffic between the pods in the specified namespace. A list of the services and pods with high-level resource and application metrics.  This script takes one other argument, start_time. This argument is specified as a relative time (e.g. -30s, -30m, -1hr) and provides the time window for the query.\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/add_nodegroup_fargate/create_fargate/",
	"title": "Add Fargate Profile",
	"tags": [],
	"description": "",
	"content": "Create IAM OIDC provider First, we will have to set up an OIDC provider with the cluster. The IAM OIDC Provider is not enabled by default, you can use the following command to enable it.\neksctl utils associate-iam-oidc-provider \\  --region ${AWS_REGION} \\  --cluster eksworkshop-eksctl \\  --approve 2021-05-02 17:37:05 [‚Ñπ] eksctl version 0.45.0 2021-05-02 17:37:05 [‚Ñπ] using region $AWS_REGION 2021-05-02 17:37:05 [‚Ñπ] will create IAM Open ID Connect provider for cluster \u0026#34;eksworkshop-eksctl\u0026#34; in \u0026#34;$AWS_REGION\u0026#34; 2021-05-02 17:37:05 [‚úî] created IAM Open ID Connect provider for cluster \u0026#34;eksworkshop-eksctl\u0026#34; in \u0026#34;$AWS_REGION\u0026#34;  Create the Namespace and IAM Role and ServiceAccount We will create the prodcatalog-ns and the IRSA for this namespace which will give permission to X-Ray, AppMesh and Cloudwatch Logs policies.\nkubectl create namespace prodcatalog-ns cd eks-app-mesh-polyglot-demo aws iam create-policy \\  --policy-name ProdEnvoyNamespaceIAMPolicy \\  --policy-document file://deployment/envoy-iam-policy.json eksctl create iamserviceaccount --cluster eksworkshop-eksctl \\  --namespace prodcatalog-ns \\  --name prodcatalog-envoy-proxies \\  --attach-policy-arn arn:aws:iam::$ACCOUNT_ID:policy/ProdEnvoyNamespaceIAMPolicy \\  --override-existing-serviceaccounts \\  --approve namespace/prodcatalog-ns created { \u0026#34;Policy\u0026#34;: { \u0026#34;PolicyName\u0026#34;: \u0026#34;ProdEnvoyNamespaceIAMPolicy\u0026#34;, \u0026#34;PermissionsBoundaryUsageCount\u0026#34;: 0, \u0026#34;CreateDate\u0026#34;: \u0026#34;2021-05-02T17:41:59Z\u0026#34;, \u0026#34;AttachmentCount\u0026#34;: 0, \u0026#34;IsAttachable\u0026#34;: true, \u0026#34;PolicyId\u0026#34;: \u0026#34;ANPAV45SCB72ZHQFMQFXR\u0026#34;, \u0026#34;DefaultVersionId\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::$ACCOUNT_ID:policy/ProdEnvoyNamespaceIAMPolicy\u0026#34;, \u0026#34;UpdateDate\u0026#34;: \u0026#34;2021-05-02T17:41:59Z\u0026#34; } } 2021-05-02 17:42:00 [‚Ñπ] eksctl version 0.45.0 2021-05-02 17:42:00 [‚Ñπ] using region $AWS_REGION 2021-05-02 17:42:00 [‚Ñπ] 1 iamserviceaccount (prodcatalog-ns/prodcatalog-envoy-proxies) was included (based on the include/exclude rules) 2021-05-02 17:42:00 [!] metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set 2021-05-02 17:42:00 [‚Ñπ] 1 task: { 2 sequential sub-tasks: { create IAM role for serviceaccount \u0026#34;prodcatalog-ns/prodcatalog-envoy-proxies\u0026#34;, create serviceaccount \u0026#34;prodcatalog-ns/prodcatalog-envoy-proxies\u0026#34; } } 2021-05-02 17:42:00 [‚Ñπ] building iamserviceaccount stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-prodcatalog-ns-prodcatalog-envoy-proxies\u0026#34; 2021-05-02 17:42:00 [‚Ñπ] deploying stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-prodcatalog-ns-prodcatalog-envoy-proxies\u0026#34; 2021-05-02 17:42:00 [‚Ñπ] waiting for CloudFormation stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-prodcatalog-ns-prodcatalog-envoy-proxies\u0026#34; 2021-05-02 17:42:17 [‚Ñπ] waiting for CloudFormation stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-prodcatalog-ns-prodcatalog-envoy-proxies\u0026#34; 2021-05-02 17:42:34 [‚Ñπ] waiting for CloudFormation stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-prodcatalog-ns-prodcatalog-envoy-proxies\u0026#34; 2021-05-02 17:42:34 [‚Ñπ] created serviceaccount \u0026#34;prodcatalog-ns/prodcatalog-envoy-proxies\u0026#34;  The IAM role gets associated with a Kubernetes Service Account. You can see details of the service account created with the following command.\nkubectl describe sa prodcatalog-envoy-proxies -n prodcatalog-ns Name: prodcatalog-envoy-proxies Namespace: prodcatalog-ns Labels: app.kubernetes.io/managed-by=eksctl Annotations: eks.amazonaws.com/role-arn: arn:aws:iam::$ACCOUNT_ID:role/eksctl-eksworkshop-eksctl-addon-iamserviceac-Role1-1PWNQ4AJFMVBF Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: prodcatalog-envoy-proxies-token-69pql Tokens: prodcatalog-envoy-proxies-token-69pql Events: \u0026lt;none\u0026gt;  Learn more about IAM Roles for Service Accounts in the Amazon EKS documentation.\n Create a Fargate profile The Fargate profile allows an administrator to declare which pods run on Fargate. Each profile can have up to five selectors that contain a namespace and optional labels. You must define a namespace for every selector. The label field consists of multiple optional key-value pairs. Pods that match a selector (by matching a namespace for the selector and all of the labels specified in the selector) are scheduled on Fargate.\nIt is generally a good practice to deploy user application workloads into namespaces other than kube-system or default so that you have more fine-grained capabilities to manage the interaction between your pods deployed on to EKS. In this chapter we will create a new Fargate profile named fargate-productcatalog that targets all pods destined for the prodcatalog-ns namespace with label app: prodcatalog. You can see the Fargate Profile configuration below from clusterconfig.yaml.\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} .... .... fargateProfiles: - name: fargate-productcatalog selectors: - namespace: prodcatalog-ns labels: app: prodcatalog  Run the below command to create the Fargate Profile\ncd eks-app-mesh-polyglot-demo envsubst \u0026lt; ./deployment/clusterconfig.yaml | eksctl create fargateprofile -f - 2021-05-02 17:44:51 [‚Ñπ] eksctl version 0.45.0 2021-05-02 17:44:51 [‚Ñπ] using region $AWS_REGION 2021-05-02 17:44:51 [‚Ñπ] deploying stack \u0026#34;eksctl-eksworkshop-eksctl-fargate\u0026#34; 2021-05-02 17:44:51 [‚Ñπ] waiting for CloudFormation stack \u0026#34;eksctl-eksworkshop-eksctl-fargate\u0026#34; 2021-05-02 17:45:08 [‚Ñπ] waiting for CloudFormation stack \u0026#34;eksctl-eksworkshop-eksctl-fargate\u0026#34; 2021-05-02 17:45:24 [‚Ñπ] waiting for CloudFormation stack \u0026#34;eksctl-eksworkshop-eksctl-fargate\u0026#34; 2021-05-02 17:45:25 [‚Ñπ] creating Fargate profile \u0026#34;fargate-productcatalog\u0026#34; on EKS cluster \u0026#34;eksworkshop-eksctl\u0026#34; 2021-05-02 17:49:43 [‚Ñπ] created Fargate profile \u0026#34;fargate-productcatalog\u0026#34; on EKS cluster \u0026#34;eksworkshop-eksctl\u0026#34;  When your EKS cluster schedules pods on Fargate, the pods will need to make calls to AWS APIs on your behalf to do things like pull container images from Amazon ECR. The Fargate Pod Execution Role provides the IAM permissions to do this. This IAM role is automatically created for you by the above command.\nCreation of a Fargate profile can take up to several minutes. Execute the following command after the profile creation is completed and you should see output similar to what is shown below.\nThe creation of the Fargate Profile will take about 5 - 7 minutes.\n eksctl get fargateprofile --cluster eksworkshop-eksctl -o yaml - name: fargate-productcatalog podExecutionRoleARN: arn:aws:iam::$ACCOUNT_ID:role/eksctl-eksworkshop-eksctl-FargatePodExecutionRole-JPQ56PZ7R9NL selectors: - labels: app: prodcatalog namespace: prodcatalog-ns status: ACTIVE subnets: - subnet-084a7XXXXXXXXXXXX - subnet-0b2dXXXXXXXXXXXXX - subnet-08565XXXXXXXXXXXX  Log into console and navigate to Amazon EKS -\u0026gt; Cluster -\u0026gt; Click eksworkshop-eksctl -\u0026gt; Configuration -\u0026gt; Compute, you should see the new Fargate Profile fargate-productcatalog you created: Notice that the profile includes the private subnets in your EKS cluster. Pods running on Fargate are not assigned public IP addresses, so only private subnets (with no direct route to an Internet Gateway) are supported when you create a Fargate profile. Hence, while provisioning an EKS cluster, you must make sure that the VPC that you create contains one or more private subnets. When you create an EKS cluster with eksctl utility, under the hoods it creates a VPC that meets these requirements.\n"
},
{
	"uri": "/beginner/091_iam-groups/create_iam_users/",
	"title": "Create IAM Users",
	"tags": [],
	"description": "",
	"content": "In order to test our scenarios, we will create 3 users, one for each groups we created :\naws iam create-user --user-name PaulAdmin aws iam create-user --user-name JeanDev aws iam create-user --user-name PierreInteg Add users to associated groups:\naws iam add-user-to-group --group-name k8sAdmin --user-name PaulAdmin aws iam add-user-to-group --group-name k8sDev --user-name JeanDev aws iam add-user-to-group --group-name k8sInteg --user-name PierreInteg Check users are correctly added in their groups:\naws iam get-group --group-name k8sAdmin aws iam get-group --group-name k8sDev aws iam get-group --group-name k8sInteg  For the sake of simplicity, in this chapter, we will save credentials to a file to make it easy to toggle back and forth between users. Never do this in production or with credentials that have priviledged access; It is not a security best practice to store credentials on the filesystem.\n Retrieve Access Keys for our fake users:\naws iam create-access-key --user-name PaulAdmin | tee /tmp/PaulAdmin.json aws iam create-access-key --user-name JeanDev | tee /tmp/JeanDev.json aws iam create-access-key --user-name PierreInteg | tee /tmp/PierreInteg.json Recap:\n PaulAdmin is in the k8sAdmin group and will be able to assume the k8sAdmin role. JeanDev is in k8sDev Group and will be able to assume IAM role k8sDev PierreInteg is in k8sInteg group and will be able to assume IAM role k8sInteg  "
},
{
	"uri": "/intermediate/241_pixie/using_pixie/service_errors/",
	"title": "Observe Service Errors",
	"tags": [],
	"description": "",
	"content": "Let‚Äôs see if we can figure out what is causing the application filtering bug we saw earlier in this tutorial.\nGiven that the bug exists on the ‚ÄúCatalogue‚Äù page, there‚Äôs a good chance that page involves the catalogue service.\nScroll down the page to the ‚ÄúService List‚Äù table and select the px-sock-shop/catalogue service.\nDeep links embedded in script views allow you to easily navigate between scripts. Selecting any service name deep link will navigate you to the px/service script, which shows us the request statistics for the selected service.\nThe px/service script view doesn\u0026rsquo;t show any errors, but that might be because the time window isn‚Äôt large enough.\nChange the start_time argument to be -30m (note the negative symbol) to make sure that the window is wide enough to include when you triggered the bug in the Sock Shop app. Once the script re-runs, you should one or more HTTP errors for the service:\nIf you don‚Äôt see an HTTP error, increase the time window or manually trigger the bug again in the Sock Shop web application. Make sure to re-run the script afterwards.\n "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/add_nodegroup_fargate/cloudwatch_setup/",
	"title": "Observability Setup",
	"tags": [],
	"description": "",
	"content": "Enable Amazon Cloudwatch Container Insights Create an IAM role for the cloudwatch-agent service account\neksctl create iamserviceaccount \\  --cluster eksworkshop-eksctl \\  --namespace amazon-cloudwatch \\  --name cloudwatch-agent \\  --attach-policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \\  --override-existing-serviceaccounts \\  --approve 2021-05-02 17:52:36 [‚Ñπ] eksctl version 0.45.0 2021-05-02 17:52:36 [‚Ñπ] using region $AWS_REGION 2021-05-02 17:52:36 [‚Ñπ] 1 existing iamserviceaccount(s) (prodcatalog-ns/prodcatalog-envoy-proxies) will be excluded 2021-05-02 17:52:36 [‚Ñπ] 1 iamserviceaccount (amazon-cloudwatch/cloudwatch-agent) was included (based on the include/exclude rules) 2021-05-02 17:52:36 [!] metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set 2021-05-02 17:52:36 [‚Ñπ] 1 task: { 2 sequential sub-tasks: { create IAM role for serviceaccount \u0026#34;amazon-cloudwatch/cloudwatch-agent\u0026#34;, create serviceaccount \u0026#34;amazon-cloudwatch/cloudwatch-agent\u0026#34; } } 2021-05-02 17:52:36 [‚Ñπ] building iamserviceaccount stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-amazon-cloudwatch-cloudwatch-agent\u0026#34; 2021-05-02 17:52:37 [‚Ñπ] deploying stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-amazon-cloudwatch-cloudwatch-agent\u0026#34; 2021-05-02 17:52:37 [‚Ñπ] waiting for CloudFormation stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-amazon-cloudwatch-cloudwatch-agent\u0026#34; 2021-05-02 17:53:10 [‚Ñπ] created namespace \u0026#34;amazon-cloudwatch\u0026#34; 2021-05-02 17:53:10 [‚Ñπ] created serviceaccount \u0026#34;amazon-cloudwatch/cloudwatch-agent\u0026#34;  Create an IAM role for the fluent service account\neksctl create iamserviceaccount \\  --cluster eksworkshop-eksctl \\  --namespace amazon-cloudwatch \\  --name fluentd \\  --attach-policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \\  --override-existing-serviceaccounts \\  --approve 2021-05-02 17:54:09 [‚Ñπ] eksctl version 0.45.0 2021-05-02 17:54:09 [‚Ñπ] using region $AWS_REGION 2021-05-02 17:54:10 [‚Ñπ] 2 existing iamserviceaccount(s) (amazon-cloudwatch/cloudwatch-agent,prodcatalog-ns/prodcatalog-envoy-proxies) will be excluded 2021-05-02 17:54:10 [‚Ñπ] 1 iamserviceaccount (amazon-cloudwatch/fluentd) was included (based on the include/exclude rules) 2021-05-02 17:54:10 [!] metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set 2021-05-02 17:54:10 [‚Ñπ] 1 task: { 2 sequential sub-tasks: { create IAM role for serviceaccount \u0026#34;amazon-cloudwatch/fluentd\u0026#34;, create serviceaccount \u0026#34;amazon-cloudwatch/fluentd\u0026#34; } } 2021-05-02 17:54:10 [‚Ñπ] building iamserviceaccount stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-amazon-cloudwatch-fluentd\u0026#34; 2021-05-02 17:54:10 [‚Ñπ] deploying stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-amazon-cloudwatch-fluentd\u0026#34; 2021-05-02 17:54:10 [‚Ñπ] waiting for CloudFormation stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-amazon-cloudwatch-fluentd\u0026#34; 2021-05-02 17:54:44 [‚Ñπ] created serviceaccount \u0026#34;amazon-cloudwatch/fluentd\u0026#34;  Now, Deploy Container Insights for Managed Nodegroup\ncurl -s https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed \u0026#34;s/{{cluster_name}}/eksworkshop-eksctl/;s/{{region_name}}/${AWS_REGION}/\u0026#34; | kubectl apply -f - % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 15552 100 15552 0 0 45840 0 --:--:-- --:--:-- --:--:-- 45876 namespace/amazon-cloudwatch configured serviceaccount/cloudwatch-agent configured clusterrole.rbac.authorization.k8s.io/cloudwatch-agent-role created clusterrolebinding.rbac.authorization.k8s.io/cloudwatch-agent-role-binding created configmap/cwagentconfig created daemonset.apps/cloudwatch-agent created configmap/cluster-info created serviceaccount/fluentd configured clusterrole.rbac.authorization.k8s.io/fluentd-role created clusterrolebinding.rbac.authorization.k8s.io/fluentd-role-binding created configmap/fluentd-config created  The command above will:\n Create the Namespace amazon-cloudwatch. Create all the necessary security objects for both DaemonSet:  SecurityAccount. ClusterRole. ClusterRoleBinding.   Deploy Cloudwatch-Agent (responsible for sending the metrics to CloudWatch) as a DaemonSet. Deploy fluentd (responsible for sending the logs to Cloudwatch) as a DaemonSet. Deploy ConfigMap configurations for both DaemonSets.  You can find the full information and manual install steps here.\n You can verify all the DaemonSets have been deployed by running the following command.\nkubectl -n amazon-cloudwatch get daemonsets NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE cloudwatch-agent 3 3 3 3 3 \u0026lt;none\u0026gt; 2m43s fluentd-cloudwatch 3 3 3 3 3 \u0026lt;none\u0026gt; 2m43s  You can also verify the deployment of DaemonSets by logging into console and navigate to Amazon EKS -\u0026gt; Cluster -\u0026gt; Workloads, Enable Prometheus Metrics in CloudWatch Create an IAM role for the prometheus service account\neksctl create iamserviceaccount \\  --cluster eksworkshop-eksctl \\  --namespace amazon-cloudwatch \\  --name cwagent-prometheus \\  --attach-policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \\  --override-existing-serviceaccounts \\  --approve 2021-05-02 18:00:25 [‚Ñπ] eksctl version 0.45.0 2021-05-02 18:00:25 [‚Ñπ] using region $AWS_REGION 2021-05-02 18:00:26 [‚Ñπ] 3 existing iamserviceaccount(s) (amazon-cloudwatch/cloudwatch-agent,amazon-cloudwatch/fluentd,prodcatalog-ns/prodcatalog-envoy-proxies) will be excluded 2021-05-02 18:00:26 [‚Ñπ] 1 iamserviceaccount (amazon-cloudwatch/cwagent-prometheus) was included (based on the include/exclude rules) 2021-05-02 18:00:26 [!] metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set 2021-05-02 18:00:26 [‚Ñπ] 1 task: { 2 sequential sub-tasks: { create IAM role for serviceaccount \u0026#34;amazon-cloudwatch/cwagent-prometheus\u0026#34;, create serviceaccount \u0026#34;amazon-cloudwatch/cwagent-prometheus\u0026#34; } } 2021-05-02 18:00:26 [‚Ñπ] building iamserviceaccount stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-amazon-cloudwatch-cwagent-prometheus\u0026#34; 2021-05-02 18:00:26 [‚Ñπ] deploying stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-amazon-cloudwatch-cwagent-prometheus\u0026#34; 2021-05-02 18:00:26 [‚Ñπ] waiting for CloudFormation stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-amazon-cloudwatch-cwagent-prometheus\u0026#34; 2021-05-02 18:00:59 [‚Ñπ] created serviceaccount \u0026#34;amazon-cloudwatch/cwagent-prometheus\u0026#34;  Install Prometheus Agent\nkubectl apply -f https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/service/cwagent-prometheus/prometheus-eks.yaml namespace/amazon-cloudwatch unchanged configmap/prometheus-cwagentconfig created configmap/prometheus-config created serviceaccount/cwagent-prometheus configured clusterrole.rbac.authorization.k8s.io/cwagent-prometheus-role created clusterrolebinding.rbac.authorization.k8s.io/cwagent-prometheus-role-binding created deployment.apps/cwagent-prometheus created  Confirm that the agent is running\nkubectl get pod -l \u0026#34;app=cwagent-prometheus\u0026#34; -n amazon-cloudwatch NAME READY STATUS RESTARTS AGE cwagent-prometheus-95896694d-99pwb 1/1 Running 0 2m33s  Enable Logging for Fargate Amazon EKS with Fargate supports a built-in log router, which means there are no sidecar containers to install or maintain. Apply a ConfigMap to your Amazon EKS cluster with a Fluent Conf data value that defines where container logs are shipped to. This logging ConfigMap has to be used in a fixed namespace called aws-observability has a cluster-wide effect, meaning that you can send application-level logs from any application in any namespace.\nIn this workshop, we will show you how to use cloudwatch_logs to send logs from a workload running in an EKS on Fargate cluster to CloudWatch.\nFirst, create the dedicated aws-observability namespace and the ConfigMap for Fluent Bit\ncd eks-app-mesh-polyglot-demo envsubst \u0026lt; ./deployment/fluentbit-config.yaml | kubectl apply -f - namespace/aws-observability created configmap/aws-logging created  Next, verify if the Fluent Bit ConfigMap is deployed correctly\nkubectl -n aws-observability get cm NAME DATA AGE aws-logging 1 18s  With Fluent Bit set up we next need to give it the permission to write to CloudWatch. We do that by first downloading the policy locally:\ncurl -o permissions.json \\  https://raw.githubusercontent.com/aws-samples/amazon-eks-fluent-logging-examples/mainline/examples/fargate/cloudwatchlogs/permissions.json % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 215 100 215 0 0 1023 0 --:--:-- --:--:-- --:--:-- 1023  And next we create the policy:\naws iam create-policy \\  --policy-name FluentBitEKSFargate \\  --policy-document file://permissions.json { \u0026#34;Policy\u0026#34;: { \u0026#34;PolicyName\u0026#34;: \u0026#34;FluentBitEKSFargate\u0026#34;, \u0026#34;PermissionsBoundaryUsageCount\u0026#34;: 0, \u0026#34;CreateDate\u0026#34;: \u0026#34;2021-02-04T07:11:07Z\u0026#34;, \u0026#34;AttachmentCount\u0026#34;: 0, \u0026#34;IsAttachable\u0026#34;: true, \u0026#34;PolicyId\u0026#34;: \u0026#34;ANPAV45SCB72QX3SZN2RS\u0026#34;, \u0026#34;DefaultVersionId\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::405710966773:policy/FluentBitEKSFargate\u0026#34;, \u0026#34;UpdateDate\u0026#34;: \u0026#34;2021-02-04T07:11:07Z\u0026#34; } }  And then, Attach the Policy to the Pod Execution Role of our EKS on Fargate cluster\nexport PodRole=$(aws eks describe-fargate-profile --cluster-name eksworkshop-eksctl --fargate-profile-name fargate-productcatalog --query \u0026#39;fargateProfile.podExecutionRoleArn\u0026#39; | sed -n \u0026#39;s/^.*role\\/\\(.*\\)\u0026#34;.*$/\\1/ p\u0026#39;) aws iam attach-role-policy \\  --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/FluentBitEKSFargate \\  --role-name ${PodRole} echo $PodRole Log into console and navigate to EKS -\u0026gt; Cluster -\u0026gt; Configuration-\u0026gt; Compute, select fargate-productcatalog Fargate Profile, you will see the below page. Click on the Pod Execution Role. You should see the below FluentBitEKSFargate policy that was attached to the Pod Execution Role. (Optional) Enable Amazon EKS Control Plane logs If you enable Amazon EKS Control Plane logging, you will be charged the standard CloudWatch Logs data ingestion and storage costs for any logs sent to CloudWatch Logs from your cluster. You are also charged for any AWS resources, such as Amazon EC2 instances or Amazon EBS volumes, that you provision as part of your cluster.\n CloudWatch logging for EKS control plane is not enabled by default due to data ingestion and storage costs. You can enable using below command.\neksctl utils update-cluster-logging \\  --enable-types all \\  --region ${AWS_REGION} \\  --cluster eksworkshop-eksctl \\  --approve [‚Ñπ] eksctl version 0.37.0 [‚Ñπ] using region us-west-2 [‚úî] CloudWatch logging for cluster \u0026#34;eksworkshop-eksctl\u0026#34; in \u0026#34;us-west-2\u0026#34; is already up-to-date  You can log into console and navigate to Amazon EKS -\u0026gt; Cluster -\u0026gt; Logging Congratulations!! You have completed the basic setup for EKS and Observability, Now lets move to the fun part of deploying our Product Catalog Application.\n"
},
{
	"uri": "/020_prerequisites/clone/",
	"title": "Clone the Service Repos",
	"tags": [],
	"description": "",
	"content": "cd ~/environment git clone https://github.com/aws-containers/ecsdemo-frontend.git git clone https://github.com/brentley/ecsdemo-nodejs.git git clone https://github.com/brentley/ecsdemo-crystal.git "
},
{
	"uri": "/intermediate/241_pixie/using_pixie/http_data/",
	"title": "Inspect the HTTP Request",
	"tags": [],
	"description": "",
	"content": "Pixie automatically traces the full request/response bodies of application w for the following supported protocols. Let‚Äôs use Pixie to see the contents of this failed HTTP request.\nInspect the HTTP Request Select the px/http_data script from the script drop-down menu. Change the start_time to -30m or any window that will include when you triggered the bug in the Sock Shop app.\nThis script shows the traced HTTP1/2 requests made within your cluster, as long as the request either originates or is received inside of your cluster. There is quite a bit of HTTP traffic in this cluster, so let‚Äôs filter it to only show errors.\nOpen the script editor using ctrl+e (Windows, Linux) or cmd+e (Mac). Or by clicking on the edit script button circled in red below.\nDelete line 29 and replace it with the following.\n# Access the service name. df.service = df.ctx[\u0026#39;service\u0026#39;] # Filter to only catalogue service. df = df[df.service == \u0026#39;px-sock-shop/catalogue\u0026#39;] # Filter to errors greater or equal to 400. df = df[df.resp_status \u0026gt;= 400]  Make sure that the new lines just added match the indentation of the existing lines. If not, you will get an invalid syntax error when running the script.\n This code accesses the service of the pod receiving HTTP request. Then, it filters the requests to only include those made by the catalogue service that have an error code.\nPixie specifies services and pods with the namespace prepended. For example, the catalogue service in the px-sock-shop namespace looks like px-sock-shop/catalogue.\n Re-run the script with the RUN button (top right of the page), or using the keyboard shortcut: ctrl+enter (Windows, Linux) or cmd+enter (Mac).\nHide the script editor using the same command to show it: ctrl+e (Windows, Linux) or cmd+e (Mac).\nThe output shows the request with the error you recently triggered. Click on a row to inspect the row data in json format. Scroll down to the resp_body json key and you can see that our error is a database connection error.\n"
},
{
	"uri": "/intermediate/241_pixie/using_pixie/mysql_data/",
	"title": "Inspect the MySQL Request",
	"tags": [],
	"description": "",
	"content": "From the demo app‚Äôs YAML file, we know that the catalog service talks to a MySQL database. Let‚Äôs inspect the catalog service\u0026rsquo;s mysql requests to see if we can get more information about the type of database connection error.\nSelect the px/mysql_data script from the script drop-down menu. This script shows all of the mysql requests Pixie has traced in the cluster. Let‚Äôs filter these requests.\nModify the script‚Äôs start_time to -30m or any window that will include when you triggered the bug in the Sock Shop app.\nOpen the script editor using ctrl+e (Windows, Linux) or cmd+e (Mac).\nOn line 34, add the following line to filter the mysql requests to just those with errors:\n# Filter requests to only include those with an error code. df = df[df.resp_status == 3]  Make sure that the new lines just added match the indentation of the existing lines. If not, you will get an invalid syntax error when running the script.\n Re-run the script with the RUN button (top right of the page), or using the keyboard shortcut: ctrl+enter (Windows, Linux) or cmd+enter (Mac).\nThe output should show one or more requests with errors.\nClick on the table row to see the row data in json format.\nScroll down to the resp_body json key, and you will see that our error is a SQL syntax error. In particular the OR condition was misspelled as ORR.\nYou have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near \u0026#39;ORR tag.name=? GROUP BY id ORDER BY ?\u0026#39; at line 1, Congratulations! You\u0026rsquo;ve used Pixie to find the bug in the microservices app!\n"
},
{
	"uri": "/intermediate/230_logging/config_es/",
	"title": "Configure Elasticsearch Access",
	"tags": [],
	"description": "",
	"content": "Mapping Roles to Users Role mapping is the most critical aspect of fine-grained access control. Fine-grained access control has some predefined roles to help you get started, but unless you map roles to users, every request to the cluster ends in a permissions error.\nBackend roles offer another way of mapping roles to users. Rather than mapping the same role to dozens of different users, you can map the role to a single backend role, and then make sure that all users have that backend role. Backend roles can be IAM roles or arbitrary strings that you specify when you create users in the internal user database.\nWe will add the Fluent Bit ARN as a backend role to the all_access role using the Elasticsearch API\n# We need to retrieve the Fluent Bit Role ARN export FLUENTBIT_ROLE=$(eksctl get iamserviceaccount --cluster eksworkshop-eksctl --namespace logging -o json | jq \u0026#39;.[].status.roleARN\u0026#39; -r) # Get the Elasticsearch Endpoint export ES_ENDPOINT=$(aws es describe-elasticsearch-domain --domain-name ${ES_DOMAIN_NAME} --output text --query \u0026#34;DomainStatus.Endpoint\u0026#34;) # Update the Elasticsearch internal database curl -sS -u \u0026#34;${ES_DOMAIN_USER}:${ES_DOMAIN_PASSWORD}\u0026#34; \\  -X PATCH \\  https://${ES_ENDPOINT}/_opendistro/_security/api/rolesmapping/all_access?pretty \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  -d\u0026#39; [ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/backend_roles\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;\u0026#39;${FLUENTBIT_ROLE}\u0026#39;\u0026#34;] } ] \u0026#39; Output\n{ \u0026#34;status\u0026#34; : \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34; : \u0026#34;\u0026#39;all_access\u0026#39; updated.\u0026#34; }  "
},
{
	"uri": "/intermediate/240_monitoring/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Uninstall Prometheus and Grafana helm uninstall prometheus --namespace prometheus kubectl delete ns prometheus helm uninstall grafana --namespace grafana kubectl delete ns grafana rm -rf ${HOME}/environment/grafana "
},
{
	"uri": "/beginner/050_deploy/servicetype/",
	"title": "Let&#39;s check Service Types",
	"tags": [],
	"description": "",
	"content": "Before we bring up the frontend service, let\u0026rsquo;s take a look at the service types we are using: This is kubernetes/service.yaml for our frontend service: apiVersion: v1 kind: Service metadata: name: ecsdemo-frontend spec: selector: app: ecsdemo-frontend type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 3000  Notice type: LoadBalancer: This will configure an ELB to handle incoming traffic to this service.\nCompare this to kubernetes/service.yaml for one of our backend services: apiVersion: v1 kind: Service metadata: name: ecsdemo-nodejs spec: selector: app: ecsdemo-nodejs ports: - protocol: TCP port: 80 targetPort: 3000  Notice there is no specific service type described. When we check the kubernetes documentation we find that the default type is ClusterIP. This Exposes the service on a cluster-internal IP. Choosing this value makes the service only reachable from within the cluster.\n"
},
{
	"uri": "/beginner/170_statefulset/testmysql/",
	"title": "Test MySQL",
	"tags": [],
	"description": "",
	"content": "You can use mysql-client to send some data to the leader, mysql-0.mysql by running the following command.\nkubectl -n mysql run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\\  mysql -h mysql-0.mysql \u0026lt;\u0026lt;EOF CREATE DATABASE test; CREATE TABLE test.messages (message VARCHAR(250)); INSERT INTO test.messages VALUES (\u0026#39;hello, from mysql-client\u0026#39;); EOF Run the following to test follower (mysql-read) received the data.\nkubectl -n mysql run mysql-client --image=mysql:5.7 -it --rm --restart=Never --\\  mysql -h mysql-read -e \u0026#34;SELECT * FROM test.messages\u0026#34; The output should look like this. \u0026#43;--------------------------\u0026#43; | message | \u0026#43;--------------------------\u0026#43; | hello, from mysql-client | \u0026#43;--------------------------\u0026#43;  To test load balancing across followers, run the following command.\nkubectl -n mysql run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\  bash -ic \u0026#34;while sleep 1; do mysql -h mysql-read -e \u0026#39;SELECT @@server_id,NOW()\u0026#39;; done\u0026#34; Each MySQL instance is assigned a unique identifier, and it can be retrieved using @@server_id. It will print the server id serving the request and the timestamp. \u0026#43;-------------\u0026#43;---------------------\u0026#43; | @@server_id | NOW() | \u0026#43;-------------\u0026#43;---------------------\u0026#43; | 101 | 2021-02-21 19:17:52 | \u0026#43;-------------\u0026#43;---------------------\u0026#43; \u0026#43;-------------\u0026#43;---------------------\u0026#43; | @@server_id | NOW() | \u0026#43;-------------\u0026#43;---------------------\u0026#43; | 101 | 2021-02-21 19:17:53 | \u0026#43;-------------\u0026#43;---------------------\u0026#43; \u0026#43;-------------\u0026#43;---------------------\u0026#43; | @@server_id | NOW() | \u0026#43;-------------\u0026#43;---------------------\u0026#43; | 100 | 2021-02-21 19:17:54 | \u0026#43;-------------\u0026#43;---------------------\u0026#43; \u0026#43;-------------\u0026#43;---------------------\u0026#43; | @@server_id | NOW() | \u0026#43;-------------\u0026#43;---------------------\u0026#43; | 100 | 2021-02-21 19:17:55 | \u0026#43;-------------\u0026#43;---------------------\u0026#43;  Leave this open in a separate window while you test failure in the next section.\n"
},
{
	"uri": "/beginner/070_healthchecks/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Our Liveness Probe example used HTTP request and Readiness Probe executed a command to check health of a pod. Same can be accomplished using a TCP request as described in the documentation.\nkubectl delete -f ~/environment/healthchecks/liveness-app.yaml kubectl delete -f ~/environment/healthchecks/readiness-deployment.yaml "
},
{
	"uri": "/intermediate/260_weave_flux/deploymentmanifests/",
	"title": "Deploy from Manifests",
	"tags": [],
	"description": "",
	"content": "Now we are ready to use Weave Flux to deploy the hello world application into our Amazon EKS cluster. To do this we will clone our GitHub config repository (k8s-config) and then commit Kubernetes manifests to deploy.\ncd .. git clone https://github.com/${YOURUSER}/k8s-config.git cd k8s-config mkdir charts namespaces releases workloads Create a namespace Kubernetes manifest.\ncat \u0026lt;\u0026lt; EOF \u0026gt; namespaces/eks-example.yaml apiVersion: v1 kind: Namespace metadata: labels: name: eks-example name: eks-example EOF Create a deployment Kubernetes manifest.\nUpdate the image below to point to your ECR repository and image tag (Do NOT use latest). You can find your Image URI from the Amazon ECR Console. Replace YOURACCOUNT and YOURTAG)\n cat \u0026lt;\u0026lt; EOF \u0026gt; workloads/eks-example-dep.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: eks-example namespace: eks-example labels: app: eks-example annotations: # Container Image Automated Updates flux.weave.works/automated: \u0026quot;true\u0026quot; # do not apply this manifest on the cluster #flux.weave.works/ignore: \u0026quot;true\u0026quot; spec: replicas: 1 selector: matchLabels: app: eks-example template: metadata: labels: app: eks-example spec: containers: - name: eks-example image: YOURACCOUNT.dkr.ecr.us-east-1.amazonaws.com/eks-example:YOURTAG imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http protocol: TCP livenessProbe: httpGet: path: / port: http readinessProbe: httpGet: path: / port: http EOF Above you see 2 Kubernetes annotations for Flux.\n flux.weave.works/automated tells Flux whether the container image should be automatically updated. flux.weave.works/ignore is commented out, but could be used to tell Flux to temporarily ignore the deployment.  Finally, create a service manifest to enable a load balancer to be created.\ncat \u0026lt;\u0026lt; EOF \u0026gt; workloads/eks-example-svc.yaml apiVersion: v1 kind: Service metadata: name: eks-example namespace: eks-example labels: app: eks-example spec: type: LoadBalancer ports: - port: 80 targetPort: http protocol: TCP name: http selector: app: eks-example EOF Now commit the changes and push to your repository.\ngit add . git commit -am \u0026quot;eks-example-deployment\u0026quot; git push Check the logs of your Flux pod. It will pull config from the k8s-config repository every 5 minutes. Ensure you replace the pod name below with the name in your deployment.\nkubectl get pods -n flux kubectl logs flux-5bd7fb6bb6-4sc78 -n flux Now get the URL for the load balancer (LoadBalancer Ingress) and connect via your browser (this may take a couple minutes for DNS).\nkubectl describe service eks-example -n eks-example Make a change to the eks-example source code and push a new change.\ncd ../eks-example vi src/index.html # Change the \u0026lt;title\u0026gt; AND \u0026lt;h\u0026gt; to Hello World Version 2 git commit -am \u0026quot;v2 Updating home page\u0026quot; git push Now you can watch in the CodePipeline console for the new image build to complete. This will take a couple minutes. Once complete, you will see a new image land in your Amazon ECR repository. Monitor the kubectl logs for the Flux pod and you should see it update the configuration within five minutes.\nVerify the web page has updated by refreshing the page in your browser.\nYour boss calls you late at night and tells you that people are complaining about the deployment. We need to back it out immediately! We could modify the code in eks-example and trigger a new image build and deploy. However, we can also use git to revert the config change in k8s-config. Lets take that approach.\ncd ../k8s-config git pull git log --oneline git revert HEAD # Save the commit message git log --oneline git push You should now be able to watch logs for the Flux pod and it will pull the config change and roll out the previous image. Check your URL in the browser to ensure it is reverted.\nPhew! Disaster averted.\n"
},
{
	"uri": "/intermediate/241_pixie/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Next Steps Congratulations on completing the Pixie tutorial.\n To export data from the Pixie platform, check out the API. To see a list of the pre-built PxL scripts, go here. To learn how to write a custom PxL script, see the PxL tutorial. To check out the open source code, see the Github Repo  Ask questions and get help from the Pixie\u0026rsquo;s Community Slack.\nCleanup Delete the microservices demo application:\nkubectl delete namespace px-sock-shop Delete Pixie from the cluster:\npx delete --clobber Remove the Pixie CLI:\nrm -rf ${HOME}/ec2-user/bin/px Delete the node group:\nenvsubst \u0026lt; clusterconfig.yaml | eksctl delete nodegroup -f - --approve "
},
{
	"uri": "/beginner/050_deploy/servicerole/",
	"title": "Ensure the ELB Service Role exists",
	"tags": [],
	"description": "",
	"content": "In AWS accounts that have never created a load balancer before, it\u0026rsquo;s possible that the service role for ELB might not exist yet.\nWe can check for the role, and create it if it\u0026rsquo;s missing.\nCopy/Paste the following commands into your Cloud9 workspace:\naws iam get-role --role-name \u0026quot;AWSServiceRoleForElasticLoadBalancing\u0026quot; || aws iam create-service-linked-role --aws-service-name \u0026quot;elasticloadbalancing.amazonaws.com\u0026quot; "
},
{
	"uri": "/advanced/350_opentelemetry/deploy_backend_microservices/",
	"title": "Deploy Backend Microservices",
	"tags": [],
	"description": "",
	"content": "Deploy Backend Microservices The microservice we are going to deploy looks like this one:\nGrab a copy of the microservices:\ngit clone https://github.com/con204/adot-demo.git cd adot-demo Deploy the microservice\u0026rsquo;s databases into your EKS Cluster:\nkubectl apply -f kubernetes/databases/ Quickly, check all the database services are deployed and ready to receive requests: $ kubectl get pods NAME READY STATUS RESTARTS AGE employee-mongo-67cd95c7d9-jdhfb 1/1 Running 0 13s jaeger-tracing-salaryamount-mysql-76cf744c75-t9q85 1/1 Running 0 13s salary-grade-postgres-database-6df4676bb-j59lp 1/1 Running 0 13s  Deploy the backend services:\nkubectl apply -f kubernetes/backend/ In the backend service manifest, we specified an ingress with ELB Load Balancer. Let‚Äôs wait until the ELB is fully deployed: $ kubectl get svc jaeger-tracing-nodejs-service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE jaeger-tracing-nodejs-service LoadBalancer 10.100.104.173 a814b34247f984307ac5d078b6e3f1b0-1991136339.us-west-2.elb.amazonaws.com 8081:32521/TCP 13s  If you do not see an External-IP, re-run the command and wait till it shows (this can take a minute or two)\n "
},
{
	"uri": "/beginner/110_irsa/iam-role-for-sa-1/",
	"title": "Creating an IAM Role for Service Account",
	"tags": [],
	"description": "",
	"content": "You will create an IAM policy that specifies the permissions that you would like the containers in your pods to have.\nIn this workshop we will use the AWS managed policy named \u0026ldquo;AmazonS3ReadOnlyAccess\u0026rdquo; which allow get and list for all your S3 buckets.\nLet\u0026rsquo;s start by finding the ARN for the \u0026ldquo;AmazonS3ReadOnlyAccess\u0026rdquo; policy\naws iam list-policies --query \u0026#39;Policies[?PolicyName==`AmazonS3ReadOnlyAccess`].Arn\u0026#39; \u0026#34;arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\u0026#34;  Now you will create a IAM role bound to a service account with read-only access to S3\neksctl create iamserviceaccount \\  --name iam-test \\  --namespace default \\  --cluster eksworkshop-eksctl \\  --attach-policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess \\  --approve \\  --override-existing-serviceaccounts 2021-07-21 10:31:14 [‚Ñπ] eksctl version 0.57.0 2021-07-21 10:31:14 [‚Ñπ] using region us-east-1 2021-07-21 10:31:17 [‚Ñπ] 1 iamserviceaccount (default/iam-test) was included (based on the include/exclude rules) 2021-07-21 10:31:17 [!] metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set 2021-07-21 10:31:17 [‚Ñπ] 1 task: { 2 sequential sub-tasks: { create IAM role for serviceaccount \u0026#34;default/iam-test\u0026#34;, create serviceaccount \u0026#34;default/iam-test\u0026#34; } } 2021-07-21 10:31:17 [‚Ñπ] building iamserviceaccount stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-default-iam-test\u0026#34; 2021-07-21 10:31:17 [‚Ñπ] deploying stack \u0026#34;eksctl-sandbox-addon-iamserviceaccount-default-iam-test\u0026#34; 2021-07-21 10:31:17 [‚Ñπ] waiting for CloudFormation stack \u0026#34;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-default-iam-test\u0026#34; 2021-07-21 10:31:53 [‚Ñπ] created serviceaccount \u0026#34;default/iam-test\u0026#34;  If you go to the CloudFormation in IAM Console, you will find that the stack \u0026ldquo;eksctl-eksworkshop-eksctl-addon-iamserviceaccount-default-iam-test\u0026rdquo; has created a role for your service account.\n "
},
{
	"uri": "/beginner/185_bottlerocket/deployapp/",
	"title": "Deploy sample application",
	"tags": [],
	"description": "",
	"content": "Deploy nginx pod on a Bottlerocket node Create a namespace\nkubectl create namespace bottlerocket-nginx Create a simple nginx pod config:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/bottlerocket-nginx.yaml apiVersion: v1 kind: Pod metadata: name: nginx namespace: bottlerocket-nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: role: bottlerocket EoF Deploy the application:\nkubectl create -f ~/environment/bottlerocket-nginx.yaml Next, run the following command to confirm the new application is running on a bottlerocket node:\nkubectl describe pod/nginx -n bottlerocket-nginx Output: Node: ip-192-168-70-50.us-east-2.compute.internal/192.168.70.50  "
},
{
	"uri": "/advanced/430_emr_on_eks/monitoring_and_logging_0/",
	"title": "Monitoring and logging Part 1 - Setup",
	"tags": [],
	"description": "",
	"content": "Logs from the EMR jobs can be sent to cloudwatch and s3. In the last section of running sample job, we did not configure logging\nWe will run the job again now, only this time we will send the logs to s3 and cloudwatch.\nLet\u0026rsquo;s create a cloudwatch log group before we can start sending the logs.\naws logs create-log-group --log-group-name=/emr-on-eks/eksworkshop-eksctl Let\u0026rsquo;s make sure that we have the variables set for the S3 bucket, virtual EMR clusters id, and the ARN of the role that EMR uses for job execution.\nexport s3DemoBucket=s3://emr-eks-demo-${ACCOUNT_ID}-${AWS_REGION} export VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query \u0026#34;virtualClusters[?state==\u0026#39;RUNNING\u0026#39;].id\u0026#34; --output text) export EMR_ROLE_ARN=$(aws iam get-role --role-name EMRContainers-JobExecutionRole --query Role.Arn --output text) Now let\u0026rsquo;s run the job again with logging enabled.\ncat \u0026gt; request.json \u0026lt;\u0026lt;EOF { \u0026quot;name\u0026quot;: \u0026quot;pi-4\u0026quot;, \u0026quot;virtualClusterId\u0026quot;: \u0026quot;${VIRTUAL_CLUSTER_ID}\u0026quot;, \u0026quot;executionRoleArn\u0026quot;: \u0026quot;${EMR_ROLE_ARN}\u0026quot;, \u0026quot;releaseLabel\u0026quot;: \u0026quot;emr-6.2.0-latest\u0026quot;, \u0026quot;jobDriver\u0026quot;: { \u0026quot;sparkSubmitJobDriver\u0026quot;: { \u0026quot;entryPoint\u0026quot;: \u0026quot;local:///usr/lib/spark/examples/src/main/python/pi.py\u0026quot;, \u0026quot;sparkSubmitParameters\u0026quot;: \u0026quot;--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\u0026quot; } }, \u0026quot;configurationOverrides\u0026quot;: { \u0026quot;applicationConfiguration\u0026quot;: [ { \u0026quot;classification\u0026quot;: \u0026quot;spark-defaults\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;spark.dynamicAllocation.enabled\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;spark.kubernetes.executor.deleteOnTermination\u0026quot;: \u0026quot;true\u0026quot; } } ], \u0026quot;monitoringConfiguration\u0026quot;: { \u0026quot;cloudWatchMonitoringConfiguration\u0026quot;: { \u0026quot;logGroupName\u0026quot;: \u0026quot;/emr-on-eks/eksworkshop-eksctl\u0026quot;, \u0026quot;logStreamNamePrefix\u0026quot;: \u0026quot;pi\u0026quot; }, \u0026quot;s3MonitoringConfiguration\u0026quot;: { \u0026quot;logUri\u0026quot;: \u0026quot;${s3DemoBucket}/\u0026quot; } } } } EOF Trigger the Spark job\naws emr-containers start-job-run --cli-input-json file://request.json Output:\n\u0026#34;id\u0026#34;: \u0026#34;00000002u5ipstrq84e\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;pi-4\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:emr-containers:us-west-2:xxxxxxxxxxx:/virtualclusters/jokbdf64kj891f7iaaot3qo9q/jobruns/00000002u5ipstrq84e\u0026#34;, \u0026#34;virtualClusterId\u0026#34;: \u0026#34;jokbdf64kj891f7iaaot3qo9q\u0026#34;  "
},
{
	"uri": "/advanced/340_appmesh_flagger/canary_deploy/",
	"title": "Deploy Canary Set Up",
	"tags": [],
	"description": "",
	"content": "Clone the repository cd ~/environment git clone https://github.com/aws-containers/eks-microservice-demo.git cd eks-microservice-demo Create a namespace and a mesh kubectl apply -f flagger/mesh.yaml namespace/flagger created mesh.appmesh.k8s.aws/flagger created  Create a deployment and a horizontal pod autoscaler export APP_VERSION=1.0 envsubst \u0026lt; ./flagger/flagger-app.yaml | kubectl apply -f - horizontalpodautoscaler.autoscaling/detail created deployment.apps/detail created  Create IAM policy and Role for flagger-loadtester # Create an IAM policy called AWSAppMeshK8sControllerIAMPolicy aws iam create-policy \\  --policy-name FlaggerEnvoyNamespaceIAMPolicy \\  --policy-document file://envoy-iam-policy.json # Create an IAM service account for flagger namespace  eksctl create iamserviceaccount --cluster eksworkshop-eksctl \\  --namespace flagger \\  --name flagger-envoy-proxies \\  --attach-policy-arn arn:aws:iam::$ACCOUNT_ID:policy/FlaggerEnvoyNamespaceIAMPolicy \\  --override-existing-serviceaccounts \\  --approve Deploy the load testing service to generate traffic during the canary analysis helm upgrade -i flagger-loadtester flagger/loadtester \\  --namespace=flagger \\  --set appmesh.enabled=true \\  --set \u0026#34;appmesh.backends[0]=detail\u0026#34; \\  --set \u0026#34;appmesh.backends[1]=detail-canary\u0026#34; \\  --set \u0026#34;serviceAccountName=flagger-envoy-proxies\u0026#34; Release \u0026#34;flagger-loadtester\u0026#34; does not exist. Installing it now. NAME: flagger-loadtester LAST DEPLOYED: Mon Mar 15 19:31:10 2021 NAMESPACE: flagger STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Flagger\u0026#39;s load testing service is available at http://flagger-loadtester.flagger/  Create a canary definition Now lets deploy the Flagger canary definition file for detail service\nkubectl apply -f flagger/flagger-canary.yaml canary.flagger.app/detail created  You can see the below objects being created by flagger\nkubectl -n appmesh-system logs deploy/flagger --tail 15 -f | jq .msg \u0026#34;Service detail-primary.flagger created\u0026#34; \u0026#34;all the metrics providers are available!\u0026#34; \u0026#34;VirtualNode detail-primary.flagger created\u0026#34; \u0026#34;VirtualNode detail-canary.flagger created\u0026#34; \u0026#34;VirtualRouter detail created\u0026#34; \u0026#34;VirtualService detail created\u0026#34; \u0026#34;VirtualRouter detail-canary created\u0026#34; \u0026#34;VirtualService detail-canary created\u0026#34; \u0026#34;Deployment detail-primary.flagger created\u0026#34; \u0026#34;detail-primary.flagger not ready: waiting for rollout to finish: observed deployment generation less then desired generation\u0026#34; \u0026#34;all the metrics providers are available!\u0026#34; \u0026#34;Scaling down Deployment detail.flagger\u0026#34; \u0026#34;HorizontalPodAutoscaler detail-primary.flagger created\u0026#34; \u0026#34;Service detail.flagger created\u0026#34; \u0026#34;Initialization done! detail.flagger\u0026#34;  You should see the below event from canary\nkubectl -n flagger describe canary/detail Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Synced 2m24s flagger detail-primary.flagger not ready: waiting for rollout to finish: observed deployment generation less then desired generation Normal Synced 85s (x2 over 2m24s) flagger all the metrics providers are available! Normal Synced 84s flagger Initialization done! detail.flagger  After the bootstrap is completed,\n detail deployment is scaled to zero. Traffic to detail.flagger will be routed to the primary pods. AppMesh resources like virtualnode, virtualservice, virtualrouter has been created for the detail service  kubectl get pod,deployment,svc,virtualnode,virtualservice,virtualrouter -n flagger NAME READY STATUS RESTARTS AGE pod/detail-primary-5f4cb44b79-xxxx 3/3 Running 0 3m58s pod/detail-primary-5f4cb44b79-xxxx 3/3 Running 0 3m58s pod/flagger-loadtester-5bdf76cfb7-yyyy 3/3 Running 0 5m35s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/detail 0/0 0 0 4m50s deployment.apps/detail-primary 2/2 2 2 3m58s deployment.apps/flagger-loadtester 1/1 1 1 5m35s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/detail ClusterIP 10.100.xxx.104 \u0026lt;none\u0026gt; 3000/TCP 2m58s service/detail-canary ClusterIP 10.100.yyy.188 \u0026lt;none\u0026gt; 3000/TCP 3m59s service/detail-primary ClusterIP 10.100.zzz.217 \u0026lt;none\u0026gt; 3000/TCP 3m59s service/flagger-loadtester ClusterIP 10.100.pp.62 \u0026lt;none\u0026gt; 80/TCP 5m35s NAME ARN AGE virtualnode.appmesh.k8s.aws/detail-canary arn:aws:appmesh:us-east-2:$ACCOUNT_ID:mesh/flagger/virtualNode/detail-canary_flagger 3m59s virtualnode.appmesh.k8s.aws/detail-primary arn:aws:appmesh:us-east-2:$ACCOUNT_ID:mesh/flagger/virtualNode/detail-primary_flagger 3m59s virtualnode.appmesh.k8s.aws/flagger-loadtester arn:aws:appmesh:us-east-2:$ACCOUNT_ID:mesh/flagger/virtualNode/flagger-loadtester_flagger 5m35s NAME ARN AGE virtualservice.appmesh.k8s.aws/detail arn:aws:appmesh:us-east-2:$ACCOUNT_ID:mesh/flagger/virtualService/detail.flagger 3m58s virtualservice.appmesh.k8s.aws/detail-canary arn:aws:appmesh:us-east-2:$ACCOUNT_ID:mesh/flagger/virtualService/detail-canary.flagger 3m58s NAME ARN AGE virtualrouter.appmesh.k8s.aws/detail arn:aws:appmesh:us-east-2:$ACCOUNT_ID:mesh/flagger/virtualRouter/detail_flagger 3m58s virtualrouter.appmesh.k8s.aws/detail-canary arn:aws:appmesh:us-east-2:$ACCOUNT_ID:mesh/flagger/virtualRouter/detail-canary_flagger 3m58s  Testing the setup Exec into flagger-loadtester pod\nkubectl exec deploy/flagger-loadtester -n flagger -it bash Defaulting container name to loadtester. Use \u0026#39;kubectl describe pod/flagger-loadtester-5bdf76cfb7-wl59d -n flagger\u0026#39; to see all of the containers in this pod. bash-5.0$  Curl to detail service to confirm if you get response from the deployed service\ncurl http://detail.flagger:3000/catalogDetail {\u0026#34;version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;vendors\u0026#34;:[\u0026#34;ABC.com\u0026#34;]}  Congratulations! You have set up the canary analysis for backend service detail succcessfuly.\n"
},
{
	"uri": "/beginner/150_spotnodegroups/deployapp/",
	"title": "Deploy an Application on Spot",
	"tags": [],
	"description": "",
	"content": "We are redesigning our Microservice example and want our frontend service to be deployed on Spot Instances when they are available. We will ensure that the NodeJS and crystal backend services are deployed on On-Demand Instances. We will use Node Affinity in our manifest file to configure this.\nConfigure Node Affinity for the services Open the deployment manifest of the frontend service in your Cloud9 editor - ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml\nEdit the spec to configure NodeAffinity to prefer Spot Instances, but not require them. This will allow the pods to be scheduled on On-Demand nodes if no spot instances were available or correctly labelled.\nOpen the deployment manifest for the backend services in your Cloud9 editor - ~/environment/ecsdemo-nodejs/kubernetes/deployment.yaml and ~/environment/ecsdemo-crystal/kubernetes/deployment.yaml\nEdit the spec to configure NodeAffinity to require On-Demand Instances. This will allow the pods to be scheduled on On-Demand nodes and not on the Spot Instances.\nFor examples of Node Affinity, check this link\nChallenge Configure Affinity\n  Expand here to see the solution    Add this to your deployment file for the frontend service under spec.template.spec  affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: eks.amazonaws.com/capacityType operator: In values: - SPOT Add this to your deployment file for the backend services under spec.template.spec  affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: eks.amazonaws.com/capacityType operator: In values: - ON_DEMAND     Deployment final files   ecsdemo-crystal-deployment.yml  (0 ko)   ecsdemo-frontend-deployment.yml  (1 ko)   ecsdemo-nodejs-deployment.yml  (0 ko)    Redeploy the application - Frontend on Spot and Backend on On-Demand First let\u0026rsquo;s take a look at all pods deployed on Spot instances\nfor n in $(kubectl get nodes -l eks.amazonaws.com/capacityType=SPOT --no-headers | cut -d \u0026#34; \u0026#34; -f1); do echo \u0026#34;Pods on instance ${n}:\u0026#34;;kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; echo ; done Now we will redeploy our microservices with our edited Frontend Manifest\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml We can again check all pods deployed on Spot Instances and should now see the frontend pods running on Spot instances\nfor n in $(kubectl get nodes -l eks.amazonaws.com/capacityType=SPOT --no-headers | cut -d \u0026#34; \u0026#34; -f1); do echo \u0026#34;Pods on instance ${n}:\u0026#34;;kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; echo ; done Let\u0026rsquo;s check all the pods deployed on On-Demand Instances and should now see all the backend pods running on On-Demand instances\nfor n in $(kubectl get nodes -l eks.amazonaws.com/capacityType=ON_DEMAND --no-headers | cut -d \u0026#34; \u0026#34; -f1); do echo \u0026#34;Pods on instance ${n}:\u0026#34;;kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; echo ; done "
},
{
	"uri": "/intermediate/246_monitoring_amp_amg/ingest_metrics/",
	"title": "Ingest Metrics into AMP",
	"tags": [],
	"description": "",
	"content": "Amazon Managed Service for Prometheus does not directly scrape operational metrics from containerized workloads in a Kubernetes cluster. It requires users to deploy and manage a standard Prometheus server, or an OpenTelemetry agent such as the AWS Distro for OpenTelemetry Collector in their cluster to perform this task.\nExecute the following commands to deploy the Prometheus server on the EKS cluster helm repo add prometheus-community https://prometheus-community.github.io/helm-charts kubectl create ns prometheus Create a file called amp_ingest_override_values.yaml with the following content in it. serviceAccounts: ## Disable alert manager roles ## server: name: \u0026quot;iamproxy-service-account\u0026quot; alertmanager: create: false ## Disable pushgateway ## pushgateway: create: false server: remoteWrite: - queue_config: max_samples_per_send: 1000 max_shards: 200 capacity: 2500 ## Use a statefulset instead of a deployment for resiliency ## statefulSet: enabled: true ## Store blocks locally for short time period only ## retention: 1h ## Disable alert manager ## alertmanager: enabled: false ## Disable pushgateway ## pushgateway: enabled: false Execute the following command to install the Prometheus server configuration and configure the remoteWrite endpoint export SERVICE_ACCOUNT_IAM_ROLE=EKS-AMP-ServiceAccount-Role export SERVICE_ACCOUNT_IAM_ROLE_ARN=$(aws iam get-role --role-name $SERVICE_ACCOUNT_IAM_ROLE --query 'Role.Arn' --output text) WORKSPACE_ID=$(aws amp list-workspaces --alias eks-workshop | jq .workspaces[0].workspaceId -r) helm install prometheus-for-amp prometheus-community/prometheus -n prometheus -f ./amp_ingest_override_values.yaml \\ --set serviceAccounts.server.annotations.\u0026quot;eks\\.amazonaws\\.com/role-arn\u0026quot;=\u0026quot;${SERVICE_ACCOUNT_IAM_ROLE_ARN}\u0026quot; \\ --set server.remoteWrite[0].url=\u0026quot;https://aps-workspaces.${AWS_REGION}.amazonaws.com/workspaces/${WORKSPACE_ID}/api/v1/remote_write\u0026quot; \\ --set server.remoteWrite[0].sigv4.region=${AWS_REGION} "
},
{
	"uri": "/beginner/115_sg-per-pod/30_cni_config/",
	"title": "CNI configuration",
	"tags": ["beginner"],
	"description": "",
	"content": "To enable this new functionality, Amazon EKS clusters have two new components running on the Kubernetes control plane:\n A mutating webhook responsible for adding limits and requests to pods requiring security groups. A resource controller responsible for managing network interfaces associated with those pods.  To facilitate this feature, each worker node will be associated with a single trunk network interface, and multiple branch network interfaces. The trunk interface acts as a standard network interface attached to the instance. The VPC resource controller then associates branch interfaces to the trunk interface. This increases the number of network interfaces that can be attached per instance. Since security groups are specified with network interfaces, we are now able to schedule pods requiring specific security groups onto these additional network interfaces allocated to worker nodes.\nFirst we need to attach a new IAM policy the Node group role to allow the EC2 instances to manage network interfaces, their private IP addresses, and their attachment and detachment to and from instances.\nThe following command adds the policy AmazonEKSVPCResourceController to a cluster role.\naws iam attach-role-policy \\  --policy-arn arn:aws:iam::aws:policy/AmazonEKSVPCResourceController \\  --role-name ${ROLE_NAME} Next, we will enable the CNI plugin to manage network interfaces for pods by setting the ENABLE_POD_ENI variable to true in the aws-node DaemonSet.\nkubectl -n kube-system set env daemonset aws-node ENABLE_POD_ENI=true # let\u0026#39;s way for the rolling update of the daemonset kubectl -n kube-system rollout status ds aws-node Once this setting is set to true, for each node in the cluster the plugin adds a label with the value vpc.amazonaws.com/has-trunk-attached=true to the compatible instances. The VPC resource controller creates and attaches one special network interface called a trunk network interface with the description aws-k8s-trunk-eni.\nkubectl get nodes \\  --selector eks.amazonaws.com/nodegroup=nodegroup-sec-group \\  --show-labels "
},
{
	"uri": "/beginner/091_iam-groups/create-k8s-rbac/",
	"title": "Configure Kubernetes RBAC",
	"tags": [],
	"description": "",
	"content": "You can refer to intro to RBAC module to understand the basic of Kubernetes RBAC.\nCreate kubernetes namespaces  development namespace will be accessible for IAM users from k8sDev group integration namespace will be accessible for IAM users from k8sInteg group  kubectl create namespace integration kubectl create namespace development Configuring access to development namespace We create a kubernetes role and rolebinding in the development namespace giving full access to the kubernetes user dev-user\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - -n development kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: dev-role rules: - apiGroups: - \u0026#34;\u0026#34; - \u0026#34;apps\u0026#34; - \u0026#34;batch\u0026#34; - \u0026#34;extensions\u0026#34; resources: - \u0026#34;configmaps\u0026#34; - \u0026#34;cronjobs\u0026#34; - \u0026#34;deployments\u0026#34; - \u0026#34;events\u0026#34; - \u0026#34;ingresses\u0026#34; - \u0026#34;jobs\u0026#34; - \u0026#34;pods\u0026#34; - \u0026#34;pods/attach\u0026#34; - \u0026#34;pods/exec\u0026#34; - \u0026#34;pods/log\u0026#34; - \u0026#34;pods/portforward\u0026#34; - \u0026#34;secrets\u0026#34; - \u0026#34;services\u0026#34; verbs: - \u0026#34;create\u0026#34; - \u0026#34;delete\u0026#34; - \u0026#34;describe\u0026#34; - \u0026#34;get\u0026#34; - \u0026#34;list\u0026#34; - \u0026#34;patch\u0026#34; - \u0026#34;update\u0026#34; --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: dev-role-binding subjects: - kind: User name: dev-user roleRef: kind: Role name: dev-role apiGroup: rbac.authorization.k8s.io EOF The role we define will give full access to everything in that namespace. It is a Role, and not a ClusterRole, so it is going to be applied only in the development namespace.\n feel free to adapt or duplicate to any namespace you prefer.\n Configuring access to integration namespace We create a kubernetes role and rolebinding in the integration namespace for full access with the kubernetes user integ-user\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - -n integration kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: integ-role rules: - apiGroups: - \u0026#34;\u0026#34; - \u0026#34;apps\u0026#34; - \u0026#34;batch\u0026#34; - \u0026#34;extensions\u0026#34; resources: - \u0026#34;configmaps\u0026#34; - \u0026#34;cronjobs\u0026#34; - \u0026#34;deployments\u0026#34; - \u0026#34;events\u0026#34; - \u0026#34;ingresses\u0026#34; - \u0026#34;jobs\u0026#34; - \u0026#34;pods\u0026#34; - \u0026#34;pods/attach\u0026#34; - \u0026#34;pods/exec\u0026#34; - \u0026#34;pods/log\u0026#34; - \u0026#34;pods/portforward\u0026#34; - \u0026#34;secrets\u0026#34; - \u0026#34;services\u0026#34; verbs: - \u0026#34;create\u0026#34; - \u0026#34;delete\u0026#34; - \u0026#34;describe\u0026#34; - \u0026#34;get\u0026#34; - \u0026#34;list\u0026#34; - \u0026#34;patch\u0026#34; - \u0026#34;update\u0026#34; --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: integ-role-binding subjects: - kind: User name: integ-user roleRef: kind: Role name: integ-role apiGroup: rbac.authorization.k8s.io EOF The role we define will give full access to everything in that namespace. It is a Role, and not a ClusterRole, so it is going to be applied only in the integration namespace.\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/deploy_applications/about_the_k8s_app/",
	"title": "About Product Catalog Application",
	"tags": [],
	"description": "",
	"content": "Application Architecture The example application architecture we\u0026rsquo;ll walk you through creating on App Mesh is called Product Catalog and is used in any eCommerce Application. We have attempted to pick different technology framework for building applications to include the polyglot nature of microservices-based applications.\nThis application is composed of three microservices:\n Frontend  Frontend service frontend-node shows the UI for the Product Catalog functionality Developed in Nodejs with ejs templating Deployed to EKS Managed Nodegroup   Product Catalog Backend  Backend service prodcatalog is a Rest API Service that performs following operations:  Add a Product into Product Catalog Get the Product from the Product Catalog Calls Catalog Detail backend service proddetail to get Product Catalog Detail information like vendors Get all the products from the Product Catalog /products { \u0026#34;products\u0026#34;: { \u0026#34;1\u0026#34;: \u0026#34;Table\u0026#34;, \u0026#34;2\u0026#34;: \u0026#34;Chair\u0026#34; }, \u0026#34;details\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;vendors\u0026#34;: [ \u0026#34;ABC.com\u0026#34;, \u0026#34;XYZ.com\u0026#34; ] } }     Developed in Python Fask Restplus with Swagger UI for Rest API Deployed to EKS Fargate   Catalog Detail Backend  Backend service proddetail is a Rest API Service that performs following operation:  Get catalog detail which includes version number and vendor names /catalogDetail { \u0026#34;version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;vendors\u0026#34;: [ \u0026#34;ABC.com\u0026#34;, \u0026#34;XYZ.com\u0026#34; ] }     Developed in Nodejs Deployed to EKS Managed Nodegroup    From the above diagram, the service-call relationship between the services in Product Catalog application can be summarized as:\n Frontend frontend-node \u0026raquo;\u0026raquo;\u0026gt; calls \u0026raquo;\u0026raquo;\u0026gt; Product Catalog backend prodcatalog. Product Catalog backend prodcatalog \u0026raquo;\u0026raquo;\u0026gt; calls \u0026raquo;\u0026raquo;\u0026gt; Catalog Detail backend proddetail.  "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/appmesh_installation/",
	"title": "App Mesh Installation",
	"tags": [],
	"description": "",
	"content": "This tutorial guides you through the installation and use of the open source AWS App Mesh Controller for Kubernetes. AWS App Mesh Controller For K8s is a controller to help manage AWS App Mesh resources for a Kubernetes cluster and injecting sidecars to Kubernetes Pods.\nThe controller maintains the custom resources (CRDs): Mesh, VirtualNode, VirtualService, VirtualRouter, VirtualGateway and GatewayRoute. When using the App Mesh Controller, you manage these App Mesh custom resources such as VirtualService and VirtualNode through the Kubernetes API the same way you manage native Kubernetes resources such as Service and Deployment.\nThe controller monitors your Kubernetes objects, and when App Mesh resources are created or changed it reflects those changes in AWS App Mesh for you. Specification of your App Mesh resources is done through the use of Custom Resource Definitions (CRDs) provided by the App Mesh Controller project. These custom resources map to below App Mesh API objects.\n Mesh VirtualService VirtualNode VirtualRouter VirtualGateway GatewayRoute  For a pod in your application to join a mesh, it must have an open source Envoy proxy container running as sidecar container within the pod. This establishes the data plane that AWS App Mesh controls. So we must run an Envoy container within each pod of the Product Catalog App deployment. App Mesh uses the Envoy sidecar container as a proxy for all ingress and egress traffic to the primary microservice. Using this sidecar pattern with Envoy we create the backbone of the service mesh, without impacting our applications.\nThe controller will handle routine App Mesh tasks such as creating and injecting Envoy proxy containers into your application pods. Automated sidecar injection is controlled by enabling a webhook on a per-namespace basis. Our Product Catalog Application services uses the below App Mesh Configuration and we will explore how to create these in detail. "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/port_to_app_mesh/",
	"title": "Porting Product Catalog to App Mesh",
	"tags": [],
	"description": "",
	"content": "Challenge Today, Product Catalog frontend frontend-node is hardwired to make requests to prodcatalog and prodcatalog is hardwired to make requests to proddetail.\nEach time there is a new version of proddetail release, we also need to release a new version of prodcatalog to support both the new and the old version to point to its version-specific endpoints. It works, but it\u0026rsquo;s not an optimal configuration to maintain for the long term.\nprodcatalog backend service is deployed to Faragate, and rest of the services frontend-node and proddetail are deployed to Managed Nodegroup, we need to add all these services into the App Mesh and ensure these microservices can communicate with each other.\nSolution We\u0026rsquo;re going to demonstrate how AWS App Mesh can be used to simplify this architecture; by virtualizing the proddetail service, we can add dynamic configuration and route traffic to the versioned endpoints of our choosing, minimizing the need for complete re-deployment of the prodcatalog service each time there is a new proddetail service release.\nWe\u0026rsquo;re also going to demonstrate how all the microservices in Nodegroup and Fargate can communicate with each other via App Mesh.\nWhen we\u0026rsquo;re done with porting the application in this chapter, our app will look more like the following.\nNow that the App Mesh Controller and CRDs are installed, we\u0026rsquo;re ready to define the App Mesh components required for our mesh-enabled version of the app.\n"
},
{
	"uri": "/advanced/420_kubeflow/jupyter/",
	"title": "Jupyter Notebook",
	"tags": [],
	"description": "",
	"content": "Jupyter Notebook using Kubeflow on Amazon EKS   The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. It is often used for data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and more.\nIn Kubeflow dashboard, click on Create a new Notebook server:\nSelect the namespace created in previous step:\nThis pre-populates the namespace field on the dashboard. Specify a name myjupyter for the notebook:\nIn the Image section, select the latest tensorflow-1.x image whose name ends in cpu (not gpu) from the dropbown box:\nChange the CPU value to 1.0:\nScroll to the bottom, take all other defaults, and click on LAUNCH.\nIt takes a few seconds for the Jupyter notebook to come online. Click on CONNECT\nThis connects to the notebook and opens the notebook interface in a new browser tab.\nCLick on New, select Python3\nThis creates an empty Python 3 Jupyter notebook\nCopy the sample training code and paste it in the first code block. This Python sample code uses TensorFlow to create a training model for MNIST database. Click on Run to load this code in notebook.\nThis also creates a new code block. Copy main() in this new code block and click on Run again\nThis starts the model training and the output is shown on the notebook:\nThe first few lines shows that TensorFlow and Keras dataset is downloaded. Training data set is 60k images and test data set is 10k images. Hyperparameters used for the training, outputs from five epochs, and finally the model accuracy is shown.\n"
},
{
	"uri": "/advanced/310_servicemesh_with_istio/install/",
	"title": "Install Istio",
	"tags": [],
	"description": "",
	"content": "We will install all the Istio components using the built-in demo configuration profile. This installation lets you quickly get started evaluating Istio.\nThe demo configuration profile is not suitable for performance evaluation. It is designed to showcase Istio functionality with high levels of tracing and access logging. For more information about Istio profile, click here.\n Istio will be installed in the istio-system namespace.\nyes | istioctl install --set profile=demo ‚úî Istio core installed ‚úî Istiod installed ‚úî Egress gateways installed ‚úî Ingress gateways installed ‚úî Installation complete  We can verify all the services have been installed.\nkubectl -n istio-system get svc The output should look like this\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-egressgateway ClusterIP 10.100.167.140 \u0026lt;none\u0026gt; 80/TCP,443/TCP,15443/TCP 107s istio-ingressgateway LoadBalancer 10.100.15.31 abc1fcfe168bb4d9e8264e8952758806-1033162489.us-east-2.elb.amazonaws.com 15021:30819/TCP,80:30708/TCP,443:32447/TCP,31400:31433/TCP,15443:30201/TCP 107s istiod ClusterIP 10.100.133.178 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 117s  and check the corresponding pods with\nkubectl -n istio-system get pods NAME READY STATUS RESTARTS AGE istio-egressgateway-cd6b59579-vlv6c 1/1 Running 0 2m35s istio-ingressgateway-78f7794d66-9jbw5 1/1 Running 0 2m35s istiod-574485bfdc-wtjcg 1/1 Running 0 2m45s  "
},
{
	"uri": "/beginner/160_advanced-networking/secondary_cidr/configure-cni/",
	"title": "Configure CNI",
	"tags": [],
	"description": "",
	"content": "Before we start making changes to VPC CNI, let\u0026rsquo;s make sure we are using latest CNI version\nRun this command to find CNI version\nkubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d \u0026quot;/\u0026quot; -f 2 Here is a sample response amazon-k8s-cni:1.6.1  Upgrade to the latest v1.7 config if you have an older version:\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.7/config/v1.7/aws-k8s-cni.yaml Wait until all the pods are recycled. You can check the status of pods by using this command\nkubectl get pods -n kube-system -w Configure Custom networking Edit aws-node DaemonSet and add AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG environment variable to the node container spec and set it to true\nNote: You only need to set one environment variable in the CNI daemonset configuration:\nkubectl set env ds aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true kubectl describe daemonset aws-node -n kube-system | grep -A5 Environment Environment: AWS_VPC_K8S_CNI_LOGLEVEL: DEBUG AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG: true MY_NODE_NAME: (v1:spec.nodeName) ...  Terminate worker nodes so that Autoscaling launches newer nodes that come bootstrapped with custom network config\nUse caution before you run the next command because it terminates all worker nodes including running pods in your workshop\n INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag-key,Values=eks:cluster-name\u0026quot; \u0026quot;Name=tag-value,Values=eksworkshop*\u0026quot; --output text` ) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;Terminating EC2 instance $i ...\u0026quot; aws ec2 terminate-instances --instance-ids $i done "
},
{
	"uri": "/advanced/410_batch/install/",
	"title": "Install Argo CLI",
	"tags": [],
	"description": "",
	"content": "Install Argo CLI Before we can get started configuring argo we\u0026rsquo;ll need to first install the command line tools that you will interact with. To do this run the following.\n# set argo version export ARGO_VERSION=\u0026#34;v2.12.13\u0026#34; # Download the binary curl -sLO https://github.com/argoproj/argo-workflows/releases/download/${ARGO_VERSION}/argo-linux-amd64.gz # Unzip gunzip argo-linux-amd64.gz # Make binary executable chmod +x argo-linux-amd64 # Move binary to path sudo mv ./argo-linux-amd64 /usr/local/bin/argo # Test installation argo version --short output argo: v2.12.13  "
},
{
	"uri": "/intermediate/330_app_mesh/port_to_app_mesh/",
	"title": "Porting DJ to App Mesh",
	"tags": [],
	"description": "",
	"content": "Now that the App Mesh Controller and CRDs are installed, we\u0026rsquo;re ready to define the App Mesh components required for our mesh-enabled version of the app.\nAs we move to this new architecture, what will it look like, and how will it be different?\nThe diagram below shows the new architecture.\nFunctionally, the mesh-enabled version will do exactly what the current version does; requests made by dj will be served by either the metal-v1, or the jazz-v1 services. The difference will be that we\u0026rsquo;ll use AWS App Mesh to create new Virtual Services called metal and jazz.\nThese services will logically send traffic to VirtualRouter instances which will be configured to route traffic to the service endpoints within your cluster, either jazz-v1 or metal-v1.\n"
},
{
	"uri": "/beginner/090_rbac/map_iam_user_to_k8s_user/",
	"title": "Map an IAM User to K8s",
	"tags": [],
	"description": "",
	"content": "Next, we\u0026rsquo;ll define a k8s user called rbac-user, and map to its IAM user counterpart. Run the following to get the existing ConfigMap and save into a file called aws-auth.yaml:\nkubectl get configmap -n kube-system aws-auth -o yaml | grep -v \u0026quot;creationTimestamp\\|resourceVersion\\|selfLink\\|uid\u0026quot; | sed '/^ annotations:/,+2 d' \u0026gt; aws-auth.yaml Next append the rbac-user mapping to the existing configMap\ncat \u0026lt;\u0026lt; EoF \u0026gt;\u0026gt; aws-auth.yaml data: mapUsers: | - userarn: arn:aws:iam::${ACCOUNT_ID}:user/rbac-user username: rbac-user EoF Some of the values may be dynamically populated when the file is created. To verify everything populated and was created correctly, run the following:\ncat aws-auth.yaml And the output should reflect that rolearn and userarn populated, similar to:\napiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapUsers: | - userarn: arn:aws:iam::123456789:user/rbac-user username: rbac-user  Next, apply the ConfigMap to apply this mapping to the system:\nkubectl apply -f aws-auth.yaml "
},
{
	"uri": "/010_introduction/basics/what_is_k8s/",
	"title": "What is Kubernetes",
	"tags": [],
	"description": "",
	"content": " Built on over a decade of experience and best practices Utilizes declarative configuration and automation Draws upon a large ecosystem of tools, services, support  More information on what Kubernetes is all about can be found on the official Kubernetes website.\n"
},
{
	"uri": "/beginner/050_deploy/deployfrontend/",
	"title": "Deploy Frontend Service",
	"tags": [],
	"description": "",
	"content": "Challenge: Let‚Äôs bring up the Ruby Frontend!\n  Expand here to see the solution   Copy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-frontend    "
},
{
	"uri": "/920_cleanup/eksctl/",
	"title": "Delete the EKSCTL Cluster",
	"tags": [],
	"description": "",
	"content": "In order to delete the resources created for this EKS cluster, run the following commands:\nDelete the cluster:\neksctl delete cluster --name=eksworkshop-eksctl  Without the --wait flag, this will only issue a delete operation to the cluster\u0026rsquo;s CloudFormation stack and won\u0026rsquo;t wait for its deletion. The nodegroup will have to complete the deletion process before the EKS cluster can be deleted. The total process will take approximately 15 minutes, and can be monitored via the CloudFormation Console.\n "
},
{
	"uri": "/030_eksctl/",
	"title": "Launch using eksctl",
	"tags": ["beginner", "kubeflow", "appmesh", "CON203", "CON205", "CON206", "OPN401"],
	"description": "",
	"content": "Launch using eksctl   eksctl is a tool jointly developed by AWS and Weaveworks that automates much of the experience of creating EKS clusters.\nIn this module, we will use eksctl to launch and configure our EKS cluster and nodes.\n"
},
{
	"uri": "/030_eksctl/test/",
	"title": "Test the Cluster",
	"tags": [],
	"description": "",
	"content": "Test the cluster: Confirm your nodes:\nkubectl get nodes # if we see our 3 nodes, we know we have authenticated correctly Export the Worker Role Name for use throughout the workshop: STACK_NAME=$(eksctl get nodegroup --cluster eksworkshop-eksctl -o json | jq -r \u0026#39;.[].StackName\u0026#39;) ROLE_NAME=$(aws cloudformation describe-stack-resources --stack-name $STACK_NAME | jq -r \u0026#39;.StackResources[] | select(.ResourceType==\u0026#34;AWS::IAM::Role\u0026#34;) | .PhysicalResourceId\u0026#39;) echo \u0026#34;export ROLE_NAME=${ROLE_NAME}\u0026#34; | tee -a ~/.bash_profile Congratulations! You now have a fully working Amazon EKS Cluster that is ready to use! Before you move on to any other labs, make sure to complete the steps on the next page to update the EKS Console Credentials.\n"
},
{
	"uri": "/intermediate/330_app_mesh/deploy_dj_app/about_the_k8s_app/",
	"title": "About DJ App",
	"tags": [],
	"description": "",
	"content": "Architecture The example app we\u0026rsquo;ll walk you through creating on App Mesh is called DJ and is used for a cloud-based music service.\nThis application is composed of three microservices:\n dj metal-v1 jazz-v1  The dj service makes requests to either the jazz-v1 or metal-v1 backends for artist lists:\n Requests made to the jazz-v1 backend may return artists such as Miles Davis or Astrud Gilberto. Requests made to the metal-v1 backend may return artists such as Judas Priest or Megadeth.  Challenge Today, dj is hardwired to make requests to metal-v1 and jazz-v1.\nEach time there is a new metal or jazz release, we also need to release a new version of dj as to point to its new version-specific endpoints. It works, but it\u0026rsquo;s not an optimal configuration to maintain for the long term.\nSolution We\u0026rsquo;re going to demonstrate how AWS App Mesh can be used to simplify this architecture; by virtualizing the metal and jazz services, we can add dynamic configuration and route traffic to the versioned endpoints of our choosing, minimizing the need for complete re-deployment of the dj service each time there is a new metal or jazz service release.\nWhen we\u0026rsquo;re done, our app will look more like the following:\n"
},
{
	"uri": "/intermediate/330_app_mesh/install_app_mesh_controller/about_sidecar/",
	"title": "About Sidecars",
	"tags": [],
	"description": "",
	"content": "For a pod in your application to join a mesh, it must have an Envoy proxy container running sidecar within the pod. This establishes the data plane that AWS App Mesh controls. So we must run an Envoy container within each pod of the DJ App deployment. For example:\nThis can be accomplished a few different ways:\n  Before installing the application, you can modify the DJ App Deployment container specs to include App Mesh sidecar containers and set a few required configuration elements and environment variables. When pods are deployed, they would run the sidecar.\n  After installing the application, you can patch each Deployment to include the sidecar container specs. Upon applying this patch, the old pods would be torn down, and the new pods would come up with the sidecar.\n  You can enable the AWS App Mesh Sidecar Injector in the meshed namespace, which watches for new pods to be created and automatically adds the sidecar container and required configuration to the pods as they are deployed.\n  In this tutorial, you will use the third option and enable automatic sidecar injection for your meshed pods.\n"
},
{
	"uri": "/beginner/040_dashboard/connect/",
	"title": "Access the Dashboard",
	"tags": [],
	"description": "",
	"content": "Now we can access the Kubernetes Dashboard\n In your Cloud9 environment, click Tools / Preview / Preview Running Application Scroll to the end of the URL and append:  /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ The Cloud9 Preview browser doesn\u0026rsquo;t appear to support the token authentication, so once you have the login screen in the cloud9 preview browser tab, press the Pop Out button to open the login screen in a regular browser tab, like below: Open a New Terminal Tab and enter\naws eks get-token --cluster-name eksworkshop-eksctl | jq -r \u0026#39;.status.token\u0026#39; Copy the output of this command and then click the radio button next to Token then in the text field below paste the output from the last command.\nThen press Sign In.\n"
},
{
	"uri": "/beginner/080_scaling/deploy_ca/",
	"title": "Configure Cluster Autoscaler (CA)",
	"tags": [],
	"description": "",
	"content": "Cluster Autoscaler for AWS provides integration with Auto Scaling groups. It enables users to choose from four different options of deployment:\n One Auto Scaling group Multiple Auto Scaling groups Auto-Discovery Control-plane Node setup  Auto-Discovery is the preferred method to configure Cluster Autoscaler. Click here for more information.\nCluster Autoscaler will attempt to determine the CPU, memory, and GPU resources provided by an Auto Scaling Group based on the instance type specified in its Launch Configuration or Launch Template.\nConfigure the ASG You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. When we created the cluster we set these settings to 3.\naws autoscaling \\  describe-auto-scaling-groups \\  --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;eksworkshop-eksctl\u0026#39;]].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026#34; \\  --output table ------------------------------------------------------------- | DescribeAutoScalingGroups | \u0026#43;-------------------------------------------\u0026#43;----\u0026#43;----\u0026#43;-----\u0026#43; | eks-1eb9b447-f3c1-0456-af77-af0bbd65bc9f | 2 | 4 | 3 | \u0026#43;-------------------------------------------\u0026#43;----\u0026#43;----\u0026#43;-----\u0026#43;  Now, increase the maximum capacity to 4 instances\n# we need the ASG name export ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;eksworkshop-eksctl\u0026#39;]].AutoScalingGroupName\u0026#34; --output text) # increase max capacity up to 4 aws autoscaling \\  update-auto-scaling-group \\  --auto-scaling-group-name ${ASG_NAME} \\  --min-size 3 \\  --desired-capacity 3 \\  --max-size 4 # Check new values aws autoscaling \\  describe-auto-scaling-groups \\  --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;eksworkshop-eksctl\u0026#39;]].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026#34; \\  --output table IAM roles for service accounts Click here if you are not familiar with IAM Roles for Service Accounts (IRSA).\n With IAM roles for service accounts on Amazon EKS clusters, you can associate an IAM role with a Kubernetes service account. This service account can then provide AWS permissions to the containers in any pod that uses that service account. With this feature, you no longer need to provide extended permissions to the node IAM role so that pods on that node can call AWS APIs.\nEnabling IAM roles for service accounts on your cluster\neksctl utils associate-iam-oidc-provider \\  --cluster eksworkshop-eksctl \\  --approve Creating an IAM policy for your service account that will allow your CA pod to interact with the autoscaling groups.\nmkdir ~/environment/cluster-autoscaler cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/cluster-autoscaler/k8s-asg-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;autoscaling:DescribeAutoScalingGroups\u0026#34;, \u0026#34;autoscaling:DescribeAutoScalingInstances\u0026#34;, \u0026#34;autoscaling:DescribeLaunchConfigurations\u0026#34;, \u0026#34;autoscaling:DescribeTags\u0026#34;, \u0026#34;autoscaling:SetDesiredCapacity\u0026#34;, \u0026#34;autoscaling:TerminateInstanceInAutoScalingGroup\u0026#34;, \u0026#34;ec2:DescribeLaunchTemplateVersions\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } ] } EoF aws iam create-policy \\  --policy-name k8s-asg-policy \\  --policy-document file://~/environment/cluster-autoscaler/k8s-asg-policy.json Finally, create an IAM role for the cluster-autoscaler Service Account in the kube-system namespace.\neksctl create iamserviceaccount \\  --name cluster-autoscaler \\  --namespace kube-system \\  --cluster eksworkshop-eksctl \\  --attach-policy-arn \u0026#34;arn:aws:iam::${ACCOUNT_ID}:policy/k8s-asg-policy\u0026#34; \\  --approve \\  --override-existing-serviceaccounts Make sure your service account with the ARN of the IAM role is annotated\nkubectl -n kube-system describe sa cluster-autoscaler Output\nName: cluster-autoscaler Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: eks.amazonaws.com/role-arn: arn:aws:iam::197520326489:role/eksctl-eksworkshop-eksctl-addon-iamserviceac-Role1-12LNPCGBD6IPZ Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: cluster-autoscaler-token-vfk8n Tokens: cluster-autoscaler-token-vfk8n Events: \u0026lt;none\u0026gt;  Deploy the Cluster Autoscaler (CA) Deploy the Cluster Autoscaler to your cluster with the following command.\nkubectl apply -f https://www.eksworkshop.com/beginner/080_scaling/deploy_ca.files/cluster-autoscaler-autodiscover.yaml To prevent CA from removing nodes where its own pod is running, we will add the cluster-autoscaler.kubernetes.io/safe-to-evict annotation to its deployment with the following command\nkubectl -n kube-system \\  annotate deployment.apps/cluster-autoscaler \\  cluster-autoscaler.kubernetes.io/safe-to-evict=\u0026#34;false\u0026#34; Finally let\u0026rsquo;s update the autoscaler image\n# we need to retrieve the latest docker image available for our EKS version export K8S_VERSION=$(kubectl version --short | grep \u0026#39;Server Version:\u0026#39; | sed \u0026#39;s/[^0-9.]*\\([0-9.]*\\).*/\\1/\u0026#39; | cut -d. -f1,2) export AUTOSCALER_VERSION=$(curl -s \u0026#34;https://api.github.com/repos/kubernetes/autoscaler/releases\u0026#34; | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | sed -s \u0026#39;s/.*-\\([0-9][0-9\\.]*\\).*/\\1/\u0026#39; | grep -m1 ${K8S_VERSION}) kubectl -n kube-system \\  set image deployment.apps/cluster-autoscaler \\  cluster-autoscaler=us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v${AUTOSCALER_VERSION} Watch the logs\nkubectl -n kube-system logs -f deployment/cluster-autoscaler We are now ready to scale our cluster\n  Related files   cluster-autoscaler-autodiscover.yaml  (4 ko)    "
},
{
	"uri": "/intermediate/230_logging/deploy/",
	"title": "Deploy Fluent Bit",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s start by downloading the fluentbit.yaml deployment file and replace some variables.\ncd ~/environment/logging # get the Elasticsearch Endpoint export ES_ENDPOINT=$(aws es describe-elasticsearch-domain --domain-name ${ES_DOMAIN_NAME} --output text --query \u0026#34;DomainStatus.Endpoint\u0026#34;) curl -Ss https://www.eksworkshop.com/intermediate/230_logging/deploy.files/fluentbit.yaml \\  | envsubst \u0026gt; ~/environment/logging/fluentbit.yaml Explore the file to see what will be deployed. The fluent bit log agent configuration is located in the Kubernetes ConfigMap and will be deployed as a DaemonSet, i.e. one pod per worker node. In our case, a 3 node cluster is used and so 3 pods will be shown in the output when we deploy.\nkubectl apply -f ~/environment/logging/fluentbit.yaml Wait for all of the pods to change to running status\nkubectl --namespace=logging get pods Output\nNAME READY STATUS RESTARTS AGE fluent-bit-2wrs4 1/1 Running 0 6s fluent-bit-9lkkm 1/1 Running 0 6s fluent-bit-x545p 1/1 Running 0 6s  "
},
{
	"uri": "/beginner/060_helm/helm_micro/deploy/",
	"title": "Deploy the eksdemo Chart",
	"tags": [],
	"description": "",
	"content": "Use the dry-run flag to test our templates To test the syntax and validity of the Chart without actually deploying it, we\u0026rsquo;ll use the --dry-run flag.\nThe following command will build and output the rendered templates without installing the Chart:\nhelm install --debug --dry-run workshop ~/environment/eksdemo Confirm that the values created by the template look correct.\nDeploy the chart Now that we have tested our template, let\u0026rsquo;s install it.\nhelm install workshop ~/environment/eksdemo Similar to what we saw previously in the nginx Helm Chart example, an output of the command will contain the information about the deployment status, revision, namespace, etc, similar to:\nNAME: workshop LAST DEPLOYED: Sat Jul 17 08:47:32 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None  In order to review the underlying services, pods and deployments, run:\nkubectl get svc,po,deploy "
},
{
	"uri": "/intermediate/210_jenkins/loggingin/",
	"title": "Logging In",
	"tags": [],
	"description": "",
	"content": "Now that we have the ELB address of your jenkins instance we can go an navigate to that address in another window.\nFrom here we can log in using:\n   Username Password     admin command from below    printf $(kubectl get secret --namespace default cicd-jenkins -o jsonpath=\u0026#34;{.data.jenkins-admin-password}\u0026#34; | base64 --decode);echo The output of this command will give you the default password for your admin user. Log into the jenkins login screen using these credentials.\n"
},
{
	"uri": "/beginner/170_statefulset/testfailure/",
	"title": "Test Failure",
	"tags": [],
	"description": "",
	"content": "Unhealthy container MySQL container uses readiness probe by running mysql -h 127.0.0.1 -e \u0026lsquo;SELECT 1\u0026rsquo; on the server to make sure MySQL server is still active. Open a new terminal and simulate MySQL as being unresponsive by following command.\nkubectl -n mysql exec mysql-1 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off This command renames the /usr/bin/mysql command so that readiness probe can\u0026rsquo;t find it. During the next health check, the pod should report one of it\u0026rsquo;s containers is not healthy. This can be verified by following command.\nkubectl -n mysql get pod mysql-1 NAME READY STATUS RESTARTS AGE mysql-1 1/2 Running 0 12m  mysql-read load balancer detects failures and takes action by not sending traffic to the failed container, @@server_id 101. You can check this by the loop running in the separate window from previous section. The loop shows the following output. \u0026#43;-------------\u0026#43;---------------------\u0026#43; | @@server_id | NOW() | \u0026#43;-------------\u0026#43;---------------------\u0026#43; | 100 | 2020-01-25 17:32:19 | \u0026#43;-------------\u0026#43;---------------------\u0026#43; \u0026#43;-------------\u0026#43;---------------------\u0026#43; | @@server_id | NOW() | \u0026#43;-------------\u0026#43;---------------------\u0026#43; | 100 | 2020-01-25 17:32:20 | \u0026#43;-------------\u0026#43;---------------------\u0026#43; \u0026#43;-------------\u0026#43;---------------------\u0026#43; | @@server_id | NOW() | \u0026#43;-------------\u0026#43;---------------------\u0026#43; | 100 | 2020-01-25 17:32:21 | \u0026#43;-------------\u0026#43;---------------------\u0026#43;  Revert back to its initial state at the previous terminal.\nkubectl -n mysql exec mysql-1 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql Check the status again to see that both containers are running and healthy\nkubectl -n mysql get pod mysql-1 NAME READY STATUS RESTARTS AGE mysql-1 2/2 Running 0 5h  The loop in another terminal is now showing @@server_id 101 is back and all three servers are running. Press Ctrl+C to stop watching.\nFailed pod To simulate a failed pod, delete mysql-1 pod by following command.\nkubectl -n mysql delete pod mysql-1 pod \u0026#34;mysql-1\u0026#34; deleted  StatefulSet controller recognizes failed pod and creates a new one to maintain the number of replicas with the same name and link to the same PersistentVolumeClaim.\nkubectl -n mysql get pod mysql-1 -w NAME READY STATUS RESTARTS AGE mysql-1 2/2 Terminating 0 15m mysql-1 0/2 Terminating 0 16m mysql-1 0/2 Terminating 0 16m mysql-1 0/2 Terminating 0 16m mysql-1 0/2 Pending 0 0s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Init:0/2 0 0s mysql-1 0/2 Init:1/2 0 11s mysql-1 0/2 PodInitializing 0 12s mysql-1 1/2 Running 0 13s mysql-1 2/2 Running 0 18s  Press Ctrl+C to stop watching.\n "
},
{
	"uri": "/intermediate/265_spinnaker_eks/configure_artifact/",
	"title": "Artifact Configuration",
	"tags": [],
	"description": "",
	"content": "Lets configure all the artifacts and storage for Spinnaker services that we will need for our usecase. We will adding all the configuration to the file located at deploy/spinnaker/basic/spinnakerservice.yml which got created by Spinnaker Operator install in previous chapter.\nConfigure Spinnaker Release Version Pick a release from https://spinnaker.io/community/releases/versions/ and export that version. Below we are using the latest Spinnaker release when this workshop was written,\nexport SPINNAKER_VERSION=1.25.4 Open the SpinnakerService manifest located at deploy/spinnaker/basic/spinnakerservice.yml, and change below for Spinnaker Version version: $SPINNAKER_VERSION # the version of Spinnaker to be deployed  Configure S3 Artifact We will configure Spinnaker to access an bucket as a source of artifacts. Spinnaker stages such as a Deploy Manifest read configuration from S3 files directly. Lets enable S3 as an artifact source.\nSpinnaker requires an external storage provider for persisting our Application settings and configured Pipelines. In this workshop we will be using S3 as a storage source means that Spinnaker will store all of its persistent data in a Bucket.\n  Create S3 Bucket first\nYou can create S3 bucket either using Admin Console or using AWS CLI (Use one of the option from below)\n  Using Admin Console\nGo to AWS Console \u0026raquo;\u0026gt; S3 and create the bucket as below\n    Using AWS CLI\nexport S3_BUCKET=spinnaker-workshop-$(cat /dev/urandom | LC_ALL=C tr -dc \u0026quot;[:alpha:]\u0026quot; | tr '[:upper:]' '[:lower:]' | head -c 10) aws s3 mb s3://$S3_BUCKET aws s3api put-public-access-block \\ --bucket $S3_BUCKET \\ --public-access-block-configuration \u0026quot;BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true\u0026quot; echo $S3_BUCKET     Set up environment variables\n  AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are the AWS profile credentials for the user who has created the above S3 bucket.\nexport S3_BUCKET=\u0026lt;your_s3_bucket\u0026gt; export AWS_ACCESS_KEY_ID=\u0026lt;your_access_key\u0026gt; export AWS_SECRET_ACCESS_KEY=\u0026lt;your_secret_access_key\u0026gt;   Configure persistentStorage  Open the SpinnakerService manifest located at deploy/spinnaker/basic/spinnakerservice.yml, then update the section spec.spinnakerConfig.config as below.\npersistentStorage: persistentStoreType: s3 s3: bucket: $S3_BUCKET rootFolder: front50 region: $AWS_REGION accessKeyId: $AWS_ACCESS_KEY_ID secretAccessKey: $AWS_SECRET_ACCESS_KEY  Configure ECR Artifact Amazon ECR requires access tokens to access the images and those access tokens expire after a time. In order to automate updating the token, use a sidecar container with a script that does it for you. Since both Clouddriver and the sidecar container need access to the ECR access token, we will use a shared volume to store the access token.\nThe sidecar needs to be able to request an access token from ECR. The Spinnaker installation must have the AmazonEC2ContainerRegistryReadOnly policy attached to the role assigned in order to request and update the required access token.\n Create ECR Repository  Clone Application Git Repo\ncd ~/environment git clone https://github.com/aws-containers/eks-microservice-demo.git cd eks-microservice-demo We need to push a test container image to the newly created ECR repository. The resaon being, empty ECR respository does not show up in the Spinnaker UI when we set up the trigger in pipeline.\nexport ECR_REPOSITORY=eks-microservice-demo/test aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com aws ecr describe-repositories --repository-name $ECR_REPOSITORY \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || \\ aws ecr create-repository --repository-name $ECR_REPOSITORY \u0026gt;/dev/null TARGET=$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPOSITORY:latest docker build -t $TARGET apps/detail docker push $TARGET  Create a configmap  Below we are creating the spinnaker namespace where all the Spinnaker services will be deployed and also creating configmap for ECR token.\nkubectl create ns spinnaker cat \u0026lt;\u0026lt; EOF \u0026gt; config.yaml interval: 30m # defines refresh interval registries: # list of registries to refresh - registryId: \u0026#34;$ACCOUNT_ID\u0026#34; region: \u0026#34;$AWS_REGION\u0026#34; passwordFile: \u0026#34;/etc/passwords/my-ecr-registry.pass\u0026#34; EOF kubectl -n spinnaker create configmap token-refresh-config --from-file config.yaml namespace/spinnaker created configmap/token-refresh-config created  Confirm if configmap is created correctly\nkubectl describe configmap token-refresh-config -n spinnaker  Add a sidecar for token refresh  Open the SpinnakerService manifest located under deploy/spinnaker/basic/spinnakerservice.yml, then add the below snippet under spec.spinnakerConfig.config.\ndeploymentEnvironment: sidecars: spin-clouddriver: - name: token-refresh dockerImage: quay.io/skuid/ecr-token-refresh:latest mountPath: /etc/passwords configMapVolumeMounts: - configMapName: token-refresh-config mountPath: /opt/config/ecr-token-refresh   Define an ECR Registry  Open the SpinnakerService manifest located under deploy/spinnaker/basic/spinnakerservice.yml, then add the below section under spec.spinnakerConfig.\nprofiles: clouddriver: dockerRegistry: enabled: true primaryAccount: my-ecr-registry accounts: - name: my-ecr-registry address: https://$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com username: AWS passwordFile: /etc/passwords/my-ecr-registry.pass trackDigests: true repositories: - $ECR_REPOSITORY  Configure Igor Igor is a is a wrapper API that provides a single point of integration with Continuous Integration (CI) and Source Control Management (SCM) services for Spinnaker. It is responsible for kicking-off jobs and reporting the state of running or completing jobs.\nClouddriver can be configured to poll the ECR registries. When that is the case, igor can then create a poller that will list the registries indexed by clouddriver, check each one for new images and submit events to echo (hence allowing Docker triggers)\nOpen the SpinnakerService manifest located under deploy/spinnaker/basic/spinnakerservice.yml, then add the below section to spec.spinnakerConfig.profiles.\nigor: docker-registry: enabled: true  Add GitHub Repository  Set up environment variables  export GITHUB_USER=\u0026lt;your_github_username\u0026gt; export GITHUB_TOKEN=\u0026lt;your_github_accesstoken\u0026gt;   Configure GitHub  To access a GitHub repo as a source of artifacts. If you actually want to use a file from the GitHub commit in your pipeline, you‚Äôll need to configure GitHub as an artifact source in Spinnaker.\nOpen the SpinnakerService manifest located under deploy/spinnaker/basic/spinnakerservice.yml, then add the below under section spec.spinnakerConfig.config.\nfeatures: artifacts: true artifacts: github: enabled: true accounts: - name: $GITHUB_USER token: $GITHUB_TOKEN # GitHub\u0026#39;s personal access token. This fields supports `encrypted` references to secrets.  "
},
{
	"uri": "/intermediate/310_opa_gatekeeper/cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Clean up steps kubectl delete -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml Make sure that all the CRD were deleted:\nkubectl delete crd \\  configs.config.gatekeeper.sh \\  constraintpodstatuses.status.gatekeeper.sh \\  constrainttemplatepodstatuses.status.gatekeeper.sh \\  constrainttemplates.templates.gatekeeper.sh "
},
{
	"uri": "/intermediate/290_argocd/deploy_application/",
	"title": "Deploy an application",
	"tags": [],
	"description": "",
	"content": "We now have an ArgoCD fully deployed, we will now deploy an application (ecsdemo-nodejs).\nFork application repository First step is to create a fork for the Github application we will deploy.\nLogin to github, go to: https://github.com/brentley/ecsdemo-nodejs.git and Fork the repo Then into your select the https URL by clicking into button Clone or download: This URL will be needed when we will configure the application into ArgoCD.\nCreate application Connect with ArgoCD CLI using our cluster context:\nCONTEXT_NAME=`kubectl config view -o jsonpath='{.current-context}'` argocd cluster add $CONTEXT_NAME  ArgoCD provides multicluster deployment functionalities. For the purpose of this workshop we will only deploy on the local cluster.\n Configure the application and link to your fork (replace the GITHUB_USERNAME):\nkubectl create namespace ecsdemo-nodejs argocd app create ecsdemo-nodejs --repo https://github.com/GITHUB_USERNAME/ecsdemo-nodejs.git --path kubernetes --dest-server https://kubernetes.default.svc --dest-namespace ecsdemo-nodejs Application is now setup, let\u0026rsquo;s have a look at the deployed application state:\nargocd app get ecsdemo-nodejs You should have this output:\nHealth Status: Missing GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE _ Service ecsdemo-nodejs ecsdemo-nodejs OutOfSync Missing apps Deployment default ecsdemo-nodejs OutOfSync Missing We can see that the application is in an OutOfSync status since the application has not been deployed yet. We are now going to sync our application:\nargocd app sync ecsdemo-nodejs After a couple of minutes our application should be synchronized.\nGROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE _ Service ecsdemo-nodejs ecsdemo-nodejs Synced Healthy service/ecsdemo-nodejs created apps Deployment default ecsdemo-nodejs Synced Healthy deployment.apps/ecsdemo-nodejs created "
},
{
	"uri": "/intermediate/260_weave_flux/deploymenthelm/",
	"title": "Deploy from Helm",
	"tags": [],
	"description": "",
	"content": "You can use this same approach to deploy Helm charts. These charts can exist within the configuration Git repository (k8s-config), or hosted from an external chart repository. In this example we will use an external chart to keep things simple.\nIn your k8s-config directory, create a namespace manifest.\nThe git pull command ensures we have the latest configuration in case Flux modified anything.\n cd ../k8s-config git pull cat \u0026lt;\u0026lt; EOF \u0026gt; namespaces/nginx.yaml apiVersion: v1 kind: Namespace metadata: labels: name: nginx name: nginx EOF Now create a Helm release manifest. This is a custom resource definition provided by Weave Flux.\ncat \u0026lt;\u0026lt; EOF \u0026gt; releases/nginx.yaml --- apiVersion: helm.fluxcd.io/v1 kind: HelmRelease metadata: name: mywebserver namespace: nginx annotations: flux.weave.works/automated: \u0026quot;true\u0026quot; flux.weave.works/tag.nginx: semver:~1.16 flux.weave.works/locked: 'true' flux.weave.works/locked_msg: '\u0026quot;Halt updates for now\u0026quot;' flux.weave.works/locked_user: User Name \u0026lt;user@example.com\u0026gt; spec: releaseName: mywebserver chart: repository: https://charts.bitnami.com/bitnami/ name: nginx version: 5.0.0 values: usePassword: true image: registry: docker.io repository: bitnami/nginx tag: 1.16.0-debian-9-r46 service: type: LoadBalancer port: 80 nodePorts: http: \u0026quot;\u0026quot; externalTrafficPolicy: Cluster ingress: enabled: false livenessProbe: httpGet: path: / port: http initialDelaySeconds: 30 timeoutSeconds: 5 failureThreshold: 6 readinessProbe: httpGet: path: / port: http initialDelaySeconds: 5 timeoutSeconds: 3 periodSeconds: 5 metrics: enabled: false EOF You will notice a few additional annotations above.\n flux.weave.works/locked tells Flux to lock the deployment so a new image version will not be deployed. flux.weave.works/tag.nginx filters the images available by semantic versioning.  Now commit the changes and wait up to 5 minutes for Flux to pull in the configuration.\ngit add . git commit -am \u0026quot;Adding nginx helm release\u0026quot; git push Verify the deployment as follows.\nUse your pod name below for kubectl logs\n kubectl get pods -n flux kubectl logs flux-5bd7fb6bb6-4sc78 -n flux helm list kubectl get all -n nginx If this doesn\u0026rsquo;t deploy, check to ensure helm was initialized. Also, look at the Flux Helm operator to see if there are any errors.\nkubectl get pods -n flux kubectl logs flux-helm-operator-df5746688-84kw8 -n flux You\u0026rsquo;ve now seen how Weave Flux can enable a GitOps approach to deployment.\n"
},
{
	"uri": "/intermediate/200_migrate_to_eks/expose-counter-app-kind/",
	"title": "Expose counter app from kind",
	"tags": [],
	"description": "",
	"content": "An app that\u0026rsquo;s not exposed isn\u0026rsquo;t very useful. We\u0026rsquo;ll manually create a load balancer to expose the app.\nThe first thing you need to do is get your local computer\u0026rsquo;s public ip address. Open a new browser tab and to icanhasip.com and copy the IP address.\nGo back to the Cloud9 shell and save that IP address as an environment variable\nexport PUBLIC_IP=#YOUR PUBLIC IP Now allow your IP address to access port 80 of your Cloud9 instance\u0026rsquo;s security group and allow traffic on the security group for a load balancer.\naws ec2 authorize-security-group-ingress \\  --group-id $SECURITY_GROUP \\  --protocol tcp \\  --port 80 \\  --cidr ${PUBLIC_IP}/25 aws ec2 authorize-security-group-ingress \\  --group-id $SECURITY_GROUP \\  --protocol -1 \\  --source-group $SECURITY_GROUP Create an Application Load Balancer (ALB) in the same security group and subnet. An ALB needs to be spread across a minimum of two subnets.\nexport ALB_ARN=$(aws elbv2 create-load-balancer \\  --name counter \\  --subnets $(aws ec2 describe-subnets \\  --filters \u0026#34;Name=vpc-id,Values=$VPC\u0026#34; \\  --query \u0026#39;Subnets[*].SubnetId\u0026#39; \\  --output text) \\  --type application --security-groups $SECURITY_GROUP \\  --query \u0026#39;LoadBalancers[0].LoadBalancerArn\u0026#39; \\  --output text) Create a target group\nexport TG_ARN=$(aws elbv2 create-target-group \\  --name counter-target --protocol HTTP \\  --port 30000 --target-type instance \\  --vpc-id ${VPC} --query \u0026#39;TargetGroups[0].TargetGroupArn\u0026#39; \\  --output text) Register our node to the TG\naws elbv2 register-targets \\  --target-group-arn ${TG_ARN} \\  --targets Id=${INSTANCE_ID} Create a listener and default action\naws elbv2 wait load-balancer-available \\  --load-balancer-arns $ALB_ARN \\  \u0026amp;\u0026amp; export ALB_LISTENER=$(aws elbv2 create-listener \\  --load-balancer-arn ${ALB_ARN} \\  --port 80 --protocol HTTP \\  --default-actions Type=forward,TargetGroupArn=${TG_ARN} \\  --query \u0026#39;Listeners[0].ListenerArn\u0026#39; \\  --output text) Our local counter app should now be exposed from the ALB\necho \u0026#34;http://\u0026#34;$(aws elbv2 describe-load-balancers \\  --load-balancer-arns $ALB_ARN \\  --query \u0026#39;LoadBalancers[0].DNSName\u0026#39; --output text) Make sure you click the button a lot because that\u0026rsquo;s the important data we\u0026rsquo;re going to migrate to EKS later.\n"
},
{
	"uri": "/intermediate/300_cis_eks_benchmark/debug-mode/",
	"title": "Module 3: Run kube-bench in debug mode",
	"tags": [],
	"description": "",
	"content": "Create a job file Create a job file named job-debug-eks.yaml using the command below.\ncat \u0026lt;\u0026lt; EOF \u0026gt; job-debug-eks.yaml --- apiVersion: batch/v1 kind: Job metadata: name: kube-bench-debug spec: template: spec: hostPID: true containers: - name: kube-bench image: aquasec/kube-bench:latest command: [\u0026#34;kube-bench\u0026#34;, \u0026#34;-v\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;--logtostderr\u0026#34;, \u0026#34;--benchmark\u0026#34;, \u0026#34;eks-1.0\u0026#34;] volumeMounts: - name: var-lib-kubelet mountPath: /var/lib/kubelet readOnly: true - name: etc-systemd mountPath: /etc/systemd readOnly: true - name: etc-kubernetes mountPath: /etc/kubernetes readOnly: true restartPolicy: Never volumes: - name: var-lib-kubelet hostPath: path: \u0026#34;/var/lib/kubelet\u0026#34; - name: etc-systemd hostPath: path: \u0026#34;/etc/systemd\u0026#34; - name: etc-kubernetes hostPath: path: \u0026#34;/etc/kubernetes\u0026#34; EOF Run the job on your cluster Run the kube-bench job on a pod in your cluster using the command below.\nkubectl apply -f job-debug-eks.yaml View job assessment results Find the pod that was created. It should be in the default namespace.\nkubectl get pods --all-namespaces Retrieve the value of this pod and the output report. Note the pod name will be different for your environment.\nkubectl logs kube-bench-debug-\u0026lt;value\u0026gt; Output I0715 04:29:42.035103 885 common.go:299] Kubernetes version: \u0026quot;\u0026quot; to Benchmark version: \u0026quot;eks-1.0\u0026quot; I0715 04:29:42.035137 885 common.go:299] Kubernetes version: \u0026quot;\u0026quot; to Benchmark version: \u0026quot;eks-1.0\u0026quot; I0715 04:29:42.035144 885 util.go:128] Looking for config specific CIS version \u0026quot;eks-1.0\u0026quot; I0715 04:29:42.035151 885 util.go:132] Looking for file: cfg/eks-1.0/master.yaml I0715 04:29:42.035199 885 common.go:240] Using config file: cfg/eks-1.0/config.yaml I0715 04:29:42.035208 885 common.go:315] Checking if the current node is running master components I0715 04:29:42.035229 885 util.go:81] ps - proc: \u0026quot;kube-apiserver\u0026quot; I0715 04:29:42.039926 885 util.go:53] [/bin/ps -C kube-apiserver -o cmd --no-headers]: exit status 1 I0715 04:29:42.039936 885 util.go:88] ps - returning: \u0026quot;\u0026quot; I0715 04:29:42.039961 885 util.go:229] verifyBin - lines(1) I0715 04:29:42.039967 885 util.go:231] reFirstWord.Match() I0715 04:29:42.053377 885 util.go:261] executable 'apiserver' not running W0715 04:29:42.053395 885 util.go:108] Unable to detect running programs for component \u0026quot;apiserver\u0026quot; The following \u0026quot;master node\u0026quot; programs have been searched, but none of them have been found: - kube-apiserver - hyperkube apiserver - hyperkube kube-apiserver - apiserver These program names are provided in the config.yaml, section 'master.apiserver.bins' I0715 04:29:42.053409 885 common.go:324] unable to detect running programs for component \u0026quot;apiserver\u0026quot; I0715 04:29:42.053437 885 root.go:91] == Running node checks == I0715 04:29:42.053443 885 common.go:299] Kubernetes version: \u0026quot;\u0026quot; to Benchmark version: \u0026quot;eks-1.0\u0026quot; I0715 04:29:42.053450 885 util.go:128] Looking for config specific CIS version \u0026quot;eks-1.0\u0026quot; I0715 04:29:42.053526 885 common.go:240] Using config file: cfg/eks-1.0/config.yaml I0715 04:29:42.053565 885 common.go:80] Using test file: cfg/eks-1.0/node.yaml I0715 04:29:42.053587 885 util.go:81] ps - proc: \u0026quot;hyperkube\u0026quot; I0715 04:29:42.057268 885 util.go:53] [/bin/ps -C hyperkube -o cmd --no-headers]: exit status 1 I0715 04:29:42.057279 885 util.go:88] ps - returning: \u0026quot;\u0026quot; I0715 04:29:42.057323 885 util.go:229] verifyBin - lines(1) I0715 04:29:42.057332 885 util.go:231] reFirstWord.Match() I0715 04:29:42.057337 885 util.go:261] executable 'hyperkube kubelet' not running I0715 04:29:42.057343 885 util.go:81] ps - proc: \u0026quot;kubelet\u0026quot; I0715 04:29:42.061305 885 util.go:88] ps - returning: \u0026quot;/usr/bin/kubelet --cloud-provider aws --config /etc/kubernetes/kubelet/kubelet-config.json --kubeconfig /var/lib/kubelet/kubeconfig --container-runtime docker --network-plugin cni --node-ip=192.168.84.9 --pod-infra-container-image=602401143452.dkr.ecr.us-west-2.amazonaws.com/eks/pause-amd64:3.1 --node-labels=alpha.eksctl.io/cluster-name=eksworkshop-eksctl,alpha.eksctl.io/nodegroup-name=nodegroup,eks.amazonaws.com/nodegroup=nodegroup,eks.amazonaws.com/nodegroup-image=ami-03cb83c4dfe25bd99\\n\u0026quot; I0715 04:29:42.061341 885 util.go:229] verifyBin - lines(2) I0715 04:29:42.061356 885 util.go:231] reFirstWord.Match(/usr/bin/kubelet --cloud-provider aws --config /etc/kubernetes/kubelet/kubelet-config.json --kubeconfig /var/lib/kubelet/kubeconfig --container-runtime docker --network-plugin cni --node-ip=192.168.84.9 --pod-infra-container-image=602401143452.dkr.ecr.us-west-2.amazonaws.com/eks/pause-amd64:3.1 --node-labels=alpha.eksctl.io/cluster-name=eksworkshop-eksctl,alpha.eksctl.io/nodegroup-name=nodegroup,eks.amazonaws.com/nodegroup=nodegroup,eks.amazonaws.com/nodegroup-image=ami-03cb83c4dfe25bd99) I0715 04:29:42.065192 885 util.go:195] Using default config file name '/etc/kubernetes/config' for component kubernetes I0715 04:29:42.065212 885 util.go:202] Component kubelet uses service file '/etc/systemd/system/kubelet.service' \u0026quot;{\\n \\\u0026quot;kind\\\u0026quot;: \\\u0026quot;KubeletConfiguration\\\u0026quot;,\\n \\\u0026quot;apiVersion\\\u0026quot;: \\\u0026quot;kubelet.config.k8s.io/v1beta1\\\u0026quot;,\\n \\\u0026quot;address\\\u0026quot;: \\\u0026quot;0.0.0.0\\\u0026quot;,\\n \\\u0026quot;authentication\\\u0026quot;: {\\n \\\u0026quot;anonymous\\\u0026quot;: {\\n \\\u0026quot;enabled\\\u0026quot;: false\\n },\\n \\\u0026quot;webhook\\\u0026quot;: {\\n \\\u0026quot;cacheTTL\\\u0026quot;: \\\u0026quot;2m0s\\\u0026quot;,\\n \\\u0026quot;enabled\\\u0026quot;: true\\n },\\n \\\u0026quot;x509\\\u0026quot;: {\\n \\\u0026quot;clientCAFile\\\u0026quot;: \\\u0026quot;/etc/kubernetes/pki/ca.crt\\\u0026quot;\\n }\\n },\\n \\\u0026quot;authorization\\\u0026quot;: {\\n \\\u0026quot;mode\\\u0026quot;: \\\u0026quot;Webhook\\\u0026quot;,\\n \\\u0026quot;webhook\\\u0026quot;: {\\n \\\u0026quot;cacheAuthorizedTTL\\\u0026quot;: \\\u0026quot;5m0s\\\u0026quot;,\\n \\\u0026quot;cacheUnauthorizedTTL\\\u0026quot;: \\\u0026quot;30s\\\u0026quot;\\n }\\n },\\n \\\u0026quot;clusterDomain\\\u0026quot;: \\\u0026quot;cluster.local\\\u0026quot;,\\n \\\u0026quot;hairpinMode\\\u0026quot;: \\\u0026quot;hairpin-veth\\\u0026quot;,\\n \\\u0026quot;readOnlyPort\\\u0026quot;: 0,\\n \\\u0026quot;cgroupDriver\\\u0026quot;: \\\u0026quot;cgroupfs\\\u0026quot;,\\n \\\u0026quot;cgroupRoot\\\u0026quot;: \\\u0026quot;/\\\u0026quot;,\\n \\\u0026quot;featureGates\\\u0026quot;: {\\n \\\u0026quot;RotateKubeletServerCertificate\\\u0026quot;: true\\n },\\n \\\u0026quot;protectKernelDefaults\\\u0026quot;: true,\\n \\\u0026quot;serializeImagePulls\\\u0026quot;: false,\\n \\\u0026quot;serverTLSBootstrap\\\u0026quot;: true,\\n \\\u0026quot;tlsCipherSuites\\\u0026quot;: [\\n \\\u0026quot;TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\\\u0026quot;,\\n \\\u0026quot;TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\\\u0026quot;,\\n \\\u0026quot;TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\\\u0026quot;,\\n \\\u0026quot;TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\\\u0026quot;,\\n \\\u0026quot;TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\\\u0026quot;,\\n \\\u0026quot;TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\\\u0026quot;,\\n \\\u0026quot;TLS_RSA_WITH_AES_256_GCM_SHA384\\\u0026quot;,\\n \\\u0026quot;TLS_RSA_WITH_AES_128_GCM_SHA256\\\u0026quot;\\n ],\\n \\\u0026quot;clusterDNS\\\u0026quot;: [\\n \\\u0026quot;10.100.0.10\\\u0026quot;\\n ],\\n \\\u0026quot;evictionHard\\\u0026quot;: {\\n \\\u0026quot;memory.available\\\u0026quot;: \\\u0026quot;100Mi\\\u0026quot;,\\n \\\u0026quot;nodefs.available\\\u0026quot;: \\\u0026quot;10%\\\u0026quot;,\\n \\\u0026quot;nodefs.inodesFree\\\u0026quot;: \\\u0026quot;5%\\\u0026quot;\\n },\\n \\\u0026quot;kubeReserved\\\u0026quot;: {\\n \\\u0026quot;cpu\\\u0026quot;: \\\u0026quot;70m\\\u0026quot;,\\n \\\u0026quot;ephemeral-storage\\\u0026quot;: \\\u0026quot;1Gi\\\u0026quot;,\\n \\\u0026quot;memory\\\u0026quot;: \\\u0026quot;574Mi\\\u0026quot;\\n },\\n \\\u0026quot;maxPods\\\u0026quot;: 29\\n}\\n\u0026quot; - Error Messages:\u0026quot;\u0026quot; I0715 04:29:42.121877 885 check.go:187] Check.ID: 3.2.11 Command: \u0026quot;/bin/cat /etc/kubernetes/kubelet/kubelet-config.json\u0026quot; TestResult: true State: \u0026quot;PASS\u0026quot; [INFO] 3 Worker Node Security Configuration [INFO] 3.1 Worker Node Configuration Files [PASS] 3.1.1 Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Scored) [PASS] 3.1.2 Ensure that the proxy kubeconfig file ownership is set to root:root (Scored) [PASS] 3.1.3 Ensure that the kubelet configuration file has permissions set to 644 or more restrictive (Scored) [PASS] 3.1.4 Ensure that the kubelet configuration file ownership is set to root:root (Scored) [INFO] 3.2 Kubelet [PASS] 3.2.1 Ensure that the --anonymous-auth argument is set to false (Scored) [PASS] 3.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored) [PASS] 3.2.3 Ensure that the --client-ca-file argument is set as appropriate (Scored) [PASS] 3.2.4 Ensure that the --read-only-port argument is set to 0 (Scored) [PASS] 3.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Scored) [PASS] 3.2.6 Ensure that the --protect-kernel-defaults argument is set to true (Scored) [PASS] 3.2.7 Ensure that the --make-iptables-util-chains argument is set to true (Scored) [PASS] 3.2.8 Ensure that the --hostname-override argument is not set (Scored) [WARN] 3.2.9 Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture (Scored) [PASS] 3.2.10 Ensure that the --rotate-certificates argument is not set to false (Scored) [PASS] 3.2.11 Ensure that the RotateKubeletServerCertificate argument is set to true (Scored) == Remediations == 3.2.9 If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service == Summary == 14 checks PASS 0 checks FAIL 1 checks WARN 0 checks INFO Cleanup  Delete the resources  kubectl delete -f job-debug-eks.yaml rm -f job-debug-eks.yaml "
},
{
	"uri": "/advanced/430_emr_on_eks/monitoring_and_logging_1/",
	"title": "Monitoring and logging Part 2 - Cloudwatch &amp; S3",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s head over to cloudwatch logs.\nopen the log group /emr-on-eks/eksworkshop-eksctl and filter stream by driver/stdout:\nNow click on the log stream and you will be able to see the output of the job.\nYou can also go to the s3 bucket that we configured for logging and see the logs.\nDownload the stdout.gz file and extract it.\nYou should be able to find its contents similar as below:\n Pi is roughly 3.138760\n "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/appmesh_installation/install_appmesh/",
	"title": "Install AWS App Mesh Controller",
	"tags": [],
	"description": "",
	"content": "Install App Mesh Helm Chart Check that Helm is installed.\nhelm list This command should either return a list of helm charts that have already been deployed or nothing.\nIf you get an error message, see installing helm for instructions.\n Add EKS Helm Repo The AWS App Mesh Controller for Kubernetes is easily installed using Helm. To get started, add the EKS Charts repository.\nhelm repo add eks https://aws.github.io/eks-charts \u0026#34;eks\u0026#34; has been added to your repositories  Install App Mesh Controller Create the namespace appmesh-system, enable OIDC and create IRSA (IAM for Service Account) for AWS App Mesh installation\n# Create the namespace kubectl create ns appmesh-system # Install the App Mesh CRDs kubectl apply -k \u0026#34;github.com/aws/eks-charts/stable/appmesh-controller//crds?ref=master\u0026#34; # Create your OIDC identity provider for the cluster eksctl utils associate-iam-oidc-provider \\  --cluster eksworkshop-eksctl \\  --approve # Download the IAM policy for AWS App Mesh Kubernetes Controller curl -o controller-iam-policy.json https://raw.githubusercontent.com/aws/aws-app-mesh-controller-for-k8s/master/config/iam/controller-iam-policy.json # Create an IAM policy called AWSAppMeshK8sControllerIAMPolicy aws iam create-policy \\  --policy-name AWSAppMeshK8sControllerIAMPolicy \\  --policy-document file://controller-iam-policy.json # Create an IAM role for the appmesh-controller service account eksctl create iamserviceaccount --cluster eksworkshop-eksctl \\  --namespace appmesh-system \\  --name appmesh-controller \\  --attach-policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSAppMeshK8sControllerIAMPolicy \\  --override-existing-serviceaccounts \\  --approve Install App Mesh Controller into the appmesh-system namespace\nhelm upgrade -i appmesh-controller eks/appmesh-controller \\  --namespace appmesh-system \\  --set region=$AWS_REGION \\  --set serviceAccount.create=false \\  --set serviceAccount.name=appmesh-controller \\  --set tracing.enabled=true \\  --set tracing.provider=x-ray Release \u0026#34;appmesh-controller\u0026#34; has been upgraded. Happy Helming! NAME: appmesh-controller LAST DEPLOYED: Wed Jan 20 21:07:01 2021 NAMESPACE: appmesh-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: AWS App Mesh controller installed!  Confirm that the controller version is¬†v1.0.0¬†or later.\nkubectl get deployment appmesh-controller \\  -n appmesh-system \\  -o json | jq -r \u0026#34;.spec.template.spec.containers[].image\u0026#34; | cut -f2 -d \u0026#39;:\u0026#39; v1.3.0  Confirm all the App Mesh CRDs are created in the Cluster\nkubectl get crds | grep appmesh gatewayroutes.appmesh.k8s.aws 2020-11-02T16:02:14Z meshes.appmesh.k8s.aws 2020-11-02T16:02:15Z virtualgateways.appmesh.k8s.aws 2020-11-02T16:02:15Z virtualnodes.appmesh.k8s.aws 2020-11-02T16:02:15Z virtualrouters.appmesh.k8s.aws 2020-11-02T16:02:15Z virtualservices.appmesh.k8s.aws 2020-11-02T16:02:15Z  Get all the resources created in appmesh-system Namespace\nkubectl -n appmesh-system get all NAME READY STATUS RESTARTS AGE pod/appmesh-controller-fcc7c4ffc-mldhk 1/1 Running 0 47s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/appmesh-controller-webhook-service ClusterIP 10.100.xx.yy \u0026lt;none\u0026gt; 443/TCP 27m √• NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/appmesh-controller 1/1 1 1 27m NAME D kubectl get crds ESIRED CURRENT READY AGE replicaset.apps/appmesh-controller-fcc7c4ffc 1 1 1 47s  Congratulations on installing the AWS App Mesh Controller in your EKS Cluster!\n"
},
{
	"uri": "/beginner/091_iam-groups/configure-aws-auth/",
	"title": "Configure Kubernetes Role Access",
	"tags": [],
	"description": "",
	"content": "Gives Access to our IAM Roles to EKS Cluster In order to give access to the IAM Roles we defined previously to our EKS cluster, we need to add specific mapRoles to the aws-auth ConfigMap\nThe Advantage of using Role to access the cluster instead of specifying directly IAM users is that it will be easier to manage: we won\u0026rsquo;t have to update the ConfigMap each time we want to add or remove users, we will just need to add or remove users from the IAM Group and we just configure the ConfigMap to allow the IAM Role associated to the IAM Group.\nUpdate the aws-auth ConfigMap to allow our IAM roles The aws-auth ConfigMap from the kube-system namespace must be edited in order to allow or delete arn Groups.\nThis file makes the mapping between IAM role and k8S RBAC rights. We can edit it manually:\nWe can edit it using eksctl :\neksctl create iamidentitymapping \\  --cluster eksworkshop-eksctl \\  --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sDev \\  --username dev-user eksctl create iamidentitymapping \\  --cluster eksworkshop-eksctl \\  --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sInteg \\  --username integ-user eksctl create iamidentitymapping \\  --cluster eksworkshop-eksctl \\  --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sAdmin \\  --username admin \\  --group system:masters It can also be used to delete entries\neksctl delete iamidentitymapping --cluster eksworkshop-eksctlv --arn arn:aws:iam::xxxxxxxxxx:role/k8sDev --username dev-user\nyou should have the config map looking something like:\nkubectl get cm -n kube-system aws-auth -o yaml apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxx:role/eksctl-eksworkshop-eksctl-nodegro-NodeInstanceRole-14TKBWBD7KWFH username: system:node:{{EC2PrivateDNSName}} - rolearn: arn:aws:iam::xxxxxxxxxx:role/k8sDev username: dev-user - rolearn: arn:aws:iam::xxxxxxxxxx:role/k8sInteg username: integ-user - groups: - system:masters rolearn: arn:aws:iam::xxxxxxxxxx:role/k8sAdmin username: admin mapUsers: | [] kind: ConfigMap  We can leverage eksctl to get a list of all identities managed in our cluster. Example:\neksctl get iamidentitymapping --cluster eksworkshop-eksctl arn:aws:iam::xxxxxxxxxx:role/eksctl-quick-nodegroup-ng-fe1bbb6-NodeInstanceRole-1KRYARWGGHPTT\tsystem:node:{{EC2PrivateDNSName}}\tsystem:bootstrappers,system:nodes arn:aws:iam::xxxxxxxxxx:role/k8sAdmin admin\tsystem:masters arn:aws:iam::xxxxxxxxxx:role/k8sDev dev-user arn:aws:iam::xxxxxxxxxx:role/k8sInteg integ-user  Here we have created:\n a RBAC role for K8sAdmin, that we map to admin user and give access to system:masters kubernetes Groups (so that it has Full Admin rights) a RBAC role for k8sDev that we map on dev-user in development namespace a RBAC role for k8sInteg that we map on integ-user in integration namespace  We will see on next section how we can test it.\n"
},
{
	"uri": "/advanced/430_emr_on_eks/monitoring_and_logging_2/",
	"title": "Monitoring and logging  Part 3 - Spark History server",
	"tags": [],
	"description": "",
	"content": "You can visit the spark history server from EMR console and look the execution details.\nNavigate to the EMR console.\nClick on View logs\nClick on App ID link\nFrom here you can navigate the spark history server to look at various metrics and details of the job.\n"
},
{
	"uri": "/020_prerequisites/kmskey/",
	"title": "Create an AWS KMS Custom Managed Key (CMK)",
	"tags": [],
	"description": "",
	"content": "Create a CMK for the EKS cluster to use when encrypting your Kubernetes secrets:\naws kms create-alias --alias-name alias/eksworkshop --target-key-id $(aws kms create-key --query KeyMetadata.Arn --output text) Let\u0026rsquo;s retrieve the ARN of the CMK to input into the create cluster command.\nexport MASTER_ARN=$(aws kms describe-key --key-id alias/eksworkshop --query KeyMetadata.Arn --output text) We set the MASTER_ARN environment variable to make it easier to refer to the KMS key later.\nNow, let\u0026rsquo;s save the MASTER_ARN environment variable into the bash_profile\necho \u0026#34;export MASTER_ARN=${MASTER_ARN}\u0026#34; | tee -a ~/.bash_profile "
},
{
	"uri": "/advanced/430_emr_on_eks/monitoring_and_logging_3/",
	"title": "Monitoring and logging  Part 4 - Prometheus and Grafana",
	"tags": [],
	"description": "",
	"content": "You will need to have prometheus and grafana installed before you can proceed with this section.\nYou can follow the Prometheus and Grafana sections to get the steps to install both of these.\nYou can also use Amazon Managed Service for Prometheus and Amazon Managed Service for Grafana. In order to get started with these, follow the official docs for AMP and AMG.\nOnce you have Prometheus and Grafana installed, head over to Grafana, and import the dashboard 11674.\nNow run the below job:\naws emr-containers start-job-run \\ --virtual-cluster-id=${VIRTUAL_CLUSTER_ID} \\ --name=pi-2 \\ --execution-role-arn=${EMR_ROLE_ARN} \\ --release-label=emr-6.2.0-latest \\ --job-driver='{ \u0026quot;sparkSubmitJobDriver\u0026quot;: { \u0026quot;entryPoint\u0026quot;: \u0026quot;local:///usr/lib/spark/examples/src/main/python/pi.py\u0026quot;, \u0026quot;sparkSubmitParameters\u0026quot;: \u0026quot;--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\u0026quot; } }' Once the job is started, head over to the dashboard and select the namespace spark as shown below.\nYou will be able to see the resource utilization by spark for running your job.\n"
},
{
	"uri": "/beginner/170_statefulset/testscaling/",
	"title": "Test Scaling",
	"tags": [],
	"description": "",
	"content": "More followers can be added to the MySQL Cluster to increase read capacity. This can be done by running the following command.\nkubectl -n mysql scale statefulset mysql --replicas=3 You can see the message that StatefulSet \u0026ldquo;mysql\u0026rdquo; scaled. statefulset \u0026#34;mysql\u0026#34; scaled  Watch the progress of ordered and graceful scaling.\nkubectl -n mysql rollout status statefulset mysql Waiting for 1 pods to be ready... partitioned roll out complete: 3 new pods have been updated...  It may take few minutes to launch all the pods.\n Open another terminal to check loop if you closed it.\nkubectl -n mysql run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\  bash -ic \u0026#34;while sleep 1; do mysql -h mysql-read -e \u0026#39;SELECT @@server_id,NOW()\u0026#39;; done\u0026#34; You will see 3 servers are running. \u0026#43;-------------\u0026#43;---------------------\u0026#43; | @@server_id | NOW() | \u0026#43;-------------\u0026#43;---------------------\u0026#43; | 100 | 2020-01-25 02:32:43 | \u0026#43;-------------\u0026#43;---------------------\u0026#43; \u0026#43;-------------\u0026#43;---------------------\u0026#43; | @@server_id | NOW() | \u0026#43;-------------\u0026#43;---------------------\u0026#43; | 102 | 2020-01-25 02:32:44 | \u0026#43;-------------\u0026#43;---------------------\u0026#43; \u0026#43;-------------\u0026#43;---------------------\u0026#43; | @@server_id | NOW() | \u0026#43;-------------\u0026#43;---------------------\u0026#43; | 101 | 2020-01-25 02:32:45 | \u0026#43;-------------\u0026#43;---------------------\u0026#43;  Verify if the newly deployed follower (mysql-2) have the same data set by following command.\nkubectl -n mysql run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\  mysql -h mysql-2.mysql -e \u0026#34;SELECT * FROM test.messages\u0026#34; It will show the same data that the leader has. \u0026#43;--------------------------\u0026#43; | message | \u0026#43;--------------------------\u0026#43; | hello, from mysql-client | \u0026#43;--------------------------\u0026#43;  Scale down replicas to 2 by running the following command.\nkubectl -n mysql scale statefulset mysql --replicas=2 You can see StatefulSet \u0026ldquo;mysql\u0026rdquo; scaled statefulset \u0026#34;mysql\u0026#34; scaled   Note that scale in doesn\u0026rsquo;t delete the data or PVCs attached to the pods. You have to delete them manually.\n Check scale in is completed by following command.\nkubectl -n mysql get pods -l app=mysql NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 1d mysql-1 2/2 Running 0 1d  Check data-mysql-2 PVCs still exist by following command.\nkubectl -n mysql get pvc -l app=mysql NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mysql-0 Bound pvc-fdb74a5e-ba51-4ccf-925b-e64761575059 10Gi RWO mysql-gp2 18m data-mysql-1 Bound pvc-355b9910-c446-4f66-8da6-629989a34d9a 10Gi RWO mysql-gp2 17m data-mysql-2 Bound pvc-12c304e4-2b3e-4621-8521-0dc17f41d107 10Gi RWO mysql-gp2 9m35s  Challenge By default, deleting a PersistentVolumeClaim will delete its associated persistent volume. What if you wanted to keep the volume?\nChange the reclaim policy of the PersistentVolume associated with PersistentVolumeClaim called \u0026ldquo;data-mysql-2\u0026rdquo; to \u0026ldquo;Retain\u0026rdquo;. Please see Kubernetes documentation for help\n  Expand here to see the solution   Change the reclaim policy:\nFind the PersistentVolume attached to the PersistentVolumeClaim data-mysql-2\nexport pv=$(kubectl -n mysql get pvc data-mysql-2 -o json | jq --raw-output \u0026#39;.spec.volumeName\u0026#39;) echo data-mysql-2 PersistentVolume name: ${pv} Now update the ReclaimPolicy\nkubectl -n mysql patch pv ${pv} -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;persistentVolumeReclaimPolicy\u0026#34;:\u0026#34;Retain\u0026#34;}}\u0026#39; Verify the ReclaimPolicy with this command.\nkubectl get persistentvolume Now, if you delete the PersistentVolumeClaim data-mysql-2, you can still see the EBS volume in your AWS EC2 console, with its state as \u0026ldquo;available\u0026rdquo;.\nLet\u0026rsquo;s change the reclaim policy back to \u0026ldquo;Delete\u0026rdquo; to avoid orphaned volumes:\nkubectl patch pv ${pv} -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;persistentVolumeReclaimPolicy\u0026#34;:\u0026#34;Delete\u0026#34;}}\u0026#39; unset pv    Delete data-mysql-2 with following commands.\nkubectl -n mysql delete pvc data-mysql-2 persistentvolumeclaim \u0026#34;data-mysql-2\u0026#34; deleted  "
},
{
	"uri": "/intermediate/260_weave_flux/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the GitOps with Weave Flux module.\nThis module is not used in subsequent steps, so you can remove the resources now, or at the end of the workshop.\nFirst, delete all images from the Amazon ECR Repository.\nNext, go to the CloudFormation Console and delete the stack used to deploy the image build CodePipeline\nNow, delete Weave Flux and your load balanced services\nhelm uninstall helm-operator --namespace flux helm uninstall flux --namespace flux kubectl delete namespace flux kubectl delete crd helmreleases.helm.fluxcd.io helm uninstall mywebserver -n nginx kubectl delete namespace nginx kubectl delete svc eks-example -n eks-example kubectl delete deployment eks-example -n eks-example kubectl delete namespace eks-example Optionally go to GitHub and delete your k8s-config and eks-example repositories.\nIf you are using your own account for this workshop, continue with the below steps. If doing this at an AWS event, skip the steps below.\n Remove IAM roles you previously created\naws iam delete-role-policy --role-name eksworkshop-CodePipelineServiceRole --policy-name codepipeline-access aws iam delete-role --role-name eksworkshop-CodePipelineServiceRole aws iam delete-role-policy --role-name eksworkshop-CodeBuildServiceRole --policy-name codebuild-access aws iam delete-role --role-name eksworkshop-CodeBuildServiceRole Remove the artifact bucket you previously created\nACCOUNT_ID=$(aws sts get-caller-identity | jq -r '.Account') aws s3 rb s3://eksworkshop-${ACCOUNT_ID}-codepipeline-artifacts --force "
},
{
	"uri": "/advanced/350_opentelemetry/deploy_frontend/",
	"title": "Deploy Frontend",
	"tags": [],
	"description": "",
	"content": "Deploy Frontend Now that we deployed our backend microservices, let\u0026rsquo;s go and deploy our frontend stack.\nGrab the Backend Service ELB, and deploy our Front End:\nexport API_BACKEND=$(kubectl get svc jaeger-tracing-nodejs-service --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\u0026#34;) echo -e \u0026#34;API_BACKEND: $API_BACKEND\u0026#34; envsubst \u0026lt; kubernetes/frontend/jaeger-tracing-frontend-service-deployment.yaml | sponge kubernetes/frontend/jaeger-tracing-frontend-service-deployment.yaml kubectl apply -f kubernetes/frontend/ Now we wait for all the pods to be up and running: $ kubectl get pods NAME READY STATUS RESTARTS AGE employee-mongo-67cd95c7d9-26w8x 1/1 Running 0 29m jaeger-tracing-frontend-service-5dd45d5bb-rdppb 1/1 Running 0 22m jaeger-tracing-go-service-95556f866-b67zh 1/1 Running 0 29m jaeger-tracing-java-service-74fcd985f6-9qnj4 1/1 Running 0 29m jaeger-tracing-nodejs-service-75fdff7c8c-pfpd4 1/1 Running 0 29m jaeger-tracing-python-service-6fbfbf9b8b-5dgnw 1/1 Running 0 29m jaeger-tracing-salaryamount-mysql-76cf744c75-4d477 1/1 Running 0 29m salary-grade-postgres-database-6df4676bb-dbtzh 1/1 Running 0 29m  Note: It can take a few minutes for all the pods to show a Status of Running and have a READY of 1/1\n Check the frontend\u0026rsquo;s ingress Load Balancer was provisioned:\n$ kubectl get svc jaeger-tracing-frontend-service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE front-end-lb LoadBalancer 10.100.2.16 *a9e7c954d96114324b80da103b888b79**-902876639.us-west-2.elb.amazonaws.**com* 80:31627/TCP 2m16s  Note: If you do not see an External-IP, re-run the command and wait till it appears (this can take a minute or two)\n Get the URL for demo microservice.\nexport SERVICE_IP=$(kubectl get svc jaeger-tracing-frontend-service --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\u0026#34;) echo http://${SERVICE_IP}/ Now check the website is able to load. Note: it can take a few minutes for the ELB‚Äôs DNS records to propagate. "
},
{
	"uri": "/beginner/110_irsa/iam-role-for-sa-2/",
	"title": "Specifying an IAM Role for Service Account",
	"tags": [],
	"description": "",
	"content": "In the previous step, we created the IAM role that is associated with a service account named iam-test in the cluster.\nFirst, let\u0026rsquo;s verify your service account iam-test exists\nkubectl get sa iam-test NAME SECRETS AGE iam-test 1 5m  Make sure your service account with the ARN of the IAM role is annotated\nkubectl describe sa iam-test Name: iam-test Namespace: default Labels: app.kubernetes.io/managed-by=eksctl Annotations: eks.amazonaws.com/role-arn: arn:aws:iam::40XXXXXXXX75:role/eksctl-sandbox-addon-iamserviceaccount-defau-Role1-1B37L4A1UEXYS Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: iam-test-token-zbk55 Tokens: iam-test-token-zbk55 Events: \u0026lt;none\u0026gt;  "
},
{
	"uri": "/beginner/185_bottlerocket/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Cleaning up To delete the resources used in this chapter:\nkubectl delete -f ~/environment/bottlerocket-nginx.yaml kubectl delete namespace bottlerocket-nginx eksctl delete nodegroup -f eksworkshop_bottlerocket.yaml --approve "
},
{
	"uri": "/advanced/430_emr_on_eks/autoscaling/",
	"title": "Configure Autoscaling",
	"tags": [],
	"description": "",
	"content": "EKS Cluster Autoscaler Cluster autoscaling in EKS is achieved using Cluster Autoscaler. The Kubernetes Cluster Autoscaler automatically adjusts the number of nodes in your cluster based on the resources required and execute the jobs.\nCluster Autoscaler for AWS provides integration with Auto Scaling groups.\nConfigure the ASG Lets configure the size of the Auto Scaling group of the newly deployed EKS managed nodegroup.\n# we need the ASG name export ASG_NAME=$(aws eks describe-nodegroup --cluster-name eksworkshop-eksctl --nodegroup-name emrnodegroup --query \u0026#34;nodegroup.resources.autoScalingGroups\u0026#34; --output text) # increase max capacity up to 6 aws autoscaling \\  update-auto-scaling-group \\  --auto-scaling-group-name ${ASG_NAME} \\  --min-size 3 \\  --desired-capacity 3 \\  --max-size 6 # Check new values aws autoscaling \\  describe-auto-scaling-groups \\  --auto-scaling-group-names ${ASG_NAME} \\  --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;eksworkshop-eksctl\u0026#39;]].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026#34; \\  --output table IAM roles for service accounts Before deploying cluster autoscaler, refer to IAM roles for service accounts section to add an IAM role to a Kubernetes service account.\nDeploy Cluster Autoscaler (CA) Follow the instructions in section Deploy Cluster Autoscaler for deployment of CA.\nkubectl get deployment cluster-autoscaler -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE cluster-autoscaler 1/1 1 1 3d8h  Now that we have setup Cluster Autoscale, lets test out a few ways of customizing how Spark jobs run on EMR on EKS. One of the way is to manually change Spark executor config parameters.\nFor a sample workload, lets use the following code, which creates multiple parallel threads and waits for a few seconds to test out cluster autoscaling.\ncat \u0026lt;\u0026lt; EOF \u0026gt; threadsleep.py import sys from time import sleep from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026#34;threadsleep\u0026#34;).getOrCreate() def sleep_for_x_seconds(x):sleep(x*20) sc=spark.sparkContext sc.parallelize(range(1,6), 5).foreach(sleep_for_x_seconds) spark.stop() EOF aws s3 cp threadsleep.py ${s3DemoBucket} Let\u0026rsquo;s run the same inbuilt example scripts that calculates the value of pi, but this time lets increase the number of executors to 15 by modifying spark.executor.instances.\n#Get required virtual cluster-id and role arn export VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query \u0026#34;virtualClusters[?state==\u0026#39;RUNNING\u0026#39;].id\u0026#34; --output text) export EMR_ROLE_ARN=$(aws iam get-role --role-name EMRContainers-JobExecutionRole --query Role.Arn --output text) #start spark job with start-job-run aws emr-containers start-job-run \\  --virtual-cluster-id=$VIRTUAL_CLUSTER_ID \\  --name=threadsleep-clusterautoscaler \\  --execution-role-arn=$EMR_ROLE_ARN \\  --release-label=emr-6.2.0-latest \\  --job-driver=\u0026#39;{ \u0026#34;sparkSubmitJobDriver\u0026#34;: { \u0026#34;entryPoint\u0026#34;: \u0026#34;\u0026#39;${s3DemoBucket}\u0026#39;/threadsleep.py\u0026#34;, \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.executor.instances=15 --conf spark.executor.memory=1G --conf spark.executor.cores=1 --conf spark.driver.cores=1\u0026#34; } }\u0026#39; \\  --configuration-overrides=\u0026#39;{ \u0026#34;applicationConfiguration\u0026#34;: [ { \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;spark.dynamicAllocation.enabled\u0026#34;:\u0026#34;false\u0026#34;, \u0026#34;spark.kubernetes.executor.deleteOnTermination\u0026#34;: \u0026#34;true\u0026#34; } } ] }\u0026#39; You can open up couple of terminals and use watch command to see how cluster autoscales adds additional nodes to schedule the additional executors spark job has requested.\nwatch kubectl get pods -n spark NAME READY STATUS RESTARTS AGE 00000002u939ovcug59-g7b49 2/2 Running 0 64s spark-00000002u939ovcug59-driver 2/2 Running 0 53s threadsleep-2dfae979234eaa16-exec-1 1/1 Running 0 39s threadsleep-2dfae979234eaa16-exec-2 1/1 Running 0 39s threadsleep-2dfae979234eaa16-exec-3 1/1 Running 0 39s threadsleep-2dfae979234eaa16-exec-4 1/1 Running 0 39s threadsleep-2dfae979234eaa16-exec-5 1/1 Running 0 39s  watch kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-17-228.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 17h v1.18.9-eks-d1db3c ip-192-168-18-78.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 2m58s v1.18.9-eks-d1db3c ip-192-168-35-160.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 37m v1.18.9-eks-d1db3c ip-192-168-38-13.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 2m58s v1.18.9-eks-d1db3c ip-192-168-4-98.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 15h v1.18.9-eks-d1db3c ip-192-168-57-45.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 15h v1.18.9-eks-d1db3c ip-192-168-72-198.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 48m v1.18.9-eks-d1db3c ip-192-168-74-210.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 2m58s v1.18.9-eks-d1db3c ip-192-168-79-114.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 15h v1.18.9-eks-d1db3c  You can also change CPU and memory of your Spark executors by modifying spark.executor.cores and spark.executor.memory. Learn more about it here.\nLet the job run to completion before moving ahead to next section of Dynamic Resource Allocation.\nDynamic Resource Allocation You can also optimize your jobs by using Dynamic Resource Allocation (DRA) provided by Spark. Its a mechanism to dynamically adjust the resources your application occupies based on the workload. With DRA, the spark driver spawns the initial number of executors and then scales up the number until the specified maximum number of executors is met to process the pending tasks. Idle executors are terminated when there are no pending tasks.\nIt is particularly useful if you are not familiar of your workload or want to use the flexibility of kubernetes to request resources as necesaary.\nDynamic resource allocation (DRA) is available in Spark 3 (EMR 6.x) without the need for an external shuffle service. Spark on Kubernetes doesn\u0026rsquo;t support external shuffle service as of Spark 3.1, but DRA can be achieved by enabling shuffle tracking.\nTo add DRA, we will enable it and define executor behavior in --configuration-overrides section.\n#start spark job with start-job-run aws emr-containers start-job-run \\  --virtual-cluster-id=$VIRTUAL_CLUSTER_ID \\  --name=threadsleep-dra \\  --execution-role-arn=$EMR_ROLE_ARN \\  --release-label=emr-6.2.0-latest \\  --job-driver=\u0026#39;{ \u0026#34;sparkSubmitJobDriver\u0026#34;: { \u0026#34;entryPoint\u0026#34;: \u0026#34;\u0026#39;${s3DemoBucket}\u0026#39;/threadsleep.py\u0026#34;, \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.executor.instances=1 --conf spark.executor.memory=1G --conf spark.executor.cores=1 --conf spark.driver.cores=1\u0026#34; } }\u0026#39;\\  --configuration-overrides=\u0026#39;{ \u0026#34;applicationConfiguration\u0026#34;: [ { \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;spark.dynamicAllocation.enabled\u0026#34;:\u0026#34;true\u0026#34;, \u0026#34;spark.dynamicAllocation.shuffleTracking.enabled\u0026#34;:\u0026#34;true\u0026#34;, \u0026#34;spark.dynamicAllocation.minExecutors\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;spark.dynamicAllocation.maxExecutors\u0026#34;:\u0026#34;10\u0026#34;, \u0026#34;spark.dynamicAllocation.initialExecutors\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;spark.dynamicAllocation.schedulerBacklogTimeout\u0026#34;: \u0026#34;1s\u0026#34;, \u0026#34;spark.dynamicAllocation.executorIdleTimeout\u0026#34;: \u0026#34;5s\u0026#34; } } ] }\u0026#39; You have set the spark.executor.instances to 1 and enabled DRA by setting spark.dynamicAllocation.enabled true. For testing purposes, we have kept smaller scale up and scale down timers. Learn more about them here.\nYou can open up couple of terminals and use watch command to see how DRA scales up and scales down executor instances.\nwatch kubectl get pods -n spark As executor instances are scaled up by DRA, kubernetes cluster autoscaler adds nodes to schedule those nodes.\nwatch kubectl get nodes You can also take a look at the spark history server to observe the event timeline for executors - where spark dynamically adds in executors and removes as they are not needed.\nNavigate to the Spark history server on EMR console:\necho -e \u0026#34;Go to the URL:\\nhttps://console.aws.amazon.com/elasticmapreduce/home?region=\u0026#34;${AWS_REGION}\u0026#34;#virtual-cluster-jobs:\u0026#34;${VIRTUAL_CLUSTER_ID} Click on View logs: Check the Event Pipeline: "
},
{
	"uri": "/advanced/340_appmesh_flagger/fronend_vg_setup/",
	"title": "Deploy Frontend/VirtualGateway",
	"tags": [],
	"description": "",
	"content": "Deploy the Frontend Service Since we want to visualize the automated canary deployment, we need an UI for which, we will use frontend service. This frontend service will call the backend service detail to get the vendor information. In order to expose the frontend service outside the mesh we will use AWS AppMesh VirtualGateway affiliated with Network Load Balancer. Lets deploy the frontend service.\nexport APP_VERSION=1.0 envsubst \u0026lt; flagger/frontend.yaml | kubectl apply -f - deployment.apps/frontend created service/frontend created virtualnode.appmesh.k8s.aws/frontend created virtualservice.appmesh.k8s.aws/frontend created  Deploy the AppMesh VirtualGateway helm upgrade -i appmesh-gateway eks/appmesh-gateway \\ \t--namespace flagger \\  --set serviceAccount.create=false \\  --set serviceAccount.name=flagger-envoy-proxies Release \u0026#34;appmesh-gateway\u0026#34; does not exist. Installing it now. NAME: appmesh-gateway LAST DEPLOYED: Tue Mar 23 01:58:55 2021 NAMESPACE: flagger STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: AWS App Mesh Gateway installed!  Create the GatewayRoute In this GatewayRoute, we are routing the traffic coming into the VirtualGateway to frontend VirtualService.\nkubectl apply -f flagger/gateway.yaml gatewayroute.appmesh.k8s.aws/frontend created  Get all the resources for AppMesh VirtualGateway appmesh-gateway\nkubectl get all -n flagger -o wide | grep appmesh-gateway pod/appmesh-gateway-65bc4cb47d-l6m6d 2/2 Running 0 19h 192.168.6.246 ip-192-168-19-33.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; service/appmesh-gateway LoadBalancer 10.100.235.124 a21cab1223ac9414da4c97dee2f2c245-XXXXX.elb.us-east-2.amazonaws.com 80:30249/TCP 19h app.kubernetes.io/name=appmesh-gateway deployment.apps/appmesh-gateway 1/1 1 1 19h envoy 840364872350.dkr.ecr.us-west-2.amazonaws.com/aws-appmesh-envoy:v1.15.1.0-prod app.kubernetes.io/name=appmesh-gateway replicaset.apps/appmesh-gateway-65bc4cb47d 1 1 1 19h envoy 840364872350.dkr.ecr.us-west-2.amazonaws.com/aws-appmesh-envoy:v1.15.1.0-prod app.kubernetes.io/name=appmesh-gateway,pod-template-hash=65bc4cb47d virtualgateway.appmesh.k8s.aws/appmesh-gateway arn:aws:appmesh:us-east-2:$ACCOUNT_ID:mesh/flagger/virtualGateway/appmesh-gateway_flagger 19h gatewayroute.appmesh.k8s.aws/frontend arn:aws:appmesh:us-east-2:$ACCOUNT_ID:mesh/flagger/virtualGateway/appmesh-gateway_flagger/gatewayRoute/frontend_flagger 19h  It takes 3 to 5 minutes to set up the Load Balancer.\n Testing Setup Find the AppMesh VirtualGateway public endpoint:\nexport URL=\u0026#34;http://$(kubectl -n flagger get svc/appmesh-gateway -ojson | jq -r \u0026#34;.status.loadBalancer.ingress[].hostname\u0026#34;)\u0026#34; echo $URL http://a21cab1223ac9414da4c97dee2f2c245-XXXXXXXX.elb.us-east-2.amazonaws.com  Wait for the NLB to become active:\nwatch curl -sS $URL Once the LoadBalancer is active, access the LoadBalancer endpoint in browser You can see that our frontend service is exposed via VirtualGateway using Network LoadBalancer. And this frontend service communicates with backend service detail to get the vendor information.\nCongratulations on exposing the frontend service via App Mesh VirtualGateway!\nLet‚Äôs test the Automated Canary Deployment for detail backend service.\n"
},
{
	"uri": "/beginner/115_sg-per-pod/40_sg_policy/",
	"title": "SecurityGroup Policy",
	"tags": ["beginner"],
	"description": "",
	"content": "SecurityGroup Policy A new Custom Resource Definition (CRD) has also been added automatically at the cluster creation. Cluster administrators can specify which security groups to assign to pods through the SecurityGroupPolicy CRD. Within a namespace, you can select pods based on pod labels, or based on labels of the service account associated with a pod. For any matching pods, you also define the security group IDs to be applied.\nYou can verify the CRD is present with this command.\nkubectl get crd securitygrouppolicies.vpcresources.k8s.aws Output securitygrouppolicies.vpcresources.k8s.aws 2020-11-04T17:01:27Z  The webhook watches SecurityGroupPolicy custom resources for any changes, and automatically injects matching pods with the extended resource request required for the pod to be scheduled onto a node with available branch network interface capacity. Once the pod is scheduled, the resource controller will create and attach a branch interface to the trunk interface. Upon successful attachment, the controller adds an annotation to the pod object with the branch interface details.\nNow let\u0026rsquo;s create our policy.\ncat \u0026lt;\u0026lt; EoF \u0026gt; ~/environment/sg-per-pod/sg-policy.yaml apiVersion: vpcresources.k8s.aws/v1beta1 kind: SecurityGroupPolicy metadata: name: allow-rds-access spec: podSelector: matchLabels: app: green-pod securityGroups: groupIds: - ${POD_SG} EoF As we can see, if the pod has the label app: green-pod, a security group will be attached to it.\nWe can finally deploy it in a specific namespace.\nkubectl create namespace sg-per-pod kubectl -n sg-per-pod apply -f ~/environment/sg-per-pod/sg-policy.yaml kubectl -n sg-per-pod describe securitygrouppolicy Output Name: allow-rds-access Namespace: sg-per-pod Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;vpcresources.k8s.aws/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;SecurityGroupPolicy\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;allow-rds-access\u0026#34;,\u0026#34;namespac... API Version: vpcresources.k8s.aws/v1beta1 Kind: SecurityGroupPolicy Metadata: Creation Timestamp: 2020-12-03T04:35:57Z Generation: 1 Resource Version: 9142629 Self Link: /apis/vpcresources.k8s.aws/v1beta1/namespaces/sg-per-pod/securitygrouppolicies/allow-rds-access UID: bf1e329d-816e-4ab0-abe8-934cadabfdd3 Spec: Pod Selector: Match Labels: App: green-pod Security Groups: Group Ids: sg-0ff967bc903e9639e Events: \u0026lt;none\u0026gt;  "
},
{
	"uri": "/intermediate/230_logging/kibana/",
	"title": "Kibana",
	"tags": [],
	"description": "",
	"content": "Finally Let\u0026rsquo;s log into Kibana to visualize our logs.\necho \u0026#34;Kibana URL: https://${ES_ENDPOINT}/_plugin/kibana/ Kibana user: ${ES_DOMAIN_USER}Kibana password: ${ES_DOMAIN_PASSWORD}\u0026#34; From the Kibana Welcome screen select Explore on my own\nNow click Connect to your Elasticsearch index\nAdd *fluent-bit* as the Index pattern and click Next step\nSelect @timestamp as the Time filter field name and close the Configuration window by clicking on Create index pattern\nFinally you can select Discover from the left panel and start exploring the logs\n"
},
{
	"uri": "/beginner/040_dashboard/",
	"title": "Deploy the Kubernetes Dashboard",
	"tags": ["beginner", "CON203"],
	"description": "",
	"content": "Deploy the Kubernetes Dashboard   In this Chapter, we will deploy the official Kubernetes dashboard, and connect through our Cloud9 Workspace.\n"
},
{
	"uri": "/beginner/091_iam-groups/test-cluster-access/",
	"title": "Test EKS access",
	"tags": [],
	"description": "",
	"content": "Automate assumerole with aws cli It is possible to automate the retrieval of temporary credentials for the assumed role by configuring the AWS CLI in the files ~/.aws/config and ~/.aws/credentials. As an example, we will define three profiles.\nAdd in ~/.aws/config: mkdir -p ~/.aws cat \u0026lt;\u0026lt; EoF \u0026gt;\u0026gt; ~/.aws/config [profile admin] role_arn=arn:aws:iam::${ACCOUNT_ID}:role/k8sAdmin source_profile=eksAdmin [profile dev] role_arn=arn:aws:iam::${ACCOUNT_ID}:role/k8sDev source_profile=eksDev [profile integ] role_arn=arn:aws:iam::${ACCOUNT_ID}:role/k8sInteg source_profile=eksInteg EoF Add in ~/.aws/credentials: cat \u0026lt;\u0026lt; EoF \u0026gt;\u0026gt; ~/.aws/credentials [eksAdmin] aws_access_key_id=$(jq -r .AccessKey.AccessKeyId /tmp/PaulAdmin.json) aws_secret_access_key=$(jq -r .AccessKey.SecretAccessKey /tmp/PaulAdmin.json) [eksDev] aws_access_key_id=$(jq -r .AccessKey.AccessKeyId /tmp/JeanDev.json) aws_secret_access_key=$(jq -r .AccessKey.SecretAccessKey /tmp/JeanDev.json) [eksInteg] aws_access_key_id=$(jq -r .AccessKey.AccessKeyId /tmp/PierreInteg.json) aws_secret_access_key=$(jq -r .AccessKey.SecretAccessKey /tmp/PierreInteg.json) EoF Test this with the dev profile: aws sts get-caller-identity --profile dev { \u0026#34;UserId\u0026#34;: \u0026#34;AROAUD5VMKW75WJEHFU4X:botocore-session-1581687024\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;xxxxxxxxxx\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:sts::xxxxxxxxxx:assumed-role/k8sDev/botocore-session-1581687024\u0026#34; }  The assumed-role is k8sDev, so we achieved our goal.\nWhen specifying the \u0026ndash;profile dev parameter we automatically ask for temporary credentials for the role k8sDev. You can test this with integ and admin also.\naws sts get-caller-identity --profile admin { \u0026#34;UserId\u0026#34;: \u0026#34;AROAUD5VMKW77KXQAL7ZX:botocore-session-1582022121\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;xxxxxxxxxx\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:sts::xxxxxxxxxx:assumed-role/k8sAdmin/botocore-session-1582022121\u0026#34; }   When specifying the \u0026ndash;profile admin parameter we automatically ask for temporary credentials for the role k8sAdmin\n Using AWS profiles with the Kubectl config file It is also possible to specify the AWS_PROFILE to use with the aws-iam-authenticator in the ~/.kube/config file, so that it will use the appropriate profile.\nWith dev profile Create a new KUBECONFIG file to test this:\nexport KUBECONFIG=/tmp/kubeconfig-dev \u0026amp;\u0026amp; eksctl utils write-kubeconfig eksworkshop-eksctl cat $KUBECONFIG | yq e \u0026#39;.users.[].user.exec.args += [\u0026#34;--profile\u0026#34;, \u0026#34;dev\u0026#34;]\u0026#39; - -- | sed \u0026#39;s/eksworkshop-eksctl./eksworkshop-eksctl-dev./g\u0026#39; | sponge $KUBECONFIG  Note: this assume you uses yq \u0026gt;= version 4. you can reference to this page to adapt this command for another version.\n We added the --profile dev parameter to our kubectl config file, so that this will ask kubectl to use our IAM role associated to our dev profile, and we rename the context using suffix -dev.\nWith this configuration we should be able to interact with the development namespace, because it has our RBAC role defined.\nLet\u0026rsquo;s create a pod:\nkubectl run --generator=run-pod/v1 nginx-dev --image=nginx -n development We can list the pods:\nkubectl get pods -n development NAME READY STATUS RESTARTS AGE nginx-dev 1/1 Running 0 28h  \u0026hellip; but not in other namespaces:\nkubectl get pods -n integration Error from server (Forbidden): pods is forbidden: User \u0026#34;dev-user\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;integration\u0026#34;  Test with integ profile export KUBECONFIG=/tmp/kubeconfig-integ \u0026amp;\u0026amp; eksctl utils write-kubeconfig eksworkshop-eksctl cat $KUBECONFIG | yq e \u0026#39;.users.[].user.exec.args += [\u0026#34;--profile\u0026#34;, \u0026#34;integ\u0026#34;]\u0026#39; - -- | sed \u0026#39;s/eksworkshop-eksctl./eksworkshop-eksctl-integ./g\u0026#39; | sponge $KUBECONFIG  Note: this assume you uses yq \u0026gt;= version 4. you can reference to this page to adapt this command for another version.\n Let\u0026rsquo;s create a pod:\nkubectl run --generator=run-pod/v1 nginx-integ --image=nginx -n integration We can list the pods:\nkubectl get pods -n integration NAME READY STATUS RESTARTS AGE nginx-integ 1/1 Running 0 43s  \u0026hellip; but not in other namespaces:\nkubectl get pods -n development Error from server (Forbidden): pods is forbidden: User \u0026#34;integ-user\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;development\u0026#34;  Test with admin profile export KUBECONFIG=/tmp/kubeconfig-admin \u0026amp;\u0026amp; eksctl utils write-kubeconfig eksworkshop-eksctl cat $KUBECONFIG | yq e \u0026#39;.users.[].user.exec.args += [\u0026#34;--profile\u0026#34;, \u0026#34;admin\u0026#34;]\u0026#39; - -- | sed \u0026#39;s/eksworkshop-eksctl./eksworkshop-eksctl-admin./g\u0026#39; | sponge $KUBECONFIG  Note: this assume you uses yq \u0026gt;= version 4. you can reference to this page to adapt this command for another version.\n Let\u0026rsquo;s create a pod in the default namespace:\nkubectl run --generator=run-pod/v1 nginx-admin --image=nginx We can list the pods:\nkubectl get pods NAME READY STATUS RESTARTS AGE nginx-integ 1/1 Running 0 43s  We can list ALL pods in all namespaces:\nkubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default nginx-admin 1/1 Running 0 15s development nginx-dev 1/1 Running 0 11m integration nginx-integ 1/1 Running 0 4m29s kube-system aws-node-mzbh4 1/1 Running 0 100m kube-system aws-node-p7nj7 1/1 Running 0 100m kube-system aws-node-v2kg9 1/1 Running 0 100m kube-system coredns-85bb8bb6bc-2qbx6 1/1 Running 0 105m kube-system coredns-85bb8bb6bc-87ndr 1/1 Running 0 105m kube-system kube-proxy-4n5lc 1/1 Running 0 100m kube-system kube-proxy-b65xm 1/1 Running 0 100m kube-system kube-proxy-pr7k7 1/1 Running 0 100m  Switching between different contexts It is possible to configure several Kubernetes API access keys in the same KUBECONFIG file, or just tell Kubectl to lookup several files:\nexport KUBECONFIG=/tmp/kubeconfig-dev:/tmp/kubeconfig-integ:/tmp/kubeconfig-admin There is a tool kubectx / kubens that will help manage KUBECONFIG files with several contexts:\ncurl -sSLO https://raw.githubusercontent.com/ahmetb/kubectx/master/kubectx \u0026amp;\u0026amp; chmod 755 kubectx \u0026amp;\u0026amp; sudo mv kubectx /usr/local/bin I can use kubectx to quickly list or switch Kubernetes contexts:\nkubectx i-0397aa1339e238a99@eksworkshop-eksctl-admin.eu-west-2.eksctl.io i-0397aa1339e238a99@eksworkshop-eksctl-dev.eu-west-2.eksctl.io i-0397aa1339e238a99@eksworkshop-eksctl-integ.eu-west-2.eksctl.io  Conclusion In this module, we have seen how to configure EKS to provide finer access to users combining IAM Groups and Kubernetes RBAC. You can create different groups depending on your needs, configure their associated RBAC access in your cluster, and simply add or remove users from the group to grant or revoke access to your cluster.\nUsers will only have to configure their AWS CLI in order to automatically retrieve their associated rights in your cluster.\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/virtual_gateway_setup/",
	"title": "VirtualGateway Setup",
	"tags": [],
	"description": "",
	"content": "A VirtualGateway allows resources that are outside of your mesh to communicate to resources that are inside of your mesh. The VirtualGateway represents an Envoy proxy running in an Amazon EC2 instance, Amazon ECS Service, Amazon Kubernetes Service. Unlike a VirtualNode, which represents Envoy running with an application, a VirtualGateway represents Envoy deployed by itself.\nExternal resources must be able to resolve a DNS name to an IP address assigned to the service or instance that runs Envoy. Envoy can then access all of the App Mesh configuration for resources that are inside of the mesh.\nThe configuration for handling the incoming requests at the VirtualGateway are specified using Gateway Routes. VirtualGateways are affiliated with a load balancer and allow you to configure ingress traffic rules using Routes, similar to VirtualRouter configuration.\nImage source:aws.amazon.com/blogs/containers/introducing-ingress-support-in-aws-app-mesh\n"
},
{
	"uri": "/advanced/420_kubeflow/training/",
	"title": "Model training",
	"tags": [],
	"description": "",
	"content": "Model Training While Jupyter notebook is good for interactive model training, you may like to package the training code as Docker image and run it in Amazon EKS cluster.\nThis chapter explains how to build a training model for Fashion-MNIST dataset using TensorFlow and Keras on Amazon EKS. This dataset contains 70,000 grayscale images in 10 categories and is meant to be a drop-in replace of MNIST.\nDocker image We will use a pre-built Docker image seedjeffwan/mnist_tensorflow_keras:1.13.1 for this exercise. This image uses tensorflow/tensorflow:1.13.1 as the base image. The image has training code and downloads training and test data sets. It also stores the generated model in an S3 bucket.\nAlternatively, you can use Dockerfile to build the image by using the command below. We will skip this step for now\ndocker build -t \u0026lt;dockerhub_username\u0026gt;/\u0026lt;repo_name\u0026gt;:\u0026lt;tag_name\u0026gt; .\nCreate S3 bucket Create an S3 bucket where training model will be saved:\nexport HASH=$(\u0026lt; /dev/urandom tr -dc a-z0-9 | head -c6) export S3_BUCKET=$HASH-eks-ml-data aws s3 mb s3://$S3_BUCKET --region $AWS_REGION This name will be used in the pod specification later. This bucket is also used for serving the model.\nIf you want to use an existing bucket in a different region, then make sure to specify the exact region as the value of AWS_REGION environment variable in mnist-training.yaml.\nSetup AWS credentials in EKS cluster AWS credentials are required to save model on S3 bucket. These credentials are stored in EKS cluster as Kubernetes secrets.\nCreate an IAM user \u0026lsquo;s3user\u0026rsquo;, attach S3 access policy and retrieve temporary credentials\naws iam create-user --user-name s3user aws iam attach-user-policy --user-name s3user --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess aws iam create-access-key --user-name s3user \u0026gt; /tmp/create_output.json Next, record the new user\u0026rsquo;s credentials into environment variables:\nexport AWS_ACCESS_KEY_ID_VALUE=$(jq -j .AccessKey.AccessKeyId /tmp/create_output.json | base64) export AWS_SECRET_ACCESS_KEY_VALUE=$(jq -j .AccessKey.SecretAccessKey /tmp/create_output.json | base64) Apply to EKS cluster:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aws-secret type: Opaque data: AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID_VALUE AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY_VALUE EOF Run training using pod Create pod:\ncurl -LO https://eksworkshop.com/advanced/420_kubeflow/kubeflow.files/mnist-training.yaml envsubst \u0026lt; mnist-training.yaml | kubectl create -f - This will start a pod which will start the training and save the generated model in S3 bucket. Check status:\nkubectl get pods You\u0026rsquo;ll see similar output\nNAME READY STATUS RESTARTS AGE mnist-training 1/1 Running 0 2m45s  Note: If your mnist-training fail for some reason, please copy our trained model by running the command under \u0026lsquo;Expand here to copy trained model\u0026rsquo;. This will unblock your inference experiment in the next chapter.\n   Expand here to copy trained model   aws s3 sync s3://reinvent-opn401/mnist/tf_saved_model s3://$S3_BUCKET/mnist/tf_saved_model      Expand here to see logs from successful run   kubectl logs mnist-training -f Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 1us/step 40960/29515 [=========================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step 26435584/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 16384/5148 [===============================================================================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step 4431872/4422102 [==============================] - 0s 0us/step WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. 2019-08-29 00:32:10.652905: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2019-08-29 00:32:10.659233: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300070000 Hz 2019-08-29 00:32:10.661111: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x45baf40 executing computations on platform Host. Devices: 2019-08-29 00:32:10.661139: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): \u0026lt;undefined\u0026gt;, \u0026lt;undefined\u0026gt; 2019-08-29 00:32:10.718125: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /root//.aws/config and using profilePrefix = 1 2019-08-29 00:32:10.718160: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /root//.aws/credentials and using profilePrefix = 0 2019-08-29 00:32:10.718174: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /root//.aws/credentials for credentials file and /root//.aws/config for the config file , for use with profile default 2019-08-29 00:32:10.718184: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http 2019-08-29 00:32:10.718196: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2 2019-08-29 00:32:10.718207: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000 2019-08-29 00:32:10.718224: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:10.718275: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25 2019-08-29 00:32:10.718341: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:10.718468: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2 2019-08-29 00:32:10.718490: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.036616: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:32:11.036661: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:32:11.036724: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.036807: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.204229: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.204327: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.281479: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:32:11.281513: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:32:11.281551: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.281615: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.388175: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.388285: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.550463: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.550639: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.628831: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.628915: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.709359: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.709455: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:12.017431: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:12.017573: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:12.096831: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:12.096933: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. train_images.shape: (60000, 28, 28, 1), of float64 test_images.shape: (10000, 28, 28, 1), of float64 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= Conv1 (Conv2D) (None, 13, 13, 8) 80 _________________________________________________________________ flatten (Flatten) (None, 1352) 0 _________________________________________________________________ Softmax (Dense) (None, 10) 13530 ================================================================= Total params: 13,610 Trainable params: 13,610 Non-trainable params: 0 _________________________________________________________________ Epoch 1/40 2019-08-29 00:32:16.840512: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:16.840633: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:17.280630: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:17.280744: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:17.384333: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:17.384520: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 87us/sample - loss: 0.5496 - acc: 0.8082 Epoch 2/40 2019-08-29 00:32:21.952054: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:21.952176: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:22.369041: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:22.369238: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:22.446531: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:22.446629: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.4137 - acc: 0.8548 Epoch 3/40 2019-08-29 00:32:27.021467: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:27.021592: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:27.454086: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:27.454230: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:27.534720: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:27.534816: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3763 - acc: 0.8685 Epoch 4/40 2019-08-29 00:32:32.130604: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:32.130728: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:32.517514: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:32.517630: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:32.629178: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:32.629262: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3555 - acc: 0.8746 Epoch 5/40 2019-08-29 00:32:37.235765: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:37.235889: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:37.736414: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:37.736525: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:37.813549: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:37.813632: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 86us/sample - loss: 0.3415 - acc: 0.8794 Epoch 6/40 2019-08-29 00:32:42.400365: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:42.400527: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:42.809268: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:42.809409: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:42.887120: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:42.887209: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3283 - acc: 0.8835 Epoch 7/40 2019-08-29 00:32:47.474549: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:47.474676: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:47.885577: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:47.885686: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:47.963577: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:47.963662: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3188 - acc: 0.8868 Epoch 8/40 2019-08-29 00:32:52.571365: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:52.571487: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:52.973365: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:52.973461: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:53.051547: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:53.051711: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3112 - acc: 0.8887 Epoch 9/40 2019-08-29 00:32:57.620454: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:57.620579: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:58.045196: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:58.045301: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:58.123871: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:58.123956: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3036 - acc: 0.8924 Epoch 10/40 2019-08-29 00:33:02.735621: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:02.735784: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:03.155609: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:03.155717: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:03.237484: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:03.237568: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 86us/sample - loss: 0.2964 - acc: 0.8943 Epoch 11/40 2019-08-29 00:33:07.847167: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:07.847295: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:08.308130: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:08.308233: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:08.385677: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:08.385761: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2904 - acc: 0.8966 Epoch 12/40 2019-08-29 00:33:12.989568: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:12.989709: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:13.425758: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:13.425871: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:13.503980: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:13.504066: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2850 - acc: 0.8979 Epoch 13/40 2019-08-29 00:33:18.084636: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:18.084799: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:18.505749: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:18.505889: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:18.584930: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:18.585086: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2788 - acc: 0.8994 Epoch 14/40 2019-08-29 00:33:23.165093: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:23.165216: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:23.583005: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:23.583125: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:23.660931: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:23.661017: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2743 - acc: 0.9016 Epoch 15/40 2019-08-29 00:33:28.273507: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:28.273630: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:28.656655: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:28.656805: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:28.735635: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:28.735757: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 84us/sample - loss: 0.2702 - acc: 0.9025 Epoch 16/40 2019-08-29 00:33:33.340967: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:33.341091: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:33.797569: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:33.797673: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:33.876101: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:33.876187: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 86us/sample - loss: 0.2668 - acc: 0.9032 Epoch 17/40 2019-08-29 00:33:38.485389: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:38.485516: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:38.911662: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:38.911776: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:38.990577: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:38.990673: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2627 - acc: 0.9059 Epoch 18/40 2019-08-29 00:33:43.586335: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:43.586462: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:43.982270: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:43.982444: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:44.061595: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:44.061765: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2587 - acc: 0.9072 Epoch 19/40 2019-08-29 00:33:48.666451: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:48.666582: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:49.113733: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:49.113835: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:49.191768: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:49.191853: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2542 - acc: 0.9082 Epoch 20/40 2019-08-29 00:33:53.778720: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:53.778845: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:54.275408: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:54.275506: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:54.354271: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:54.354356: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 86us/sample - loss: 0.2521 - acc: 0.9092 Epoch 21/40 2019-08-29 00:33:58.946098: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:58.946222: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:59.369881: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:59.369985: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:59.449359: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:59.449538: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2481 - acc: 0.9108 Epoch 22/40 2019-08-29 00:34:04.040611: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:04.040733: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:04.459577: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:04.459698: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:04.537060: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:04.537154: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2457 - acc: 0.9116 Epoch 23/40 2019-08-29 00:34:09.122286: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:09.122409: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:09.542468: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:09.542659: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:09.633226: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:09.633310: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 86us/sample - loss: 0.2419 - acc: 0.9119 Epoch 24/40 2019-08-29 00:34:14.283736: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:14.283861: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:14.759453: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:14.759588: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:14.840762: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:14.840865: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:14.924147: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:14.924254: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:15.297162: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:15.297277: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:15.374905: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:15.375009: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 6s 95us/sample - loss: 0.2388 - acc: 0.9141 Epoch 25/40 2019-08-29 00:34:20.010218: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:20.010338: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:20.431755: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:20.431867: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:20.511302: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:20.511404: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2368 - acc: 0.9146 Epoch 26/40 2019-08-29 00:34:25.085846: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:25.085965: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:25.497865: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:25.497980: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:25.575489: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:25.575573: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 84us/sample - loss: 0.2345 - acc: 0.9151 Epoch 27/40 2019-08-29 00:34:30.165576: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:30.165696: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:30.585389: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:30.585504: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:30.663307: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:30.663409: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2308 - acc: 0.9172 Epoch 28/40 2019-08-29 00:34:35.239820: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:35.239945: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:35.664925: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:35.665038: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:35.743716: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:35.743799: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2294 - acc: 0.9172 Epoch 29/40 2019-08-29 00:34:40.319353: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:40.319497: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:40.729421: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:40.729536: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:40.807044: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:40.807129: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2273 - acc: 0.9182 Epoch 30/40 2019-08-29 00:34:45.400274: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:45.400403: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:46.006187: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:46.006303: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:46.080739: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:46.080829: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 87us/sample - loss: 0.2253 - acc: 0.9193 Epoch 31/40 2019-08-29 00:34:50.675446: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:50.675569: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:51.083387: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:51.083492: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:51.158345: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:51.158437: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2238 - acc: 0.9199 Epoch 32/40 2019-08-29 00:34:55.735525: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:55.735650: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:56.186660: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:56.186764: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:56.260818: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:56.260911: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2213 - acc: 0.9203 Epoch 33/40 2019-08-29 00:35:00.860052: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:00.860199: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:01.251599: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:01.251755: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:01.327938: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:01.328027: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 84us/sample - loss: 0.2196 - acc: 0.9205 Epoch 34/40 2019-08-29 00:35:05.913785: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:05.913909: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:06.448875: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:06.448994: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:06.523964: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:06.524112: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 87us/sample - loss: 0.2184 - acc: 0.9206 Epoch 35/40 2019-08-29 00:35:11.114671: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:11.114823: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:11.521477: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:11.521598: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:11.596112: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:11.596214: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2159 - acc: 0.9218 Epoch 36/40 2019-08-29 00:35:16.230868: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:16.230993: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:16.631740: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:16.631860: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:16.709297: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:16.709410: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2145 - acc: 0.9225 Epoch 37/40 2019-08-29 00:35:21.293198: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:21.293319: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:21.807158: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:21.807261: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:21.930544: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:21.930631: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 87us/sample - loss: 0.2136 - acc: 0.9232 Epoch 38/40 2019-08-29 00:35:26.531272: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:26.531393: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:26.934413: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:26.934524: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:27.041029: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:27.041135: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2117 - acc: 0.9235 Epoch 39/40 2019-08-29 00:35:31.632210: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:31.632333: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:32.032924: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:32.033065: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:32.107077: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:32.107193: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 84us/sample - loss: 0.2108 - acc: 0.9241 Epoch 40/40 2019-08-29 00:35:36.705902: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:36.706024: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:37.106458: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:37.106617: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:37.183601: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:37.183817: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2098 - acc: 0.9239 2019-08-29 00:35:37.263849: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:37.263982: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:37.457410: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0032111567038731387 10000/10000 [==============================] - 0s 44us/sample - loss: 0.3531 - acc: 0.8830 WARNING:tensorflow:From mnist.py:69: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version. Instructions for updating: This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save. WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version. Instructions for updating: This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info. 2019-08-29 00:35:37.903206: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:37.903336: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:37.978201: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:37.978248: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:37.978318: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:37.978431: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.060440: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.060574: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.133815: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:38.133858: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:38.133913: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.134018: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.211956: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.212154: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.287561: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:38.287603: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:38.287662: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.287762: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.365346: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.365482: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.437001: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:38.437062: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:38.437133: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.437263: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.618714: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.618821: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.703515: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035381567038938618 2019-08-29 00:35:38.703638: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.703727: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.796327: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035381567038938703 2019-08-29 00:35:38.796732: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.796826: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.871391: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:38.871426: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:38.871468: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.871535: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.000565: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.000661: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.074122: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:39.074157: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:39.074197: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.074271: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.151349: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.151439: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.225500: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:39.225536: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:39.225574: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.225640: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.305893: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.305997: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.393168: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035391567038939305 2019-08-29 00:35:39.451779: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.451888: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.534538: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035391567038939451 2019-08-29 00:35:39.539846: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.539981: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.790995: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035391567038939534 2019-08-29 00:35:39.791131: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.791234: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.871382: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.871496: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.027665: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.027772: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.115533: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.115638: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.273357: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035401567038940115 2019-08-29 00:35:40.273461: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.273543: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.394230: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.394419: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.495666: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.495803: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.578868: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.578965: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.658188: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035401567038940578 2019-08-29 00:35:40.658293: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.658393: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.733400: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.733490: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.813995: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.814163: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.907589: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.907716: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.987771: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.987873: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.064912: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.065012: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.149777: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.149924: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.304768: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.304904: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.388975: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.389106: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.547755: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035411567038941388 2019-08-29 00:35:41.547853: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.547992: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.636644: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.636728: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.719947: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.720068: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.897549: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.897646: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.971144: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:41.971186: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:41.971243: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.971367: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:42.059414: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:42.059526: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:42.219035: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035421567038942059 Test accuracy: 0.883000016212 Saved model: s3://eks-ml-data/mnist/tf_saved_model/1    The last line shows that the exported model is saved to S3 bucket.\n"
},
{
	"uri": "/beginner/160_advanced-networking/secondary_cidr/eniconfig_crd/",
	"title": "Create CRDs",
	"tags": [],
	"description": "",
	"content": "Create custom resources for ENIConfig CRD As next step, we will add custom resources to ENIConfig custom resource definition (CRD). CRDs are extensions of Kubernetes API that stores collection of API objects of certain kind. In this case, we will store VPC Subnet and SecurityGroup configuration information in these CRDs so that Worker nodes can access them to configure VPC CNI plugin.\nYou should have ENIConfig CRD already installed with latest CNI version (1.3+). You can check if its installed by running this command.\nkubectl get crd You should see a response similar to this NAME CREATED AT eniconfigs.crd.k8s.amazonaws.com 2019-03-07T20:06:48Z  If you don\u0026rsquo;t have ENIConfig installed, you can install it by using this command\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.7/config/v1.7/aws-k8s-cni.yaml Create custom resources for each subnet by replacing Subnet and SecurityGroup IDs. Since we created three secondary subnets, we need create three custom resources.\nHere is the template for custom resource. Notice the values for Subnet ID and SecurityGroup ID needs to be replaced with appropriate values\napiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: group1-pod-netconfig spec: subnet: $SUBNETID1 securityGroups: - $SECURITYGROUPID1 - $SECURITYGROUPID2 Check the AZs and Subnet IDs for these subnets. Make note of AZ info as you will need this when you apply annotation to Worker nodes using custom network config\naws ec2 describe-subnets --filters \u0026quot;Name=cidr-block,Values=100.64.*\u0026quot; --query 'Subnets[*].[CidrBlock,SubnetId,AvailabilityZone]' --output table -------------------------------------------------------------- | DescribeSubnets | \u0026#43;-----------------\u0026#43;----------------------------\u0026#43;-------------\u0026#43; | 100.64.32.0/19 | subnet-07dab05836e4abe91 | us-east-2a | | 100.64.64.0/19 | subnet-0692cd08cc4df9b6a | us-east-2c | | 100.64.0.0/19 | subnet-04f960ffc8be6865c | us-east-2b | \u0026#43;-----------------\u0026#43;----------------------------\u0026#43;-------------\u0026#43;  Check your Worker Node SecurityGroup\nINSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag-key,Values=eks:cluster-name\u0026quot; \u0026quot;Name=tag-value,Values=eksworkshop*\u0026quot; --output text`) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;SecurityGroup for EC2 instance $i ...\u0026quot; aws ec2 describe-instances --instance-ids $i | jq -r '.Reservations[].Instances[].SecurityGroups[].GroupId' done SecurityGroup for EC2 instance i-03ea1a083c924cd78 ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef SecurityGroup for EC2 instance i-0a635aed890c7cc3e ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef SecurityGroup for EC2 instance i-048e5ec8815e5ea8a ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef  Create custom resource group1-pod-netconfig.yaml for first subnet (100.64.0.0/19). Replace the SubnetId and SecuritGroupIds with the values from above. Here is how it looks with the configuration values for my environment\nNote: We are using same SecurityGroup for pods as your Worker Nodes but you can change these and use custom SecurityGroups for your Pod Networking\napiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: group1-pod-netconfig spec: subnet: subnet-04f960ffc8be6865c securityGroups: - sg-070d03008bda531ad - sg-06e5cab8e5d6f16ef Create custom resource group2-pod-netconfig.yaml for second subnet (100.64.32.0/19). Replace the SubnetId and SecuritGroupIds as above.\nSimilarly, create custom resource group3-pod-netconfig.yaml for third subnet (100.64.64.0/19). Replace the SubnetId and SecuritGroupIds as above.\nCheck the instance details using this command as you will need AZ info when you apply annotation to Worker nodes using custom network config\naws ec2 describe-instances --filters \u0026quot;Name=tag-key,Values=eks:cluster-name\u0026quot; \u0026quot;Name=tag-value,Values=eksworkshop*\u0026quot; --query 'Reservations[*].Instances[*].[PrivateDnsName,Tags[?Key==`eks:nodegroup-name`].Value|[0],Placement.AvailabilityZone,PrivateIpAddress,PublicIpAddress]' --output table ------------------------------------------------------------------------------------------------------------------------------------------ | DescribeInstances | \u0026#43;-----------------------------------------------\u0026#43;---------------------------------------\u0026#43;-------------\u0026#43;-----------------\u0026#43;----------------\u0026#43; | ip-192-168-9-228.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2c | 192.168.9.228 | 18.191.57.131 | | ip-192-168-71-211.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2a | 192.168.71.211 | 18.221.77.249 | | ip-192-168-33-135.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2b | 192.168.33.135 | 13.59.167.90 | \u0026#43;-----------------------------------------------\u0026#43;---------------------------------------\u0026#43;-------------\u0026#43;-----------------\u0026#43;----------------\u0026#43;  Apply the CRDs\nkubectl apply -f group1-pod-netconfig.yaml kubectl apply -f group2-pod-netconfig.yaml kubectl apply -f group3-pod-netconfig.yaml As last step, we will annotate nodes with custom network configs.\nBe sure to annotate the instance with config that matches correct AZ. For ex, in my environment instance ip-192-168-33-135.us-east-2.compute.internal is in us-east-2b. So, I will apply group1-pod-netconfig.yaml to this instance. Similarly, I will apply group2-pod-netconfig.yaml to ip-192-168-71-211.us-east-2.compute.internal and group3-pod-netconfig.yaml to ip-192-168-9-228.us-east-2.compute.internal\n kubectl annotate node \u0026lt;nodename\u0026gt;.\u0026lt;region\u0026gt;.compute.internal k8s.amazonaws.com/eniConfig=group1-pod-netconfig As an example, here is what I would run in my environment kubectl annotate node ip-192-168-33-135.us-east-2.compute.internal k8s.amazonaws.com/eniConfig=group1-pod-netconfig  You should now see secondary IP address from extended CIDR assigned to annotated nodes.\nAdditional notes on ENIConfig naming and automatic matching Optionally, you specify which node label will be used to match the ENIConfig name. Consider the following example: you have one ENIConfig per availability zone, named after the AZ (us-east-2a, us-east-2b, us-east-2c). You can then use a label already applied to your nodes, such as topology.kubernetes.io/zone where the value of the label matches the ENIConfig name.\n$ kubectl describe nodes | grep \u0026#39;topology.kubernetes.io/zone\u0026#39; topology.kubernetes.io/zone=us-east-2a topology.kubernetes.io/zone=us-east-2c topology.kubernetes.io/zone=us-east-2b  kubectl set env daemonset aws-node -n kube-system ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone  Kubernetes will now apply the corresponding ENIConfig matching the nodes AZ.\n"
},
{
	"uri": "/advanced/410_batch/deploy/",
	"title": "Deploy Argo",
	"tags": [],
	"description": "",
	"content": "Deploy Argo Controller Argo run in its own namespace and deploys as a CustomResourceDefinition.\nDeploy the Controller and UI.\nkubectl create namespace argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/${ARGO_VERSION}/manifests/install.yaml customresourcedefinition.apiextensions.k8s.io/clusterworkflowtemplates.argoproj.io created customresourcedefinition.apiextensions.k8s.io/cronworkflows.argoproj.io created customresourcedefinition.apiextensions.k8s.io/workflows.argoproj.io created customresourcedefinition.apiextensions.k8s.io/workflowtemplates.argoproj.io created serviceaccount/argo created serviceaccount/argo-server created role.rbac.authorization.k8s.io/argo-role created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-admin created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-edit created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-view created clusterrole.rbac.authorization.k8s.io/argo-cluster-role created clusterrole.rbac.authorization.k8s.io/argo-server-cluster-role created rolebinding.rbac.authorization.k8s.io/argo-binding created clusterrolebinding.rbac.authorization.k8s.io/argo-binding created clusterrolebinding.rbac.authorization.k8s.io/argo-server-binding created configmap/workflow-controller-configmap created service/argo-server created service/workflow-controller-metrics created deployment.apps/argo-server created deployment.apps/workflow-controller created  Configure the service account to run Workflows In order for Argo to support features such as artifacts, outputs, access to secrets, etc. it needs to communicate with Kubernetes resources using the Kubernetes API. To communicate with the Kubernetes API, Argo uses a ServiceAccount to authenticate itself to the Kubernetes API. You can specify which Role (i.e. which permissions) the ServiceAccount that Argo uses by binding a Role to a ServiceAccount using a RoleBinding\nkubectl -n argo create rolebinding default-admin --clusterrole=admin --serviceaccount=argo:default  Note that this will grant admin privileges to the default ServiceAccount in the namespace that the command is run from, so you will only be able to run Workflows in the namespace where the RoleBinding was made.\n "
},
{
	"uri": "/advanced/310_servicemesh_with_istio/deploy/",
	"title": "Deploy Sample Apps",
	"tags": [],
	"description": "",
	"content": "The Envoy Sidecar As mentioned during the Istio architecture overview, in order to take advantage of all of Istio‚Äôs features pods must be running an Istio sidecar proxy.\nIstio offers two ways injecting the Istio sidecar into a pod:\n  Manually using the istioctl command.\nManual injection directly modifies configuration, like deployments, and injects the proxy configuration into it.\n  Automatically using the Istio sidecar injector.\nYou will still need to manually enable Istio in each namespace that you want to be managed by Istio.\n  We will install the Bookinfo application inside its own namespace and allow Istio to automatically inject the Sidecar Proxy.\n kubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled kubectl get ns bookinfo --show-labels Now, we can deploy a vanilla definition of the Bookinfo application inside the bookinfo namespace, and the Mutating Webhook will alter the definition of any pod it sees to include the Envoy sidecar container.\nArchitecture of the Bookinfo application The Bookinfo application is broken into four separate microservices:\n  productpage The¬†productpage¬†microservice calls the¬†details¬†and¬†reviews¬†microservices to populate the page.    details The¬†details¬†microservice contains book information.    reviews The¬†reviews¬†microservice contains book reviews. It also calls the¬†ratings¬†microservice.    ratings The¬†ratings¬†microservice contains book ranking information that accompanies a book review.    There are 3 versions of the¬†reviewsmicroservice:\n  Version v1\n doesn‚Äôt call the¬†ratings¬†service.    Version v2\n calls the¬†ratings¬†service, and displays each rating as 1 to 5 black stars.    Version v3\n calls the¬†ratings¬†service, and displays each rating as 1 to 5 red stars.    Deploy the Sample Apps Now we will deploy the Bookinfo application to review key capabilities of Istio such as intelligent routing, and review telemetry data using Prometheus and Grafana.\nkubectl -n bookinfo apply \\  -f ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/platform/kube/bookinfo.yaml Let\u0026rsquo;s verify the deployment\nkubectl -n bookinfo get pod,svc NAME READY STATUS RESTARTS AGE pod/details-v1-5f449bdbb9-pmf66 2/2 Running 0 33s pod/productpage-v1-6f9df695b7-rxqww 2/2 Running 0 32s pod/ratings-v1-857bb87c57-95499 2/2 Running 0 32s pod/reviews-v1-68f9c47f69-f5psn 2/2 Running 0 33s pod/reviews-v2-5d56c488f5-g25r6 2/2 Running 0 33s pod/reviews-v3-869ff44845-c8c4c 2/2 Running 0 33s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/details ClusterIP 10.100.75.171 \u0026lt;none\u0026gt; 9080/TCP 33s service/productpage ClusterIP 10.100.192.219 \u0026lt;none\u0026gt; 9080/TCP 33s service/ratings ClusterIP 10.100.45.201 \u0026lt;none\u0026gt; 9080/TCP 33s service/reviews ClusterIP 10.100.239.94 \u0026lt;none\u0026gt; 9080/TCP 33s  Create an Istio Gateway Now that the Bookinfo services are up and running, you need to make the application accessible from outside of your Kubernetes cluster, e.g., from a browser. An Istio Gateway is used for this purpose.\nWe\u0026rsquo;ll define the virtual service and ingress gateway.\nkubectl -n bookinfo \\  apply -f ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/networking/bookinfo-gateway.yaml  This may take a minute or two, first for the Ingress to be created, and secondly for the Ingress to hook up with the services it exposes.\n To verify that the application is reachable, run the command below, click on the link and choose open.\nexport GATEWAY_URL=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) echo \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; Click reload multiple times to see how the layout and content of the reviews changes as different versions (v1, v2, v3) of the app are called.\n "
},
{
	"uri": "/010_introduction/basics/concepts_nodes/",
	"title": "Kubernetes Nodes",
	"tags": [],
	"description": "",
	"content": "The machines that make up a Kubernetes cluster are called nodes.\nNodes in a Kubernetes cluster may be physical, or virtual.\nThere are two types of nodes:\n  A Control-plane-node type, which makes up the Control Plane, acts as the ‚Äúbrains‚Äù of the cluster.\n  A Worker-node type, which makes up the Data Plane, runs the actual container images (via pods).\n  We‚Äôll dive deeper into how nodes interact with each other later in the presentation.\n"
},
{
	"uri": "/beginner/090_rbac/test_rbac_user_without_roles/",
	"title": "Test the new user",
	"tags": [],
	"description": "",
	"content": "Up until now, as the cluster operator, you\u0026rsquo;ve been accessing the cluster as the admin user. Let\u0026rsquo;s now see what happens when we access the cluster as the newly created rbac-user.\nIssue the following command to source the rbac-user\u0026rsquo;s AWS IAM user environmental variables:\n. rbacuser_creds.sh By running the above command, you\u0026rsquo;ve now set AWS environmental variables which should override the default admin user or role. To verify we\u0026rsquo;ve overrode the default user settings, run the following command:\naws sts get-caller-identity You should see something similar to below, where we\u0026rsquo;re now making API calls as rbac-user:\n{ \u0026#34;Account\u0026#34;: \u0026lt;AWS Account ID\u0026gt;, \u0026#34;UserId\u0026#34;: \u0026lt;AWS User ID\u0026gt;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AWS Account ID\u0026gt;:user/rbac-user\u0026#34; }  Now that we\u0026rsquo;re making calls in the context of the rbac-user, lets quickly make a request to get all pods:\nkubectl get pods -n rbac-test You should get a response back similar to:\nNo resources found. Error from server (Forbidden): pods is forbidden: User \u0026#34;rbac-user\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;rbac-test\u0026#34;  We already created the rbac-user, so why did we get that error?\nJust creating the user doesn\u0026rsquo;t give that user access to any resources in the cluster. In order to achieve that, we\u0026rsquo;ll need to define a role, and then bind the user to that role. We\u0026rsquo;ll do that next.\n"
},
{
	"uri": "/beginner/050_deploy/viewservices/",
	"title": "Find the Service Address",
	"tags": [],
	"description": "",
	"content": "Now that we have a running service that is type: LoadBalancer we need to find the ELB\u0026rsquo;s address. We can do this by using the get services operation of kubectl:\nkubectl get service ecsdemo-frontend Notice the field isn\u0026rsquo;t wide enough to show the FQDN of the ELB. We can adjust the output format with this command:\nkubectl get service ecsdemo-frontend -o wide If we wanted to use the data programatically, we can also output via json. This is an example of how we might be able to make use of json output:\nELB=$(kubectl get service ecsdemo-frontend -o json | jq -r '.status.loadBalancer.ingress[].hostname') curl -m3 -v $ELB  It will take several minutes for the ELB to become healthy and start passing traffic to the frontend pods.\n You should also be able to copy/paste the loadBalancer hostname into your browser and see the application running. Keep this tab open while we scale the services up on the next page.\n"
},
{
	"uri": "/030_eksctl/console/",
	"title": "Console Credentials",
	"tags": [],
	"description": "",
	"content": "This step is optional, as nearly all of the workshop content is CLI-driven. But, if you\u0026rsquo;d like full access to your workshop cluster in the EKS console this step is recommended.\nThe EKS console allows you to see not only the configuration aspects of your cluster, but also to view Kubernetes cluster objects such as Deployments, Pods, and Nodes. For this type of access, the console IAM User or Role needs to be granted permission within the cluster.\nBy default, the credentials used to create the cluster are automatically granted these permissions. Following along in the workshop, you\u0026rsquo;ve created a cluster using temporary IAM credentials from within Cloud9. This means that you\u0026rsquo;ll need to add your AWS Console credentials to the cluster.\nImport your EKS Console credentials to your new cluster: IAM Users and Roles are bound to an EKS Kubernetes cluster via a ConfigMap named aws-auth. We can use eksctl to do this with one command.\nYou\u0026rsquo;ll need to determine the correct credential to add for your AWS Console access. If you know this already, you can skip ahead to the eksctl create iamidentitymapping step below.\nIf you\u0026rsquo;ve built your cluster from Cloud9 as part of this tutorial, invoke the following within your environment to determine your IAM Role or User ARN.\nc9builder=$(aws cloud9 describe-environment-memberships --environment-id=$C9_PID | jq -r \u0026#39;.memberships[].userArn\u0026#39;) if echo ${c9builder} | grep -q user; then rolearn=${c9builder} echo Role ARN: ${rolearn} elif echo ${c9builder} | grep -q assumed-role; then assumedrolename=$(echo ${c9builder} | awk -F/ \u0026#39;{print $(NF-1)}\u0026#39;) rolearn=$(aws iam get-role --role-name ${assumedrolename} --query Role.Arn --output text) echo Role ARN: ${rolearn} fi With your ARN in hand, you can issue the command to create the identity mapping within the cluster.\neksctl create iamidentitymapping --cluster eksworkshop-eksctl --arn ${rolearn} --group system:masters --username admin Note that permissions can be restricted and granular but as this is a workshop cluster, you\u0026rsquo;re adding your console credentials as administrator.\nNow you can verify your entry in the AWS auth map within the console.\nkubectl describe configmap -n kube-system aws-auth Now you\u0026rsquo;re all set to move on. For more information, check out the EKS documentation on this topic.\n"
},
{
	"uri": "/beginner/170_statefulset/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "export EBS_CSI_POLICY_NAME=\u0026#34;Amazon_EBS_CSI_Driver\u0026#34; export EBS_CSI_POLICY_ARN=$(aws --region ${AWS_REGION} iam list-policies --query \u0026#39;Policies[?PolicyName==`\u0026#39;${EBS_CSI_POLICY_NAME}\u0026#39;`].Arn\u0026#39; --output text) kubectl delete \\  -f ${HOME}/environment/ebs_statefulset/mysql-statefulset.yaml \\  -f ${HOME}/environment/ebs_statefulset/mysql-services.yaml \\  -f ${HOME}/environment/ebs_statefulset/mysql-configmap.yaml \\  -f ${HOME}/environment/ebs_statefulset/mysql-storageclass.yaml # Delete the mysql namespace  kubectl delete namespace mysql # Uninstall the aws-ebs-csi-driver helm -n kube-system uninstall aws-ebs-csi-driver # Delete the service account eksctl delete iamserviceaccount \\  --cluster eksworkshop-eksctl \\  --namespace kube-system \\  --name ebs-csi-controller-irsa \\  --wait # Delete the IAM Amazon_EBS_CSI_Driver policy aws iam delete-policy \\  --region ${AWS_REGION} \\  --policy-arn ${EBS_CSI_POLICY_ARN} cd ${HOME}/environment rm -rf ${HOME}/environment/ebs_statefulset Congratulation! You\u0026rsquo;ve finished the StatefulSets lab "
},
{
	"uri": "/intermediate/330_app_mesh/deploy_dj_app/clone_repo/",
	"title": "Clone the Repo",
	"tags": [],
	"description": "",
	"content": "To begin, clone the repository that holds the DJ App\u0026rsquo;s files.\n# First, be sure you are in your environment directory cd ~/environment git clone https://github.com/aws/aws-app-mesh-examples # Change to the repo\u0026#39;s project directory: cd aws-app-mesh-examples/examples/apps/djapp/ "
},
{
	"uri": "/intermediate/330_app_mesh/install_app_mesh_controller/install_controller/",
	"title": "Install the App Mesh Controller",
	"tags": [],
	"description": "",
	"content": "Helm V3 If the command below gives you an error, follow this link to install the latest version of Helm.\n helm version --short The AWS App Mesh Controller for Kubernetes is easily installed using Helm. To get started, add the EKS Charts repository.\nhelm repo add eks https://aws.github.io/eks-charts helm repo list | grep eks-charts eks https://aws.github.io/eks-charts  Create the appmesh-system namespace and attach IAM Policies for AWS App Mesh and AWS Cloud Map full access.\nif you are new to the IAM Roles for Service Accounts (IRSA) concept, Click here for me information.\n kubectl create ns appmesh-system # Create your OIDC identity provider for the cluster eksctl utils associate-iam-oidc-provider \\  --cluster eksworkshop-eksctl \\  --approve # Download the IAM policy document for the controller curl -o controller-iam-policy.json https://raw.githubusercontent.com/aws/aws-app-mesh-controller-for-k8s/master/config/iam/controller-iam-policy.json # Create an IAM policy for the controller from the policy document aws iam create-policy \\  --policy-name AWSAppMeshK8sControllerIAMPolicy \\  --policy-document file://controller-iam-policy.json # Create an IAM role and service account for the controller eksctl create iamserviceaccount \\  --cluster eksworkshop-eksctl \\  --namespace appmesh-system \\  --name appmesh-controller \\  --attach-policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSAppMeshK8sControllerIAMPolicy \\  --override-existing-serviceaccounts \\  --approve Now install App Mesh Controller into the appmesh-system namespace using the project\u0026rsquo;s Helm chart.\nhelm upgrade -i appmesh-controller eks/appmesh-controller \\  --namespace appmesh-system \\  --set region=${AWS_REGION} \\  --set serviceAccount.create=false \\  --set serviceAccount.name=appmesh-controller Now list all resources in the appmesh-system namespace and verify the installation was successful.\nkubectl -n appmesh-system get all The output should be similar to this:\nNAME READY STATUS RESTARTS AGE pod/appmesh-controller-866f8b8cdf-twkcq 1/1 Running 0 2m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/appmesh-controller-webhook-service ClusterIP 10.100.209.34 \u0026lt;none\u0026gt; 443/TCP 2m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/appmesh-controller 1/1 1 1 2m NAME DESIRED CURRENT READY AGE replicaset.apps/appmesh-controller-866f8b8cdf 1 1 1 2m  You can also see that the App Mesh Custom Resource Definitions were installed.\nkubectl get crds | grep appmesh gatewayroutes.appmesh.k8s.aws 2020-10-15T15:49:26Z meshes.appmesh.k8s.aws 2020-10-15T15:49:26Z virtualgateways.appmesh.k8s.aws 2020-10-15T15:49:26Z virtualnodes.appmesh.k8s.aws 2020-10-15T15:49:26Z virtualrouters.appmesh.k8s.aws 2020-10-15T15:49:26Z virtualservices.appmesh.k8s.aws 2020-10-15T15:49:26Z  "
},
{
	"uri": "/beginner/080_scaling/test_ca/",
	"title": "Scale a Cluster with CA",
	"tags": [],
	"description": "",
	"content": "Deploy a Sample App We will deploy an sample nginx application as a ReplicaSet of 1 Pod\ncat \u0026lt;\u0026lt;EoF\u0026gt; ~/environment/cluster-autoscaler/nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-to-scaleout spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: nginx-to-scaleout resources: limits: cpu: 500m memory: 512Mi requests: cpu: 500m memory: 512Mi EoF kubectl apply -f ~/environment/cluster-autoscaler/nginx.yaml kubectl get deployment/nginx-to-scaleout Scale our ReplicaSet Let\u0026rsquo;s scale out the replicaset to 10\nkubectl scale --replicas=10 deployment/nginx-to-scaleout Some pods will be in the Pending state, which triggers the cluster-autoscaler to scale out the EC2 fleet.\nkubectl get pods -l app=nginx -o wide --watch NAME READY STATUS RESTARTS AGE nginx-to-scaleout-7cb554c7d5-2d4gp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-2nh69 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-45mqz 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-4qvzl 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5jddd 1/1 Running 0 34s nginx-to-scaleout-7cb554c7d5-5sx4h 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5xbjp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-6l84p 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-7vp7l 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-86pr6 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-88ttw 0/1 Pending 0 12s  View the cluster-autoscaler logs\nkubectl -n kube-system logs -f deployment/cluster-autoscaler You will notice Cluster Autoscaler events similar to below Check the EC2 AWS Management Console to confirm that the Auto Scaling groups are scaling up to meet demand. This may take a few minutes. You can also follow along with the pod deployment from the command line. You should see the pods transition from pending to running as nodes are scaled up.\nor by using the kubectl\nkubectl get nodes Output\nip-192-168-12-114.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 3d6h v1.17.7-eks-bffbac ip-192-168-29-155.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 63s v1.17.7-eks-bffbac ip-192-168-55-187.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 3d6h v1.17.7-eks-bffbac ip-192-168-82-113.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 8h v1.17.7-eks-bffbac  "
},
{
	"uri": "/intermediate/210_jenkins/multibranch/",
	"title": "Setup multibranch projects",
	"tags": [],
	"description": "",
	"content": "After logging into the Jenkins web console, we\u0026rsquo;re ready to add our eksworkshop-app repository. Start by selecting New Item in the menu on the left side.\nSet the name of the item to codecommit and select the AWS Code commit item type.\nIn your Cloud9 workspace, execute the following commands to get your Git username and password.\necho $GIT_USERNAME echo $GIT_PASSWORD Back to Jenkins. In the Projects section, to the right of Code Commit Credentials, select Add then CodeCommit.\nSet the Username and Password to the corresponding values from the previous command and click Add.\nConfirm your current AWS Region.\necho https://codecommit.$AWS_REGION.amazonaws.com Copy that value to theURL field under project and select your use from the Code Commit Credentials DropDown menu.\nSelect Save at the bottom left of the screen. Jenkins will begin executing the pipelines in repositories and branches that contain a Jenkinsfile.\n"
},
{
	"uri": "/intermediate/330_app_mesh/deploy_dj_app/test_worker_perms/",
	"title": "Test Permissions",
	"tags": [],
	"description": "",
	"content": "To test that your worker nodes are able to use these permissions correctly, we\u0026rsquo;ll run a job that attempts to list all existing meshes.\nRun this command to set the script to run against the correct region:\nsed -i'.old' -e 's/\\\u0026quot;us-west-2\\\u0026quot;/\\\u0026quot;'$AWS_REGION'\\\u0026quot;/' awscli.yaml Next, execute the job:\nkubectl apply -f awscli.yaml Make sure its completed by issuing the command:\nkubectl get jobs And see that desired and successful are both one:\nNAME DESIRED SUCCESSFUL AGE awscli 1 1 1m Inspect the output of the job:\nkubectl logs jobs/awscli The output of this command will illustrate if your nodes can make App Mesh API calls successfully as well.\nThis output shows the workers have proper access:\n{ \u0026#34;meshes\u0026#34;: [] }  And this output shows they don\u0026rsquo;t:\nAn error occurred (AccessDeniedException) when calling the ListMeshes operation: User: arn:aws:iam::123abc:user/foo is not authorized to perform: appmesh:ListMeshes on resource: *  If you need to troubleshoot further, in order to run the job again to test, you must first delete it:\nkubectl delete jobs/awscli Once you\u0026rsquo;ve successfully tested for the proper permissions, continue on to the next step.\n"
},
{
	"uri": "/beginner/060_helm/helm_micro/service/",
	"title": "Test the Service",
	"tags": [],
	"description": "",
	"content": "To test the service our eksdemo Chart created, we\u0026rsquo;ll need to get the name of the ELB endpoint that was generated when we deployed the Chart:\nkubectl get svc ecsdemo-frontend -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[*].hostname}\u0026#34;; echo Copy that address, and paste it into a new tab in your browser. You should see something similar to:\n"
},
{
	"uri": "/intermediate/265_spinnaker_eks/add_eks-cccount/",
	"title": "Add EKS Account",
	"tags": [],
	"description": "",
	"content": "At a high level, Spinnaker operates in the following way when deploying to Kubernetes:\n Spinnaker is configured with one or more ‚ÄúCloud Provider‚Äù Kubernetes accounts (which you can think of as deployment targets) For each Kubernetes account, Spinnaker is provided a kubeconfig to connect to that Kubernetes cluster The kubeconfig should have the following contents:  A Kubernetes kubeconfig cluster A Kubernetes kubeconfig user A Kubernetes kubeconfig context Metadata such as which context to use by default   Each Kubernetes account is configured in the SpinnakerService manifest under spec.spinnakerConfig.config.providers.kubernetes.accounts key. Each entity has these (and other) fields:  name: A Spinnaker-internal name kubeconfigFile: A file path referencing the contents of the kubeconfig file for connecting to the target cluster. onlySpinnakerManaged: When true, Spinnaker only caches and displays applications that have been created by Spinnaker. namespaces: An array of namespaces that Spinnaker will be allowed to deploy to. If this is left blank, Spinnaker will be allowed to deploy to all namespaces omitNamespaces: If namespaces is left blank, you can blacklist specific namespaces to indicate to Spinnaker that it should not deploy to those namespaces   If the kubeconfig is properly referenced and available, Operator will take care of the following:  Creating a Kubernetes secret containing your kubeconfig in the namespace where Spinnaker lives Dynamically generating a clouddriver.yml file that properly references the kubeconfig from where it is mounted within the Clouddriver container Creating/Updating the Kubernetes Deployment (spin-clouddriver) which runs Clouddriver so that it is aware of the secret and properly mounts it in the Clouddriver pod    Now, lets add a Kubernetes/EKS Account Deployment Target in Spinnaker.\nDownload the latest spinnaker-tools release This tool helps to create the ServiceAccount, ClusterRoleBinding, kubeconfig for the service account for the EKS/Kubernetes account\ncd ~/environment git clone https://github.com/armory/spinnaker-tools.git cd spinnaker-tools go mod download all go build Cloning into \u0026#39;spinnaker-tools\u0026#39;... remote: Enumerating objects: 278, done. remote: Counting objects: 100% (6/6), done. remote: Compressing objects: 100% (6/6), done. remote: Total 278 (delta 0), reused 4 (delta 0), pack-reused 272 Receiving objects: 100% (278/278), 84.72 KiB | 4.71 MiB/s, done. Resolving deltas: 100% (124/124), done.  Setup environment variables export CONTEXT=$(kubectl config current-context) export SOURCE_KUBECONFIG=${HOME}/.kube/config export SPINNAKER_NAMESPACE=\u0026#34;spinnaker\u0026#34; export SPINNAKER_SERVICE_ACCOUNT_NAME=\u0026#34;spinnaker-ws-sa\u0026#34; export DEST_KUBECONFIG=${HOME}/Kubeconfig-ws-sa echo $CONTEXT echo $SOURCE_KUBECONFIG echo $SPINNAKER_NAMESPACE echo $SPINNAKER_SERVICE_ACCOUNT_NAME echo $DEST_KUBECONFIG  If you do not see output from the above command for all the above Environment Variables, do not proceed to next step\n Create the service account Create the kubernetes service account with namespace-specific permissions\n./spinnaker-tools create-service-account --kubeconfig ${SOURCE_KUBECONFIG} --context ${CONTEXT} --output ${DEST_KUBECONFIG} --namespace ${SPINNAKER_NAMESPACE} --service-account-name ${SPINNAKER_SERVICE_ACCOUNT_NAME} Cloning into \u0026#39;spinnaker-tools\u0026#39;... remote: Enumerating objects: 278, done. remote: Counting objects: 100% (6/6), done. remote: Compressing objects: 100% (6/6), done. remote: Total 278 (delta 0), reused 4 (delta 0), pack-reused 272 Receiving objects: 100% (278/278), 84.72 KiB | 4.71 MiB/s, done. Resolving deltas: 100% (124/124), done. Getting namespaces ... Creating service account spinnaker-ws-sa ... Created ServiceAccount spinnaker-ws-sa in namespace spinnaker Adding cluster-admin binding to service account spinnaker-ws-sa ... Created ClusterRoleBinding spinnaker-spinnaker-ws-sa-admin in namespace spinnaker Getting token for service account ... Cloning kubeconfig ... Renaming context in kubeconfig ... Switching context in kubeconfig ... Creating token user in kubeconfig ... Updating context to use token user in kubeconfig ... Updating context with namespace in kubeconfig ... Minifying kubeconfig ... Deleting temp kubeconfig ... Created kubeconfig file at /home/ec2-user/Kubeconfig-ws-sa  Configure EKS Account Open the SpinnakerService manifest located under deploy/spinnaker/basic/spinnakerservice.yml, then add the below to the section spec.spinnakerConfig.config.\nproviders: dockerRegistry: enabled: true kubernetes: enabled: true accounts: - name: spinnaker-workshop requiredGroupMembership: [] providerVersion: V2 permissions: dockerRegistries: - accountName: my-ecr-registry configureImagePullSecrets: true cacheThreads: 1 namespaces: [spinnaker,detail] omitNamespaces: [] kinds: [] omitKinds: [] customResources: [] cachingPolicies: [] oAuthScopes: [] onlySpinnakerManaged: false kubeconfigFile: kubeconfig-sp # File name must match \u0026#34;files\u0026#34; key primaryAccount: spinnaker-workshop # Change to a desired account from the accounts array  Open the SpinnakerService manifest located under deploy/spinnaker/basic/spinnakerservice.yml, then add the below section under spec.spinnakerConfig. Replace the \u0026lt;FILE CONTENTS HERE\u0026gt; in below section with kubeconfig file content created from previous step from the location ${HOME}/Kubeconfig-ws-sa.\nfiles: kubeconfig-sp: | \u0026lt;FILE CONTENTS HERE\u0026gt; # Content from kubeconfig created by Spinnaker Tool  Congratulations! You are done with the Spinnaker configuration for all the Spinnaker services! Lets install Spinnaker now.\n"
},
{
	"uri": "/intermediate/300_cis_eks_benchmark/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Conclusion In this chapter, we have explained:\n CIS Kubernetes Benchmark and CIS Amazon EKS Benchmark; Introduced kube-bench as an open source tool to assess against CIS Kubernetes Benchmarks; Demonstrated steps to assess Amazon EKS clusters for managed and self managed nodes Kubernetes security configurations using kube-bench.  "
},
{
	"uri": "/intermediate/200_migrate_to_eks/configure-eks-cluster/",
	"title": "Configure EKS cluster",
	"tags": [],
	"description": "",
	"content": "We created an EKS cluster cluster with a managed node group and OIDC. For Postgres persistent storage we\u0026rsquo;re going to use a host path for the sake of this workshop but it would be advised to use Amazon Elastic File System (EFS) because it\u0026rsquo;s a regional storage service. If the Postgres pod moves availability zones data will still be available. If you\u0026rsquo;d like to do it manually you can follow the EFS workshop here.\nTo let traffic cross between EKS and Cloud9 we need to create a VPC peer between our Cloud9 instance and our EKS cluster.\nexport EKS_VPC=$(aws eks describe-cluster \\  --name ${CLUSTER} \\  --query \u0026#34;cluster.resourcesVpcConfig.vpcId\u0026#34; \\  --output text) export PEERING_ID=$(aws ec2 create-vpc-peering-connection \\  --vpc-id $VPC --peer-vpc-id $EKS_VPC \\  --query \u0026#39;VpcPeeringConnection.VpcPeeringConnectionId\u0026#39; \\  --output text) aws ec2 accept-vpc-peering-connection \\  --vpc-peering-connection-id $PEERING_ID aws ec2 modify-vpc-peering-connection-options \\  --vpc-peering-connection-id $PEERING_ID \\  --requester-peering-connection-options \u0026#39;{\u0026#34;AllowDnsResolutionFromRemoteVpc\u0026#34;:true}\u0026#39; \\  --accepter-peering-connection-options \u0026#39;{\u0026#34;AllowDnsResolutionFromRemoteVpc\u0026#34;:true}\u0026#39; Allow traffic from the EKS from the EKS VPC and Security group to our Cloud9 instance\nexport EKS_SECURITY_GROUP=$(aws cloudformation list-exports \\  --query \u0026#34;Exports[*]|[?Name==\u0026#39;eksctl-$CLUSTER-cluster::SharedNodeSecurityGroup\u0026#39;].Value\u0026#34; \\  --output text) export EKS_CIDR_RANGES=$(aws ec2 describe-subnets \\  --filter \u0026#34;Name=vpc-id,Values=$EKS_VPC\u0026#34; \\  --query \u0026#39;Subnets[*].CidrBlock\u0026#39; \\  --output text) for CIDR in $(echo $EKS_CIDR_RANGES); do aws ec2 authorize-security-group-ingress \\  --group-id $SECURITY_GROUP \\  --ip-permissions IpProtocol=tcp,FromPort=1024,ToPort=65535,IpRanges=\u0026#34;[{CidrIp=$CIDR}]\u0026#34; done export CIDR_RANGES=$(aws ec2 describe-subnets \\  --filter \u0026#34;Name=vpc-id,Values=$VPC\u0026#34; \\  --query \u0026#39;Subnets[*].CidrBlock\u0026#39; \\  --output text) for CIDR in $(echo $CIDR_RANGES); do aws ec2 authorize-security-group-ingress \\  --group-id $EKS_SECURITY_GROUP \\  --ip-permissions IpProtocol=tcp,FromPort=1024,ToPort=65535,IpRanges=\u0026#34;[{CidrIp=$CIDR}]\u0026#34; done Finally create routes in both VPCs to route traffic\nexport CIDR_BLOCK_1=$(aws ec2 describe-vpc-peering-connections \\  --query \u0026#34;VpcPeeringConnections[?VpcPeeringConnectionId==\u0026#39;$PEERING_ID\u0026#39;].AccepterVpcInfo.CidrBlock\u0026#34; \\  --output text) export CIDR_BLOCK_2=$(aws ec2 describe-vpc-peering-connections \\  --query \u0026#34;VpcPeeringConnections[?VpcPeeringConnectionId==\u0026#39;$PEERING_ID\u0026#39;].RequesterVpcInfo.CidrBlock\u0026#34; \\  --output text) export EKS_RT=$(aws cloudformation list-stack-resources \\  --query \u0026#34;StackResourceSummaries[?LogicalResourceId==\u0026#39;PublicRouteTable\u0026#39;].PhysicalResourceId\u0026#34; \\  --stack-name eksctl-${CLUSTER}-cluster \\  --output text) export RT=$(aws ec2 describe-route-tables \\  --filter \u0026#34;Name=vpc-id,Values=${VPC}\u0026#34; \\  --query \u0026#39;RouteTables[0].RouteTableId\u0026#39; \\  --output text) aws ec2 create-route \\  --route-table-id $EKS_RT \\  --destination-cidr-block $CIDR_BLOCK_2 \\  --vpc-peering-connection-id $PEERING_ID aws ec2 create-route \\  --route-table-id $RT \\  --destination-cidr-block $CIDR_BLOCK_1 \\  --vpc-peering-connection-id $PEERING_ID Now that traffic will route between our clusters we can deploy our application to the EKS cluster.\n"
},
{
	"uri": "/advanced/350_opentelemetry/otel_collector/",
	"title": "OTEL Collector",
	"tags": [],
	"description": "",
	"content": "In this section we will install AWS Distro for Open Telemetry, a secure, production-ready, AWS-supported distribution of the OpenTelemetry project. We will also be configuring Container Insights, which collects container metrics and analyzes them along with other metrics in Amazon CloudWatch.\nFirst let\u0026rsquo;s get our EKS Cluster ARN:\nexport CLUSTER_NAME=$(aws eks list-clusters --output=json | jq \u0026#39;.clusters[0]\u0026#39; -r) echo -e \u0026#34;EKS Cluster Name: ${CLUSTER_NAME}\u0026#34; Now let\u0026rsquo;s start configuring our IAM permissions\n"
},
{
	"uri": "/intermediate/290_argocd/update_application/",
	"title": "Update the application",
	"tags": [],
	"description": "",
	"content": "Our application is now deployed into our ArgoCD. We are now going to update our github repository synced with our application\nUpdate your application Go to your Github fork repository:\nUpdate spec.replicas: 2 in ecsdemo-nodejs/kubernetes/deployment.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: ecsdemo-nodejs labels: app: ecsdemo-nodejs namespace: default spec: replicas: 2 selector: matchLabels: app: ecsdemo-nodejs strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: ecsdemo-nodejs spec: containers: - image: brentley/ecsdemo-nodejs:latest imagePullPolicy: Always name: ecsdemo-nodejs ports: - containerPort: 3000 protocol: TCP Add a commit message and click on Commit changes\nAccess ArgoCD Web Interface To deploy our change we can access to ArgoCD UI. Open your web browser and go to the Load Balancer url:\necho $ARGOCD_SERVER Login using admin / $ARGO_PWD. You now have access to the ecsdemo-nodejds application. After clicking to refresh button status should be OutOfSync:\nThis means our Github repository is not synchronised with the deployed application. To fix this and deploy the new version (with 2 replicas) click on the sync button, and select the APPS/DEPLOYMENT/DEFAULT/ECSDEMO-NODEJS and SYNCHRONIZE:\nAfter the sync completed our application should have the Synced status with 2 pods:\nAll those actions could have been made with the Argo CLI also.\n"
},
{
	"uri": "/advanced/350_opentelemetry/otel_collector/iam-setup/",
	"title": "IAM Setup",
	"tags": [],
	"description": "",
	"content": "Creating IAM permissions for OTEL Collector Enable the IAM OIDC provider on our EKS Cluster\neksctl utils associate-iam-oidc-provider --region=$AWS_REGION \\  --cluster=$CLUSTER_NAME \\  --approve export OIDC_PROVIDER=$(aws eks describe-cluster --name $CLUSTER_NAME --region=$AWS_REGION \\  --query \u0026#34;cluster.identity.oidc.issuer\u0026#34; --output text | sed -e \u0026#34;s/^https:\\/\\///\u0026#34;) echo -e \u0026#34;OIDC_PROVIDER: $OIDC_PROVIDER\u0026#34; Create IAM policy for the collector:\ncat \u0026lt;\u0026lt; EOF \u0026gt; AWSDistroOpenTelemetryPolicy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;xray:PutTraceSegments\u0026#34;, \u0026#34;xray:PutTelemetryRecords\u0026#34;, \u0026#34;xray:GetSamplingRules\u0026#34;, \u0026#34;xray:GetSamplingTargets\u0026#34;, \u0026#34;xray:GetSamplingStatisticSummaries\u0026#34;, \u0026#34;cloudwatch:PutMetricData\u0026#34;, \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:DescribeTags\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } EOF aws iam create-policy --region=${AWS_REGION} \\  --policy-name AWSDistroOpenTelemetryPolicy \\  --policy-document file://AWSDistroOpenTelemetryPolicy.json export ADOT_IAMPOLICY_ARN=$(aws iam list-policies --region=${AWS_REGION} | jq \u0026#39;.Policies | .[] | select(.PolicyName == \u0026#34;AWSDistroOpenTelemetryPolicy\u0026#34;).Arn\u0026#39; --raw-output) echo -e \u0026#34;Created ADOT IAM Policy: $ADOT_IAMPOLICY_ARN\u0026#34; Create an IAM role for the collector:\nread -r -d '' TRUST_RELATIONSHIP \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Federated\u0026quot;: \u0026quot;arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRoleWithWebIdentity\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;${OIDC_PROVIDER}:sub\u0026quot;: \u0026quot;system:serviceaccount:aws-otel-eks:aws-otel-sa\u0026quot; } } } ] } EOF echo \u0026quot;${TRUST_RELATIONSHIP}\u0026quot; \u0026gt; trust.json aws iam create-role --role-name AWSDistroOpenTelemetryRole \\ --assume-role-policy-document file://trust.json \\ --description \u0026quot;IAM Role for ADOT\u0026quot; Associate the IAM Policy (AWSDistroOpenTelemetryPolicy) with the IAM Role (AWSDistroOpenTelemetryRole):\naws iam attach-role-policy --role-name AWSDistroOpenTelemetryRole \\  --policy-arn=$ADOT_IAMPOLICY_ARN "
},
{
	"uri": "/advanced/350_opentelemetry/otel_collector/collector-manifest/",
	"title": "Collector Manifest",
	"tags": [],
	"description": "",
	"content": "Configuring AWS Distro for Open Telemetry Collector Kubernetes manifest Open $ vim kubernetes/adot/otel-container-insights-infra.yaml. Inside you‚Äôll notice an annotation for the AWSDistroOpenTelemetryRole in the ServiceAccount resource. --- # create cwagent service account and role binding apiVersion: v1 kind: ServiceAccount metadata: name: aws-otel-sa namespace: aws-otel-eks annotations: eks.amazonaws.com/role-arn: arn:aws:iam::${ACCOUNT_ID}:role/AWSDistroOpenTelemetryRole  Also take a look at the ConfigMap outlining the OTEL receivers, processors, and exporters: data: otel-agent-config: | extensions: health_check: receivers: awscontainerinsightreceiver: processors: batch/metrics: timeout: 60s exporters: awsemf: namespace: ContainerInsights ...  Scroll down further in the file, and you\u0026rsquo;ll see a pipeline, which grabs data from the Container Insight receiver, batching, and exporting to CloudWatch (Embedded Metric).\nOpen Telemetry allows multiple receivers, processors, and exporters in a pipeline. For example if you wanted to send all these collected Container Insight metrics to a second destination it‚Äôd be as simple as defining the additional exporter and adding it to the pipeline.\nNow Let‚Äôs close the file by hitting esc, typing :q! , and hitting enter.\n"
},
{
	"uri": "/advanced/350_opentelemetry/otel_collector/deploying-collector/",
	"title": "Deploying the Collector",
	"tags": [],
	"description": "",
	"content": "Deploying Open Telemetry Collector Set variables in the OTEL collector\u0026rsquo;s kubernetes manifest file:\nenvsubst \u0026lt; kubernetes/adot/otel-container-insights-infra.yaml | sponge kubernetes/adot/otel-container-insights-infra.yaml Install the OTEL collector to the EKS Cluster as a DaemonSet:\nkubectl apply -f kubernetes/adot/otel-container-insights-infra.yaml Check to see that the OTEL collector‚Äôs DaemonSet is running on each node: $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-29-122.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 2d6h v1.21.2-eks-55daa9d ip-192-168-43-204.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 2d6h v1.21.2-eks-55daa9d ip-192-168-70-26.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 2d6h v1.21.2-eks-55daa9d $ kubectl get pods -n aws-otel-eks -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES aws-otel-eks-ci-47x6x 1/1 Running 0 9m26s 192.168.39.13 ip-192-168-43-204.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; aws-otel-eks-ci-hj8vs 1/1 Running 0 9m26s 192.168.13.246 ip-192-168-29-122.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; aws-otel-eks-ci-vhcrs 1/1 Running 0 9m26s 192.168.78.46 ip-192-168-70-26.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  At this point the OTEL collector is installed and sending CloudWatch Insights metrics to CloudWatch. Open your CloudWatch console, and head to Metrics \u0026gt; All Metrics. Then select the Pod level metrics: Filter to just CPU Utilization, and change the visualization to be of type Number. And you\u0026rsquo;ll have a dashboard of the deployed Pod\u0026rsquo;s CPU Utilization: "
},
{
	"uri": "/beginner/",
	"title": "Beginner",
	"tags": ["beginner"],
	"description": "",
	"content": "Beginner "
},
{
	"uri": "/beginner/110_irsa/deploy/",
	"title": "Deploy Sample Pod",
	"tags": [],
	"description": "",
	"content": "Now that we have completed all the necessary configuration, we will run two kubernetes jobs with the newly created IAM role:\n job-s3.yaml: that will output the result of the command aws s3 ls (this job should be successful). job-ec2.yaml: that will output the result of the command aws ec2 describe-instances --region ${AWS_REGION} (this job should failed).  Before deploying the workloads, make sure to have the environment variables AWS_REGION and ACCOUNT_ID configured in your terminal prompt.\nList S3 buckets Let\u0026rsquo;s start by testing if the service account can list the S3 buckets\nmkdir ~/environment/irsa cat \u0026lt;\u0026lt;EoF\u0026gt; ~/environment/irsa/job-s3.yaml apiVersion: batch/v1 kind: Job metadata: name: eks-iam-test-s3 spec: template: metadata: labels: app: eks-iam-test-s3 spec: serviceAccountName: iam-test containers: - name: eks-iam-test image: amazon/aws-cli:latest args: [\u0026#34;s3\u0026#34;, \u0026#34;ls\u0026#34;] restartPolicy: Never EoF kubectl apply -f ~/environment/irsa/job-s3.yaml Make sure your job is completed.\nkubectl get job -l app=eks-iam-test-s3 Output: NAME COMPLETIONS DURATION AGE eks-iam-test-s3 1/1 2s 21m  Let\u0026rsquo;s check the logs to verify that the command ran successfully.\nkubectl logs -l app=eks-iam-test-s3 Output: 2021-07-17 20:09:41 eksworkshop-eksctl-helm-charts 2021-07-18 19:22:37 eksworkshop-logs  If the output lists some buckets, please move on to List EC2 Instances. If not, it is possible your account doesn\u0026rsquo;t have any s3 buckets. Please try to run theses extra commands.\n Let\u0026rsquo;s create an S3 bucket.\naws s3 mb s3://eksworkshop-$ACCOUNT_ID-$AWS_REGION --region $AWS_REGION Output: make_bucket: eksworkshop-40XXXXXXXX75-us-east-1  Now, let\u0026rsquo;s try that job again. But first, we should remove the old job.\nkubectl delete job -l app=eks-iam-test-s3 Then we can re-create the job.\nkubectl apply -f ~/environment/irsa/job-s3.yaml Finally, we can have a look at the output.\nkubectl logs -l app=eks-iam-test-s3 Output: 2021-07-21 14:06:24 eksworkshop-40XXXXXXXX75-us-east-1  List EC2 Instances Now Let\u0026rsquo;s confirm that the service account cannot list the EC2 instances\ncat \u0026lt;\u0026lt;EoF\u0026gt; ~/environment/irsa/job-ec2.yaml apiVersion: batch/v1 kind: Job metadata: name: eks-iam-test-ec2 spec: template: metadata: labels: app: eks-iam-test-ec2 spec: serviceAccountName: iam-test containers: - name: eks-iam-test image: amazon/aws-cli:latest args: [\u0026#34;ec2\u0026#34;, \u0026#34;describe-instances\u0026#34;, \u0026#34;--region\u0026#34;, \u0026#34;${AWS_REGION}\u0026#34;] restartPolicy: Never backoffLimit: 0 EoF kubectl apply -f ~/environment/irsa/job-ec2.yaml Let\u0026rsquo;s verify the job status\nkubectl get job -l app=eks-iam-test-ec2 Output: NAME COMPLETIONS DURATION AGE eks-iam-test-ec2 0/1 39s 39s  It is normal that the job didn\u0026rsquo;t complete succesfuly.\n Finally we will review the logs\nkubectl logs -l app=eks-iam-test-ec2 Output: An error occurred (UnauthorizedOperation) when calling the DescribeInstances operation: You are not authorized to perform this operation.  "
},
{
	"uri": "/advanced/430_emr_on_eks/spot_instances_1/",
	"title": "Using Spot Instances Part 1 - Setup",
	"tags": [],
	"description": "",
	"content": "EC2 Spot Instances Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 90% discount compared to On-Demand prices. You can use Spot Instances for various stateless, fault-tolerant, or flexible applications such as big data, containerized workloads, CI/CD, web servers, high-performance computing (HPC), and test \u0026amp; development workloads. Click here to know more about Spot Instances.\nEKS Managed Node Groups with Spot Instances Previously on EMR on EKS Prerequisites, you created managed node group named emrnodegroup with On-Demand Instances. Now you will be creating another managed node group with Spot Instances.\nManaged node groups automatically create a label - eks.amazonaws.com/capacityType - to identify which nodes are Spot Instances and which are On-Demand Instances. We will use this label to schedule the appropriate workloads to run on Spot Instances.\nRefer this deep dive blog to learn the best practices you need to follow to provision, manage and maintain EKS managed node groups with Spot Instances.\n Create a Managed Node Group with Spot Instances To maximize the availability of your applications while using Spot Instances, we recommend that you configure a managed node group to use multiple instance types. When selecting what instance types to use, you can look to all instance types that supply a certain amount of vCPUs and memory to the cluster, and group those in each node group. For example, a single node group can be configured with: m5.xlarge, m4.xlarge, m5a.xlarge, m5d.xlarge, m5ad.xlarge, m5n.xlarge, and m5dn.xlarge. These instance types supply almost identical vCPU and memory capacity, which is important in order for Kubernetes cluster autoscaler to efficiently scale the node groups.\nEC2 Instance Selector is an open source tool that can help you find suitable instance types with a single CLI command.\n We will now create a managed node group with Spot Instances, let\u0026rsquo;s create a config file (addnodegroup-spot.yaml) with details of a new managed node group.\ncat \u0026lt;\u0026lt; EOF \u0026gt; addnodegroup-spot.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} managedNodeGroups: - name: emrnodegroup-spot minSize: 1 desiredCapacity: 3 maxSize: 10 instanceTypes: [\u0026#34;m5.xlarge\u0026#34;,\u0026#34;m4.xlarge\u0026#34;,\u0026#34;m5d.xlarge\u0026#34;,\u0026#34;m5n.xlarge\u0026#34;,\u0026#34;m5dn.xlarge\u0026#34;,\u0026#34;m5a.xlarge\u0026#34;,\u0026#34;m5ad.xlarge\u0026#34;] spot: true ssh: enableSsm: true EOF Create the new EKS managed nodegroup with Spot Instances.\neksctl create nodegroup --config-file=addnodegroup-spot.yaml  Creation of node group will take 3-4 minutes.\n You can use the eks.amazonaws.com/capacityType label to identify the lifecycle of the nodes. The output of this command should return nodes with the capacityType set to SPOT.\nkubectl get nodes \\  --label-columns=eks.amazonaws.com/capacityType \\  --selector=eks.amazonaws.com/capacityType=SPOT  You can review the dedicated chapter on Spot Instances SPOT CONFIGURATION AND LIFECYCLE  to learn more about how EKS managed node groups handle Spot Interruption automatically.\n "
},
{
	"uri": "/advanced/340_appmesh_flagger/testing/",
	"title": "Testing Canary Deployment",
	"tags": [],
	"description": "",
	"content": "Automated Canary Promotion According to the Flagger documentation, canary deployment is triggered by changes in any of the following objects:\n Deployment PodSpec (container image, command, ports, env, resources, etc). ConfigMaps and Secrets mounted as volumes or mapped to environment variables  We will trigger a canary deployment by updating the container image of detail service to version 2. Flagger will detect changes to the target deployment and will perform a canary analysis before promoting the new version as primary.\nexport APP_VERSION_2=2.0 kubectl -n flagger set image deployment/detail detail=public.ecr.aws/u2g6w7p2/eks-microservice-demo/detail:${APP_VERSION_2} deployment.apps/detail image updated  Flagger detects that the deployment revision changed and starts a new rollout. You can see the log events for this deployment\nkubectl -n appmesh-system logs deploy/flagger --tail 10 -f | jq .msg \u0026#34;New revision detected! Scaling up detail.flagger\u0026#34; \u0026#34;Starting canary analysis for detail.flagger\u0026#34; \u0026#34;Pre-rollout check acceptance-test passed\u0026#34; \u0026#34;Advance detail.flagger canary weight 5\u0026#34; \u0026#34;Advance detail.flagger canary weight 10\u0026#34; \u0026#34;Advance detail.flagger canary weight 15\u0026#34; \u0026#34;Copying detail.flagger template spec to detail-primary.flagger\u0026#34; \u0026#34;Routing all traffic to primary\u0026#34; \u0026#34;Promotion completed! Scaling down detail.flagger\u0026#34;  You can also check the events using the below command\nkubectl describe canary detail -n flagger Normal Synced 5m38s flagger New revision detected! Scaling up detail.flagger Normal Synced 4m38s flagger Starting canary analysis for detail.flagger Normal Synced 4m38s flagger Pre-rollout check acceptance-test passed Normal Synced 4m38s flagger Advance detail.flagger canary weight 5 Normal Synced 3m38s flagger Advance detail.flagger canary weight 10 Normal Synced 2m38s flagger Advance detail.prodcatalog-ns canary weight 15 Normal Synced 13s (x2 over 73s) flagger (combined from similar events): Copying detail.flagger template spec to detail-primary.flagger Normal Synced 7s (x3 over 2m7s) flagger (combined from similar events): Routing all traffic to primary Normal Synced 14s (x4 over 3m14s) flagger (combined from similar events): Promotion completed! Scaling down detail.flagger  When the canary analysis starts, Flagger will call the pre-rollout webhooks before routing traffic to the canary. Note that if you apply new changes to the deployment during the canary analysis, Flagger will restart the analysis.\nGo to the LoadBalancer endpoint in browser and verify if new version 2 has been deployed. Automated Rollback We will create the scenario for automated rollback. During the canary analysis we will generate HTTP 500 errors to test if Flagger pauses the rollout.\nTrigger a canary deployment by updating the container image of detail service to version 3:\nexport APP_VERSION_3=3.0 kubectl -n flagger set image deployment/detail detail=public.ecr.aws/u2g6w7p2/eks-microservice-demo/detail:${APP_VERSION_3} deployment.apps/detail image updated  Once the canary analysis starts, you see the below message\nkubectl -n appmesh-system logs deploy/flagger --tail 10 -f | jq .msg \u0026#34;New revision detected! Scaling up detail.flagger\u0026#34; \u0026#34;Starting canary analysis for detail.flagger\u0026#34;  Exec into the loadtester pod\nkubectl -n flagger exec -it deploy/flagger-loadtester bash Defaulting container name to loadtester. Use \u0026#39;kubectl describe pod/flagger-loadtester-5bdf76cfb7-wl59d -n flagger\u0026#39; to see all of the containers in this pod. bash-5.0$  Generate HTTP 500 errors from http://detail-canary.flagger:3000/catalogDetail\ncurl http://detail-canary.flagger:3000/injectFault hey -z 1m -c 5 -q 5 http://detail-canary.flagger:3000/catalogDetail When the number of failed checks reaches the canary analysis threshold which we have set as 1 in our setup, the traffic is routed back to the primary, the canary is scaled to zero and the rollout is marked as failed.\nkubectl -n appmesh-system logs deploy/flagger --tail 10 -f | jq .msg \u0026#34;New revision detected! Scaling up detail.flagger\u0026#34; \u0026#34;Starting canary analysis for detail.flagger\u0026#34; \u0026#34;Pre-rollout check acceptance-test passed\u0026#34; \u0026#34;Advance detail.flagger canary weight 5\u0026#34; \u0026#34;Halt detail.flagger advancement success rate 23.03% \u0026lt; 99%\u0026#34; \u0026#34;Rolling back detail.flagger failed checks threshold reached 1\u0026#34; \u0026#34;Canary failed! Scaling down detail.flagger\u0026#34;  Go to the LoadBalancer endpoint in browser and you will still see version 2 and version 3 did not get deployed. Redeploy Version 3 again Lets deploy the version 3 again, this time without injecting errors.\nAs per the Flagger FAQ you can set an update to an annotation so that flagger knows to retry the release. The canary that failed has already been updated to the new image version 3, which is why setting the image version to the same value a second time will have no effect. So we need to update the annotation of the pod spec, which will then trigger a new canary progressing which is what we will be doing using below yaml.\nexport APP_VERSION_3=3.0 envsubst \u0026lt; ./flagger/flagger-app_noerror.yaml | kubectl apply -f - deployment.apps/detail configured  Flagger detects that the deployment revision changed and starts a new rollout:\nkubectl -n appmesh-system logs deploy/flagger --tail 10 -f | jq .msg \u0026#34;New revision detected! Scaling up detail.flagger\u0026#34; \u0026#34;Starting canary analysis for detail.flagger\u0026#34; \u0026#34;Pre-rollout check acceptance-test passed\u0026#34; \u0026#34;Advance detail.flagger canary weight 5\u0026#34; \u0026#34;Advance detail.flagger canary weight 10\u0026#34; \u0026#34;Advance detail.flagger canary weight 15\u0026#34; \u0026#34;Advance detail.flagger canary weight 20\u0026#34; \u0026#34;Copying detail.flagger template spec to detail-primary.flagger\u0026#34; \u0026#34;Routing all traffic to primary\u0026#34; \u0026#34;Promotion completed! Scaling down detail.flagger\u0026#34;  Go to the LoadBalancer endpoint in browser and verify if new version 3 has been deployed. "
},
{
	"uri": "/intermediate/246_monitoring_amp_amg/create_amg_workspace/",
	"title": "Create AMG workspace",
	"tags": [],
	"description": "",
	"content": "Prerequisite AMG requires AWS SSO enabled in your account. AWS SSO is used as the authentication provider to sign into the AMG workspace.\nFollow the steps below to enable AWS SSO in your account  Sign in to the AWS Management Console with your AWS Organizations management account credentials. Open the AWS SSO console. Choose Enable AWS SSO.  If you have not yet set up AWS Organizations, you will be prompted to create an organization. Choose Create AWS organization to complete this process.\nNow go ahead and create a new AWS SSO user that we will use to provide access to the AMG workspace later.\nCreate AMG workspace Go to the AMG console and provide a workspace name as shown below Choose Service managed in the Configure Settings page and click Next. Choosing this option will allow the wizard to automatically provision the permissions for you based on the AWS services we will choose later on.\nIn the Service managed permission settings screen, you can choose to configure Grafana to monitor resources in the same account where you are creating the workspace or allow Grafana to reach into multiple AWS accounts by choosing the Organization option and providing the necessary OU IDs.\nWe will simply leave the option to Current account and select all the Data sources and the Notification channels. Click Next\nIn the Review screen, take a look at the options and click on Create workspace\nAdd Users Once the AMG workspace turns to ACTIVE, click on Assign user and select the SSO user created in previously. Click Assign user\nBy default, all newly assigned users are added as Viewers that only provides read-only permissions on Grafana. To make the user as Administrator, select the user under Users and select Make admin. Now you should see that the user is an Administrator.\n"
},
{
	"uri": "/beginner/115_sg-per-pod/50_deploy/",
	"title": "Pods Deployments",
	"tags": ["beginner"],
	"description": "",
	"content": "Kubernetes secrets Before deploying our two pods we need to provide them with the RDS endpoint and password. We will create a kubernetes secret.\nexport RDS_PASSWORD=$(cat ~/environment/sg-per-pod/rds_password) export RDS_ENDPOINT=$(aws rds describe-db-instances \\  --db-instance-identifier rds-eksworkshop \\  --query \u0026#39;DBInstances[0].Endpoint.Address\u0026#39; \\  --output text) kubectl create secret generic rds\\  --namespace=sg-per-pod \\  --from-literal=\u0026#34;password=${RDS_PASSWORD}\u0026#34; \\  --from-literal=\u0026#34;host=${RDS_ENDPOINT}\u0026#34; kubectl -n sg-per-pod describe secret rds Output\nName: rds Namespace: sg-per-pod Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== host: 56 bytes password: 32 bytes  Deployments Let\u0026rsquo;s download both pods deployment files\ncd ~/environment/sg-per-pod curl -s -O https://www.eksworkshop.com/beginner/115_sg-per-pod/deployments.files/green-pod.yaml curl -s -O https://www.eksworkshop.com/beginner/115_sg-per-pod/deployments.files/red-pod.yaml Take some time to explore both YAML files and see the different between the two.\nGreen Pod Now let\u0026rsquo;s deploy the green pod\nkubectl -n sg-per-pod apply -f ~/environment/sg-per-pod/green-pod.yaml kubectl -n sg-per-pod rollout status deployment green-pod The container will try to:\n Connect to the database and will output the content of a table to STDOUT. If the database connection failed, the error message will also be outputted to STDOUT.  Let\u0026rsquo;s verify the logs.\nexport GREEN_POD_NAME=$(kubectl -n sg-per-pod get pods -l app=green-pod -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n sg-per-pod logs -f ${GREEN_POD_NAME} Output\n[(\u0026#39;--------------------------\u0026#39;,), (\u0026#39;Welcome to the eksworkshop\u0026#39;,), (\u0026#39;--------------------------\u0026#39;,)] [(\u0026#39;--------------------------\u0026#39;,), (\u0026#39;Welcome to the eksworkshop\u0026#39;,), (\u0026#39;--------------------------\u0026#39;,)]  use CTRL+C to exit the log\n As we can see, our attempt was successful!\nNow let\u0026rsquo;s verify that:\n An ENI is attached to the pod. And the ENI has the security group POD_SG attached to it.  We can find the ENI ID in the pod Annotations section using this command.\nkubectl -n sg-per-pod describe pod $GREEN_POD_NAME | head -11 Output\nName: green-pod-5c786d8dff-4kmvc Namespace: sg-per-pod Priority: 0 Node: ip-192-168-33-222.us-east-2.compute.internal/192.168.33.222 Start Time: Thu, 03 Dec 2020 05:25:54 \u0026#43;0000 Labels: app=green-pod pod-template-hash=5c786d8dff Annotations: kubernetes.io/psp: eks.privileged vpc.amazonaws.com/pod-eni: [{\u0026#34;eniId\u0026#34;:\u0026#34;eni-0d8a3a3a7f2eb57ab\u0026#34;,\u0026#34;ifAddress\u0026#34;:\u0026#34;06:20:0d:3c:5f:bc\u0026#34;,\u0026#34;privateIp\u0026#34;:\u0026#34;192.168.47.64\u0026#34;,\u0026#34;vlanId\u0026#34;:1,\u0026#34;subnetCidr\u0026#34;:\u0026#34;192.168.32.0/19\u0026#34;}] Status: Running  You can verify that the security group POD_SG is attached to the eni shown above by opening this link.\nRed Pod We will deploy the red pod and verify that it\u0026rsquo;s unable to connect to the database.\nJust like for the green pod, the container will try to:\n Connect to the database and will output to STDOUT the content of a table. If the database connection failed, the error message will also be outputted to STDOUT.  kubectl -n sg-per-pod apply -f ~/environment/sg-per-pod/red-pod.yaml kubectl -n sg-per-pod rollout status deployment red-pod Let\u0026rsquo;s verify the logs (use CTRL+C to exit the log)\nexport RED_POD_NAME=$(kubectl -n sg-per-pod get pods -l app=red-pod -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n sg-per-pod logs -f ${RED_POD_NAME} Output\nDatabase connection failed due to timeout expired  Finally let\u0026rsquo;s verify that the pod doesn\u0026rsquo;t have an eniId annotation.\nkubectl -n sg-per-pod describe pod ${RED_POD_NAME} | head -11 Output\nName: red-pod-7f68d78475-vlm77 Namespace: sg-per-pod Priority: 0 Node: ip-192-168-6-158.us-east-2.compute.internal/192.168.6.158 Start Time: Thu, 03 Dec 2020 07:08:28 \u0026#43;0000 Labels: app=red-pod pod-template-hash=7f68d78475 Annotations: kubernetes.io/psp: eks.privileged Status: Running IP: 192.168.0.188 IPs:  Conclusion In this module, we configured our EKS cluster to enable the security groups per pod feature.\nWe created a SecurityGroup Policy and deployed 2 pods (using the same docker image) and a RDS Database protected by a Security Group.\nBased on this policy, only one of the two pods was able to connect to the database.\nFinally using the CLI and the AWS console, we were able to locate the pod\u0026rsquo;s ENI and verify that the Security Group was attached to it.\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/virtual_gateway_setup/add_virtual_gateway/",
	"title": "Add VirtualGateway",
	"tags": [],
	"description": "",
	"content": "Adding App Mesh VirtualGateway Until now we have verified the communication between services is routed through envoy proxy, lets expose the frontend service frontend-node using AWS App Mesh VirtualGateway.\nCreate VirtualGateway components uisng the virtual_gateway.yaml as shown below. This will create the kubernetes service as Type Load Balancer and will use the AWS Network Load balancer for routing the external internet traffic.\nkubectl apply -f deployment/virtual_gateway.yaml virtualgateway.appmesh.k8s.aws/ingress-gw created gatewayroute.appmesh.k8s.aws/gateway-route-frontend created service/ingress-gw created deployment.apps/ingress-gw created  Get all the resources that are running in the namespace You can see VirtualGateway components named as ingress-gw below:\nkubectl get all -n prodcatalog-ns -o wide | grep ingress pod/ingress-gw-5fb995f6fd-45nnm 2/2 Running 0 35s 192.168.24.144 ip-192-168-21-156.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; service/ingress-gw LoadBalancer 10.100.24.17 ad34ee9dea9944ed78e78d0578060ba6-869c67fd174d0f4d.elb.us-west-2.amazonaws.com 80:31569/TCP 35s app=ingress-gw deployment.apps/ingress-gw 1/1 1 1 35s envoy 840364872350.dkr.ecr.us-west-2.amazonaws.com/aws-appmesh-envoy:v1.15.1.0-prod app=ingress-gw replicaset.apps/ingress-gw-5fb995f6fd 1 1 1 35s envoy 840364872350.dkr.ecr.us-west-2.amazonaws.com/aws-appmesh-envoy:v1.15.1.0-prod app=ingress-gw,pod-template-hash=5fb995f6fd virtualgateway.appmesh.k8s.aws/ingress-gw arn:aws:appmesh:us-west-2:405710966773:mesh/prodcatalog-mesh/virtualGateway/ingress-gw_prodcatalog-ns 35s gatewayroute.appmesh.k8s.aws/gateway-route-frontend arn:aws:appmesh:us-west-2:405710966773:mesh/prodcatalog-mesh/virtualGateway/ingress-gw_prodcatalog-ns/gatewayRoute/gateway-route-frontend_prodcatalog-ns 35s  Log into console and navigate to AWS App Mesh -\u0026gt; Click on prodcatalog-mesh -\u0026gt; Click on Virtual gateways, you should see below page. "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/canary_deployment/",
	"title": "Canary Release",
	"tags": [],
	"description": "",
	"content": "A canary release is a method of slowly exposing a new version of software. The theory behind it is that by serving the new version of the software initially to say, 5% of requests, if there is a problem, the problem only impacts a very small percentage of users before its discovered and rolled back.\nSo now back to our Product Catalog App scenario, proddetail-v2 service is released, and they now include more product catalog vendors e.g \u0026ldquo;XYZ.com\u0026rdquo; (see below). { \u0026#34;version\u0026#34;:\u0026#34;2\u0026#34;, \u0026#34;names\u0026#34;:[\u0026#34;ABC.com\u0026#34;,\u0026#34;XYZ.com\u0026#34;] }  Let\u0026rsquo;s see how we can release this new version of proddetail-v2 in a canary fashion using AWS App Mesh. When we\u0026rsquo;re done, our app will look more like the following:\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/port_to_app_mesh/create_meshed_app/",
	"title": "Create the Meshed Application",
	"tags": [],
	"description": "",
	"content": "Create Mesh Object Configure namespace with App Mesh Labels and deploy Mesh Object\nkubectl apply -f deployment/mesh.yaml namespace/prodcatalog-ns configured mesh.appmesh.k8s.aws/prodcatalog-mesh created  Confirm the Mesh object and Namespace are created\nkubectl describe namespace prodcatalog-ns Name: prodcatalog-ns Labels: appmesh.k8s.aws/sidecarInjectorWebhook=enabled gateway=ingress-gw mesh=prodcatalog-mesh Annotations: Status: Active  kubectl describe mesh prodcatalog-mesh Status: Conditions: Last Transition Time: 2020-11-02T16:43:03Z Status: True Type: MeshActive  Create App Mesh Resources for the services kubectl apply -f deployment/meshed_app.yaml virtualnode.appmesh.k8s.aws/prodcatalog created virtualservice.appmesh.k8s.aws/prodcatalog created virtualservice.appmesh.k8s.aws/proddetail created virtualrouter.appmesh.k8s.aws/proddetail-router created virtualrouter.appmesh.k8s.aws/prodcatalog-router created virtualnode.appmesh.k8s.aws/proddetail-v1 created virtualnode.appmesh.k8s.aws/frontend-node created virtualservice.appmesh.k8s.aws/frontend-node created  Get all the Meshed resources for your application services, you should see below response.\nkubectl get virtualnode,virtualservice,virtualrouter -n prodcatalog-ns NAME ARN AGE virtualnode.appmesh.k8s.aws/frontend-node arn:aws:appmesh:us-west-2:$ACCOUNT_ID:mesh/prodcatalog-mesh/virtualNode/frontend-node_prodcatalog-ns 3m4s virtualnode.appmesh.k8s.aws/prodcatalog arn:aws:appmesh:us-west-2:$ACCOUNT_ID:mesh/prodcatalog-mesh/virtualNode/prodcatalog_prodcatalog-ns 35m virtualnode.appmesh.k8s.aws/proddetail-v1 arn:aws:appmesh:us-west-2:$ACCOUNT_ID:mesh/prodcatalog-mesh/virtualNode/proddetail-v1_prodcatalog-ns 35m NAME ARN AGE virtualservice.appmesh.k8s.aws/frontend-node arn:aws:appmesh:us-west-2:$ACCOUNT_ID:mesh/prodcatalog-mesh/virtualService/frontend-node.prodcatalog-ns.svc.cluster.local 3m4s virtualservice.appmesh.k8s.aws/prodcatalog arn:aws:appmesh:us-west-2:$ACCOUNT_ID:mesh/prodcatalog-mesh/virtualService/prodcatalog.prodcatalog-ns.svc.cluster.local 35m virtualservice.appmesh.k8s.aws/proddetail arn:aws:appmesh:us-west-2:$ACCOUNT_ID:mesh/prodcatalog-mesh/virtualService/proddetail.prodcatalog-ns.svc.cluster.local 35m NAME ARN AGE virtualrouter.appmesh.k8s.aws/prodcatalog-router arn:aws:appmesh:us-west-2:$ACCOUNT_ID:mesh/prodcatalog-mesh/virtualRouter/prodcatalog-router_prodcatalog-ns 35m virtualrouter.appmesh.k8s.aws/proddetail-router arn:aws:appmesh:us-west-2:$ACCOUNT_ID:mesh/prodcatalog-mesh/virtualRouter/proddetail-router_prodcatalog-ns 35m  Go to Console and check the App Mesh Resources information "
},
{
	"uri": "/advanced/420_kubeflow/inference/",
	"title": "Model inference",
	"tags": [],
	"description": "",
	"content": "Model Inference After the model is trained and stored in S3 bucket, the next step is to use that model for inference.\nThis chapter explains how to use the previously trained model and run inference using TensorFlow and Keras on Amazon EKS.\nRun inference pod A model from training was stored in the S3 bucket in previous section. Make sure S3_BUCKET and AWS_REGION environment variables are set correctly.\ncurl -LO https://eksworkshop.com/advanced/420_kubeflow/kubeflow.files/mnist-inference.yaml envsubst \u0026lt;mnist-inference.yaml | kubectl apply -f - Wait for the containers to start and run the next command to check its status\nkubectl get pods -l app=mnist,type=inference You should see similar output\nNAME READY STATUS RESTARTS AGE mnist-96fb6f577-k8pm6 1/1 Running 0 116s Now, we are going to use Kubernetes port forward for the inference endpoint to do local testing:\nkubectl port-forward `kubectl get pods -l=app=mnist,type=inference -o jsonpath='{.items[0].metadata.name}' --field-selector=status.phase=Running` 8500:8500 Leave the current terminal running and open a new terminal for installing tensorflow\nInstall packages Install tensorflow package:\ncurl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user pip3 install requests tensorflow --user Run inference Use the script inference_client.py to make prediction request. It will randomly pick one image from test dataset and make prediction.\ncurl -LO https://eksworkshop.com/advanced/420_kubeflow/kubeflow.files/inference_client.py python inference_client.py --endpoint http://localhost:8500/v1/models/mnist:predict It will randomly pick one image from test dataset and make prediction.\nData: {\u0026#34;instances\u0026#34;: [[[[0.0], [0.0], [0.0], [0.0], [0.0] ... 0.0], [0.0]]]], \u0026#34;signature_name\u0026#34;: \u0026#34;serving_default\u0026#34;} The model thought this was a Ankle boot (class 9), and it was actually a Ankle boot (class 9)  Cleanup Now that we saw how to run training job and inference, let\u0026rsquo;s terminate these pods to free up resources\nkubectl delete -f mnist-training.yaml kubectl delete -f mnist-inference.yaml "
},
{
	"uri": "/beginner/160_advanced-networking/secondary_cidr/test_networking/",
	"title": "Test Networking",
	"tags": [],
	"description": "",
	"content": "Launch pods into Secondary CIDR network Let\u0026rsquo;s launch few pods and test networking\nkubectl create deployment nginx --image=nginx kubectl scale --replicas=3 deployments/nginx kubectl expose deployment/nginx --type=NodePort --port 80 kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE nginx-64f497f8fd-k962k 1/1 Running 0 40m 100.64.6.147 ip-192-168-52-113.us-east-2.compute.internal \u0026lt;none\u0026gt; nginx-64f497f8fd-lkslh 1/1 Running 0 40m 100.64.53.10 ip-192-168-74-125.us-east-2.compute.internal \u0026lt;none\u0026gt; nginx-64f497f8fd-sgz6f 1/1 Running 0 40m 100.64.80.186 ip-192-168-26-65.us-east-2.compute.internal \u0026lt;none\u0026gt;  You can use busybox pod and ping pods within same host or across hosts using IP address\nkubectl run -i --rm --tty debug --image=busybox -- sh Test access to internet and to nginx service # connect to internet / # wget google.com -O - Connecting to google.com (172.217.5.238:80) Connecting to www.google.com (172.217.5.228:80) \u0026lt;!doctype html\u0026gt;\u0026lt;html itemscope=\u0026#34;\u0026#34; itemtype=\u0026#34;http://schema.org/WebPage\u0026#34; lang=\u0026#34;en\u0026#34;\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta content=\u0026#34;Search the world\u0026#39;s information, including webpages, images, videos and more. Google has many special ... # connect to service (testing core-dns) / # wget nginx -O - Connecting to nginx (10.100.170.156:80) \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ...  "
},
{
	"uri": "/advanced/410_batch/artifact/",
	"title": "Configure Artifact Repository",
	"tags": [],
	"description": "",
	"content": "Configure Artifact Repository Argo uses an artifact repository to pass data between jobs in a workflow, known as artifacts. Amazon S3 can be used as an artifact repository.\nLet\u0026rsquo;s create a S3 bucket using the AWS CLI.\naws s3 mb s3://batch-artifact-repository-${ACCOUNT_ID}/ Next, we will add this bucket as an argo artifactRepository in the configmap workflow-controller-configmap\nCreate the patch\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/batch_policy/argo-patch.yaml data: config: | artifactRepository: s3: bucket: batch-artifact-repository-${ACCOUNT_ID} endpoint: s3.amazonaws.com EoF deploy the patch\nkubectl -n argo patch \\  configmap/workflow-controller-configmap \\  --patch \u0026#34;$(cat ~/environment/batch_policy/argo-patch.yaml)\u0026#34; Let\u0026rsquo;s verify the configmap\nkubectl -n argo get configmap/workflow-controller-configmap -o yaml Output Example\napiVersion: v1 data: config: |- artifactRepository: s3: bucket: batch-artifact-repository-197520326489 endpoint: s3.amazonaws.com kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;workflow-controller-configmap\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;argo\u0026#34;}} creationTimestamp: \u0026#34;2020-07-07T19:07:48Z\u0026#34; name: workflow-controller-configmap namespace: argo resourceVersion: \u0026#34;1082653\u0026#34; selfLink: /api/v1/namespaces/argo/configmaps/workflow-controller-configmap uid: a1accd9e-c528-41a3-b811-226f5662c446  Create an IAM Policy In order for Argo to read from/write to the S3 bucket, we need to configure an inline policy and add it to the EC2 instance profile of the worker nodes.\nFirst, we will need to ensure the Role Name our workers use is set in our environment:\ntest -n \u0026#34;$ROLE_NAME\u0026#34; \u0026amp;\u0026amp; echo ROLE_NAME is \u0026#34;$ROLE_NAME\u0026#34; || echo ROLE_NAME is not set If you receive an error or empty response, expand the steps below to export.\n  Expand here if you need to export the Role Name   If ROLE_NAME is not set, please review: /030_eksctl/test/\n  # Example Output ROLE_NAME is eksctl-eksworkshop-eksctl-nodegro-NodeInstanceRole-RPDET0Z4IJIF  Create and policy and attach to the worker node role.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/batch_policy/k8s-s3-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::batch-artifact-repository-${ACCOUNT_ID}\u0026#34;, \u0026#34;arn:aws:s3:::batch-artifact-repository-${ACCOUNT_ID}/*\u0026#34; ] } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker --policy-document file://~/environment/batch_policy/k8s-s3-policy.json Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker "
},
{
	"uri": "/advanced/310_servicemesh_with_istio/routing/",
	"title": "Traffic Management",
	"tags": [],
	"description": "",
	"content": "Create the default destination rules Deploying a microservice-based application in an Istio service mesh allows one to externally control service monitoring and tracing, request (version) routing, resiliency testing, security and policy enforcement, and more in a consistent manner across the services, and the application.\nBefore you can use Istio to control the Bookinfo version routing, you\u0026rsquo;ll need to define the available versions, called¬†subsets.\nIn a continuous deployment scenario, for a given service, there can be distinct subsets of instances running different variants of the application binary. These variants are not necessarily different API versions. They could be iterative changes to the same service, deployed in different environments (prod, staging, dev, etc.). Common scenarios where this occurs include A/B testing, canary rollouts, etc. The choice of a particular version can be decided based on various criterion (headers, url, etc.) and/or by weights assigned to each version. Each service has a default version consisting of all its instances.\nkubectl -n bookinfo apply \\  -f ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/networking/destination-rule-all.yaml We can display the destination rules with the following command.\nkubectl -n bookinfo get destinationrules -o yaml Route traffic to one version of a service To route to one version only, we apply virtual services that set the default version for the microservices. In this case, the virtual services will route all traffic to¬†reviews:v1¬†of the microservice.\nkubectl -n bookinfo \\  apply -f ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/networking/virtual-service-all-v1.yaml We can display the review virtual service with the following command.\nkubectl -n bookinfo get virtualservices reviews -o yaml The subset is set to v1 for all reviews request.\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1  Try now to reload the page multiple times, and note how only version 1 of reviews is displayed each time.\nRoute based on user identity Next, we\u0026rsquo;ll change the route configuration so that all traffic from a specific user is routed to a specific service version.\nIn this case, all traffic from a user named Jasonwill be routed to the service¬†reviews:v2.\nkubectl -n bookinfo \\  apply -f ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml We can display the updated virtual service with the following command.\nkubectl -n bookinfo get virtualservices reviews -o yaml The subset is set to v1 in default and route v2 if the logged user is match with \u0026lsquo;jason\u0026rsquo; for reviews request.\nspec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1  To test:\n Click Sign in from the top right corner of the page. Log in using jason as user name with a blank password.  You will only see reviews:v2 all the time. Others will see reviews:v1.\nInjecting an HTTP delay fault To test for resiliency, inject a 7s delay between the¬†reviews:v2¬†and¬†ratings¬†microservices for user¬†jason. This test will uncover a bug that was intentionally introduced into the Bookinfo app.\nkubectl -n bookinfo \\  apply -f ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml We can display the updated virtual service with the following command.\nkubectl -n bookinfo get virtualservice ratings -o yaml The subset is set to v1 in default and added 7s delay for all the request if the logged user is match with \u0026lsquo;jason\u0026rsquo; for ratings.\nspec: hosts: - ratings http: - fault: delay: fixedDelay: 7s percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1  Logout, then click Sign in from the top right corner of the page, using jason as the user name with a blank password. You will see the delays and it ends up display error for reviews. Others will see reviews without error.\nThe timeout between the¬†productpage¬†and the¬†reviews¬†service is 6 seconds - coded as 3s + 1 retry for 6s total.\nTo test for another resiliency, we will introduce an HTTP abort to the¬†ratings¬†microservices for the test user¬†jason. The page will immediately display the ‚ÄúRatings service is currently unavailable‚Äù\nkubectl -n bookinfo \\  apply -f ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml We can display the updated virtual service with the following command.\nkubectl -n bookinfo get virtualservice ratings -o yaml The subset is set to v1 and by default returns an error message of \u0026ldquo;Ratings service is currently unavailable\u0026rdquo; below the reviewer name if the logged username matches \u0026lsquo;jason\u0026rsquo;.\nspec: hosts: - ratings http: - fault: abort: httpStatus: 500 percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1  To test, click Sign in from the top right corner of the page and login using jason for the user name with a blank password. As jason you will see the error message.\nOthers (not logged in as jason) will see no error message.\nTraffic Shifting Next, we\u0026rsquo;ll demonstrate how to gradually migrate traffic from one version of a microservice to another. In our example, we\u0026rsquo;ll send 50% of traffic to¬†reviews:v1and 50% to¬†reviews:v3.\nTo get started, run this command to route all traffic to the v1 version of each microservice.\nkubectl -n bookinfo \\  apply -f ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/networking/virtual-service-all-v1.yaml Open the Bookinfo site in your browser. Notice that the reviews part of the page displays with no rating stars, no matter how many times you refresh.\nWe can now transfer 50% of the traffic from reviews:v1 to reviews:v3\nkubectl -n bookinfo \\  apply -f ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml kubectl -n bookinfo get virtualservice reviews -o yaml The subset is set to 50% of traffic to v1 and 50% of traffic to v3 for all reviews request.\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 50 - destination: host: reviews subset: v3 weight: 50  To test it, refresh your browser over and over, and you\u0026rsquo;ll see only reviews:v1 and reviews:v3.\nAssuming you decide that the reviews:v3 microservice is stable, you can route 100% of the traffic to it\nkubectl -n bookinfo apply -f ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/networking/virtual-service-reviews-v3.yaml Now when you refresh the /productpage you will always see reviews:v3 (red colored star ratings).\n"
},
{
	"uri": "/010_introduction/basics/concepts_objects/",
	"title": "K8s Objects Overview",
	"tags": [],
	"description": "",
	"content": "Kubernetes objects are entities that are used to represent the state of the cluster.\nAn object is a ‚Äúrecord of intent‚Äù ‚Äì once created, the cluster does its best to ensure it exists as defined. This is known as the cluster‚Äôs ‚Äúdesired state.‚Äù\nKubernetes is always working to make an object‚Äôs ‚Äúcurrent state‚Äù equal to the object‚Äôs ‚Äúdesired state.‚Äù A desired state can describe:\n What pods (containers) are running, and on which nodes IP endpoints that map to a logical group of containers How many replicas of a container are running And much more\u0026hellip;  Let‚Äôs explain these k8s objects in a bit more detail\u0026hellip;\n"
},
{
	"uri": "/beginner/090_rbac/create_role_and_binding/",
	"title": "Create the Role and Binding",
	"tags": [],
	"description": "",
	"content": "As mentioned earlier, we have our new user rbac-user, but its not yet bound to any roles. In order to do that, we\u0026rsquo;ll need to switch back to our default admin user.\nRun the following to unset the environmental variables that define us as rbac-user:\nunset AWS_SECRET_ACCESS_KEY unset AWS_ACCESS_KEY_ID To verify we\u0026rsquo;re the admin user again, and no longer rbac-user, issue the following command:\naws sts get-caller-identity The output should show the user is no longer rbac-user:\n{ \u0026#34;Account\u0026#34;: \u0026lt;AWS Account ID\u0026gt;, \u0026#34;UserId\u0026#34;: \u0026lt;AWS User ID\u0026gt;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;your AWS account ID\u0026gt;:assumed-role/eksworkshop-admin/i-123456789\u0026#34; }  Now that we\u0026rsquo;re the admin user again, we\u0026rsquo;ll create a role called pod-reader that provides list, get, and watch access for pods and deployments, but only for the rbac-test namespace. Run the following to create this role:\ncat \u0026lt;\u0026lt; EoF \u0026gt; rbacuser-role.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: rbac-test name: pod-reader rules: - apiGroups: [\u0026quot;\u0026quot;] # \u0026quot;\u0026quot; indicates the core API group resources: [\u0026quot;pods\u0026quot;] verbs: [\u0026quot;list\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;extensions\u0026quot;,\u0026quot;apps\u0026quot;] resources: [\u0026quot;deployments\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] EoF We have the user, we have the role, and now we\u0026rsquo;re bind them together with a RoleBinding resource. Run the following to create this RoleBinding:\ncat \u0026lt;\u0026lt; EoF \u0026gt; rbacuser-role-binding.yaml kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-pods namespace: rbac-test subjects: - kind: User name: rbac-user apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io EoF Next, we apply the Role, and RoleBindings we created:\nkubectl apply -f rbacuser-role.yaml kubectl apply -f rbacuser-role-binding.yaml "
},
{
	"uri": "/beginner/050_deploy/",
	"title": "Deploy the Example Microservices",
	"tags": ["beginner", "CON203"],
	"description": "",
	"content": "Deploy the Example Microservices    Deploy our Sample Applications   Deploy NodeJS Backend API   Deploy Crystal Backend API   Let\u0026#39;s check Service Types   Ensure the ELB Service Role exists   Deploy Frontend Service   Find the Service Address   Scale the Backend Services   Scale the Frontend   Cleanup the applications   "
},
{
	"uri": "/beginner/050_deploy/scalebackend/",
	"title": "Scale the Backend Services",
	"tags": [],
	"description": "",
	"content": "When we launched our services, we only launched one container of each. We can confirm this by viewing the running pods:\nkubectl get deployments Now let\u0026rsquo;s scale up the backend services:\nkubectl scale deployment ecsdemo-nodejs --replicas=3 kubectl scale deployment ecsdemo-crystal --replicas=3 Confirm by looking at deployments again:\nkubectl get deployments Also, check the browser tab where we can see our application running. You should now see traffic flowing to multiple backend services.\n"
},
{
	"uri": "/intermediate/230_logging/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "cd ~/environment/ kubectl delete -f ~/environment/logging/fluentbit.yaml aws es delete-elasticsearch-domain \\  --domain-name ${ES_DOMAIN_NAME} eksctl delete iamserviceaccount \\  --name fluent-bit \\  --namespace logging \\  --cluster eksworkshop-eksctl \\  --wait aws iam delete-policy \\  --policy-arn \u0026#34;arn:aws:iam::${ACCOUNT_ID}:policy/fluent-bit-policy\u0026#34; kubectl delete namespace logging rm -rf ~/environment/logging unset ES_DOMAIN_NAME unset ES_VERSION unset ES_DOMAIN_USER unset ES_DOMAIN_PASSWORD unset FLUENTBIT_ROLE unset ES_ENDPOINT "
},
{
	"uri": "/beginner/080_scaling/cleanup/",
	"title": "Cleanup Scaling",
	"tags": [],
	"description": "",
	"content": "kubectl delete -f ~/environment/cluster-autoscaler/nginx.yaml kubectl delete -f https://www.eksworkshop.com/beginner/080_scaling/deploy_ca.files/cluster-autoscaler-autodiscover.yaml eksctl delete iamserviceaccount \\  --name cluster-autoscaler \\  --namespace kube-system \\  --cluster eksworkshop-eksctl \\  --wait aws iam delete-policy \\  --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/k8s-asg-policy export ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;eksworkshop-eksctl\u0026#39;]].AutoScalingGroupName\u0026#34; --output text) aws autoscaling \\  update-auto-scaling-group \\  --auto-scaling-group-name ${ASG_NAME} \\  --min-size 3 \\  --desired-capacity 3 \\  --max-size 3 kubectl delete hpa,svc php-apache kubectl delete deployment php-apache kubectl delete pod load-generator cd ~/environment rm -rf ~/environment/cluster-autoscaler kubectl delete -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.1/components.yaml kubectl delete ns metrics helm uninstall kube-ops-view unset ASG_NAME unset AUTOSCALER_VERSION unset K8S_VERSION "
},
{
	"uri": "/intermediate/330_app_mesh/port_to_app_mesh/create_meshed_app/",
	"title": "Create the Meshed Application",
	"tags": [],
	"description": "",
	"content": "Using the YAML we just reviewed, apply the meshed application resources with kubectl.\nkubectl apply -f 2_meshed_application/meshed_app.yaml namespace/prod configured mesh.appmesh.k8s.aws/dj-app created virtualnode.appmesh.k8s.aws/dj created virtualservice.appmesh.k8s.aws/jazz created virtualservice.appmesh.k8s.aws/metal created virtualrouter.appmesh.k8s.aws/jazz-router created virtualrouter.appmesh.k8s.aws/metal-router created virtualnode.appmesh.k8s.aws/jazz-v1 created virtualnode.appmesh.k8s.aws/metal-v1 created service/jazz created service/metal created namespace/prod configured  This creates the Kubernetes objects, and the App Mesh controller in turn creates resources within AWS App Mesh for you.\nYou can see that your mesh object was created using kubectl.\nkubectl get meshes NAME ARN AGE dj-app arn:aws:appmesh:us-west-2:1234567890:mesh/dj-app 119s  You can also see that the mesh was created in App Mesh using the aws CLI.\naws appmesh list-meshes { \u0026#34;meshes\u0026#34; : [ { \u0026#34;arn\u0026#34; : \u0026#34;arn:aws:appmesh:us-west-2:1234567890:mesh/dj-app\u0026#34;, \u0026#34;meshName\u0026#34; : \u0026#34;dj-app\u0026#34;, \u0026#34;version\u0026#34; : 1, \u0026#34;meshOwner\u0026#34; : \u0026#34;1234567890\u0026#34;, \u0026#34;createdAt\u0026#34; : \u0026#34;2020-06-18T10:01:21.411000-04:00\u0026#34;, \u0026#34;resourceOwner\u0026#34; : \u0026#34;1234567890\u0026#34;, \u0026#34;lastUpdatedAt\u0026#34; : \u0026#34;2020-06-18T10:01:21.411000-04:00\u0026#34; } ] }  Examine the objects within the prod namespace and you will see your App Mesh resources along with your native Kubernetes objects.\nkubectl get all -n prod NAME READY STATUS RESTARTS AGE pod/dj-6bf5fb7f45-qkhv7 1/1 Running 0 18m pod/jazz-v1-6f688dcbf9-djb9h 1/1 Running 0 18m pod/metal-v1-566756fbd6-8k2rs 1/1 Running 0 18m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dj ClusterIP 10.100.42.60 \u0026lt;none\u0026gt; 9080/TCP 18m service/jazz ClusterIP 10.100.134.233 \u0026lt;none\u0026gt; 9080/TCP 17s service/jazz-v1 ClusterIP 10.100.192.113 \u0026lt;none\u0026gt; 9080/TCP 18m service/metal ClusterIP 10.100.175.238 \u0026lt;none\u0026gt; 9080/TCP 17s service/metal-v1 ClusterIP 10.100.238.120 \u0026lt;none\u0026gt; 9080/TCP 18m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/dj 1/1 1 1 18m deployment.apps/jazz-v1 1/1 1 1 18m deployment.apps/metal-v1 1/1 1 1 18m NAME DESIRED CURRENT READY AGE replicaset.apps/dj-6bf5fb7f45 1 1 1 18m replicaset.apps/jazz-v1-6f688dcbf9 1 1 1 18m replicaset.apps/metal-v1-566756fbd6 1 1 1 18m NAME ARN AGE virtualnode.appmesh.k8s.aws/dj arn:aws:appmesh:us-west-2:1234567890:mesh/dj-app/virtualNode/dj_prod 19s virtualnode.appmesh.k8s.aws/jazz-v1 arn:aws:appmesh:us-west-2:1234567890:mesh/dj-app/virtualNode/jazz-v1_prod 17s virtualnode.appmesh.k8s.aws/metal-v1 arn:aws:appmesh:us-west-2:1234567890:mesh/dj-app/virtualNode/metal-v1_prod 17s NAME ARN AGE virtualservice.appmesh.k8s.aws/jazz arn:aws:appmesh:us-west-2:1234567890:mesh/dj-app/virtualService/jazz.prod.svc.cluster.local 19s virtualservice.appmesh.k8s.aws/metal arn:aws:appmesh:us-west-2:1234567890:mesh/dj-app/virtualService/metal.prod.svc.cluster.local 18s NAME ARN AGE virtualrouter.appmesh.k8s.aws/jazz-router arn:aws:appmesh:us-west-2:1234567890:mesh/dj-app/virtualRouter/jazz-router_prod 19s virtualrouter.appmesh.k8s.aws/metal-router arn:aws:appmesh:us-west-2:1234567890:mesh/dj-app/virtualRouter/metal-router_prod 19s  "
},
{
	"uri": "/beginner/060_helm/helm_micro/rolling_back/",
	"title": "Rolling Back",
	"tags": [],
	"description": "",
	"content": "Mistakes will happen during deployment, and when they do, Helm makes it easy to undo, or \u0026ldquo;roll back\u0026rdquo; to the previously deployed version.\nUpdate the demo application chart with a breaking change Open values.yaml and modify the image name under nodejs.image to brentley/ecsdemo-nodejs-non-existing. This image does not exist, so this will break our deployment.\nDeploy the updated demo application chart:\nhelm upgrade workshop ~/environment/eksdemo The rolling upgrade will begin by creating a new nodejs pod with the new image. The new ecsdemo-nodejs Pod should fail to pull non-existing image. Run kubectl get pods to see the ImagePullBackOff error.\nkubectl get pods NAME READY STATUS RESTARTS AGE ecsdemo-crystal-56976b4dfd-9f2rf 1/1 Running 0 2m10s ecsdemo-frontend-7f5ddc5485-8vqck 1/1 Running 0 2m10s ecsdemo-nodejs-56487f6c95-mv5xv 0/1 ImagePullBackOff 0 6s ecsdemo-nodejs-58977c4597-r6hvj 1/1 Running 0 2m10s  Run helm status workshop to verify the LAST DEPLOYED timestamp.\nhelm status workshop NAME: workshop LAST DEPLOYED: Fri Jul 16 13:53:22 2021 NAMESPACE: default STATUS: deployed REVISION: 2 TEST SUITE: None ...  This should correspond to the last entry on helm history workshop\nhelm history workshop Rollback the failed upgrade Now we are going to rollback the application to the previous working release revision.\nFirst, list Helm release revisions:\nhelm history workshop Then, rollback to the previous application revision (can rollback to any revision too):\n# rollback to the 1st revision helm rollback workshop 1 Validate workshop release status and you will see a new revision that is based on the rollback.\nhelm status workshop NAME: workshop LAST DEPLOYED: Fri Jul 16 13:55:27 2021 NAMESPACE: default STATUS: deployed REVISION: 3 TEST SUITE: None  Verify that the error is gone\nkubectl get pods NAME READY STATUS RESTARTS AGE ecsdemo-crystal-56976b4dfd-9f2rf 1/1 Running 0 6m ecsdemo-frontend-7f5ddc5485-8vqck 1/1 Running 0 6m ecsdemo-nodejs-58977c4597-r6hvj 1/1 Running 0 6m  "
},
{
	"uri": "/beginner/150_spotnodegroups/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Cleanup our Microservices deployment\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml To delete the label and the Node Group created by this module, run the following commands\nkubectl label nodes --all lifecycle- eksctl delete nodegroup --cluster=eksworkshop-eksctl --region=${AWS_REGION} --name=ng-spot "
},
{
	"uri": "/intermediate/290_argocd/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the Continuous Deployment with ArgoCD module.\nThis module is not used in subsequent steps, so you can remove the resources now, or at the end of the workshop:\nargocd app delete ecsdemo-nodejs -y watch argocd app get ecsdemo-nodejs Wait until all ressources are cleared with this message:\nFATA[0000] rpc error: code = NotFound desc = applications.argoproj.io \u0026quot;ecsdemo-nodejs\u0026quot; not found And then delete ArgoCD from your cluster:\nkubectl delete -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.0.4/manifests/install.yaml Delete namespaces created for this chapter:\nkubectl delete ns argocd kubectl delete ns ecsdemo-nodejs You may also delete the cloned repository ecsdemo-nodejs within your GitHub account.\n"
},
{
	"uri": "/920_cleanup/workspace/",
	"title": "Cleanup the Workspace",
	"tags": [],
	"description": "",
	"content": "Since we no longer need the Cloud9 instance to have Administrator access to our account, we can delete the workspace we created:\n Go to your Cloud9 Environment Select the environment named eksworkshop and pick delete  "
},
{
	"uri": "/intermediate/200_migrate_to_eks/deploy-counter-app-eks/",
	"title": "Deploy counter app to EKS",
	"tags": [],
	"description": "",
	"content": "Now it\u0026rsquo;s time to migrate our app to EKS. We\u0026rsquo;re going to do this in two stages.\nFirst we\u0026rsquo;ll move the frontend component but have it talk to the database in our old cluster. Then we\u0026rsquo;ll set up the database in EKS, migrate the data, and configure the frontend to use it instead.\nThe counter app deployment and service is the same as it was in kind except we added two environment varibles for the DB_HOST and DB_PORT and the service type is LoadBalancer instead of NodePort.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - --- apiVersion: apps/v1 kind: Deployment metadata: name: counter labels: app: counter spec: replicas: 2 selector: matchLabels: app: counter template: metadata: labels: app: counter spec: containers: - name: counter image: public.ecr.aws/aws-containers/stateful-counter:latest env: - name: DB_HOST value: $IP - name: DB_PORT value: \u0026#34;30001\u0026#34; ports: - containerPort: 8000 resources: requests: memory: \u0026#34;16Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; --- apiVersion: v1 kind: Service metadata: name: counter spec: ports: - port: 80 targetPort: 8000 type: LoadBalancer selector: app: counter EOF Now create a postgres-external service in kind that exposes postgres on a NodePort.\ncat \u0026lt;\u0026lt;EOF | kubectl --context kind-kind apply -f - --- apiVersion: v1 kind: Service metadata: name: postgres-external labels: app: postgres spec: type: NodePort ports: - port: 5432 nodePort: 30001 selector: app: postgres EOF Now you should be able to get the endpoint for your load balancer and when you load the counter app the same count will be shown in the app.\nkubectl get svc "
},
{
	"uri": "/intermediate/265_spinnaker_eks/install_spinnaker/",
	"title": "Install Spinnaker",
	"tags": [],
	"description": "",
	"content": "By now we have completed our configuration for Spinnaker and the SpinnakerService manifest located at deploy/spinnaker/basic/spinnakerservice.yml should look like below:\napiVersion: spinnaker.io/v1alpha2 kind: SpinnakerService metadata: name: spinnaker spec: spinnakerConfig: config: version: $SPINNAKER_VERSION # the version of Spinnaker to be deployed persistentStorage: persistentStoreType: s3 s3: bucket: $S3_BUCKET rootFolder: front50 region: $AWS_REGION accessKeyId: $AWS_ACCESS_KEY_ID secretAccessKey: $AWS_SECRET_ACCESS_KEY deploymentEnvironment: sidecars: spin-clouddriver: - name: token-refresh dockerImage: quay.io/skuid/ecr-token-refresh:latest mountPath: /etc/passwords configMapVolumeMounts: - configMapName: token-refresh-config mountPath: /opt/config/ecr-token-refresh features: artifacts: true artifacts: github: enabled: true accounts: - name: $GITHUB_USER token: $GITHUB_TOKEN # GitHub\u0026#39;s personal access token. This fields supports `encrypted` references to secrets. providers: dockerRegistry: enabled: true kubernetes: enabled: true accounts: - name: spinnaker-workshop requiredGroupMembership: [] providerVersion: V2 permissions: dockerRegistries: - accountName: my-ecr-registry configureImagePullSecrets: true cacheThreads: 1 namespaces: [spinnaker,detail] omitNamespaces: [] kinds: [] omitKinds: [] customResources: [] cachingPolicies: [] oAuthScopes: [] onlySpinnakerManaged: false kubeconfigFile: kubeconfig-sp # File name must match \u0026#34;files\u0026#34; key primaryAccount: spinnaker-workshop # Change to a desired account from the accounts array profiles: clouddriver: dockerRegistry: enabled: true primaryAccount: my-ecr-registry accounts: - name: my-ecr-registry address: https://$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com username: AWS passwordFile: /etc/passwords/my-ecr-registry.pass trackDigests: true repositories: - $ECR_REPOSITORY igor: docker-registry: enabled: true files: kubeconfig-sp: | \u0026lt;FILE CONTENTS HERE\u0026gt; # Content from kubeconfig created by Spinnaker Tool # spec.expose - This section defines how Spinnaker should be publicly exposed expose: type: service # Kubernetes LoadBalancer type (service/ingress), note: only \u0026#34;service\u0026#34; is supported for now service: type: LoadBalancer  Install Spinnaker Service Confirm if all the environment variables is set correctly\necho $ACCOUNT_ID echo $AWS_REGION echo $SPINNAKER_VERSION echo $GITHUB_USER echo $GITHUB_TOKEN echo $S3_BUCKET echo $AWS_ACCESS_KEY_ID echo $AWS_SECRET_ACCESS_KEY echo $ECR_REPOSITORY  If you do not see output from the above command for all the Environment Variables, do not proceed to next step\n cd ~/environment/spinnaker-operator/ envsubst \u0026lt; deploy/spinnaker/basic/spinnakerservice.yml | kubectl -n spinnaker apply -f - spinnakerservice.spinnaker.io/spinnaker created  It will take some time to bring up all the pods, so wait for few minutes..\n  # Get all the resources created kubectl get svc,pod -n spinnaker NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/spin-clouddriver ClusterIP 10.1x0.xx.71 \u0026lt;none\u0026gt; 7002/TCP 8d service/spin-deck LoadBalancer 10.1x0.yy.xx ae33c1a7185b14yyyy-1275091989.us-east-2.elb.amazonaws.com 80:32392/TCP 8d service/spin-echo ClusterIP 10.1x0.54.127 \u0026lt;none\u0026gt; 8089/TCP 8d service/spin-front50 ClusterIP 10.1x0.xx.241 \u0026lt;none\u0026gt; 8080/TCP 8d service/spin-gate LoadBalancer 10.1x0.75.xx ac3a38db81ebXXXX-1555475316.us-east-2.elb.amazonaws.com 80:32208/TCP 8d service/spin-igor ClusterIP 10.1x0.yy.xx \u0026lt;none\u0026gt; 8088/TCP 8d service/spin-orca ClusterIP 10.xx.64.yy \u0026lt;none\u0026gt; 8083/TCP 8d service/spin-redis ClusterIP 10.1x0.xx.242 \u0026lt;none\u0026gt; 6379/TCP 1x0 service/spin-rosco ClusterIP 10.1x0.yy.xx \u0026lt;none\u0026gt; 8087/TCP 8d NAME READY STATUS RESTARTS AGE pod/spin-clouddriver-7c5dbf658b-spl64 2/2 Running 0 8d pod/spin-deck-7f785d675f-2q4q8 1/1 Running 0 8d pod/spin-echo-d9b7799b4-4wjnn 1/1 Running 0 8d pod/spin-front50-76d9f8bd58-n96sl 1/1 Running 0 8d pod/spin-gate-7f48c76b55-bpc22 1/1 Running 0 8d pod/spin-igor-5c98f5b46f-mcmvs 1/1 Running 0 8d pod/spin-orca-6bd7c69f-mml4c 1/1 Running 0 8d pod/spin-redis-7f7d9659bf-whkf7 1/1 Running 0 8d pod/spin-rosco-7c6f77c64c-2qztw 1/1 Running 0 8d  # Watch the install progress. kubectl -n spinnaker get spinsvc spinnaker -w NAME VERSION LASTCONFIGURED STATUS SERVICES URL spinnaker 1.24.0 3h8m OK 9 http://ae33c1a7185b1402mmmmm-1275091989.us-east-2.elb.amazonaws.com  Test the setup on Spinnaker UI Access Spinakker UI Grab the load balancer url from the previous step, and load into the browser, you should see the below Spinnaker UI Create a test application Click on Create Application and enter details. Create a test pipeline Click on Pipelines under test-application and click on Configure a new pipeline and add the name. Click on Add Stage and select Deploy (Manifest) from the dropdown for Type, select spinnaker-workshop from the dropdown for Account and put the below yaml into the Manifest text area and click on \u0026ldquo;Save Changes\u0026rdquo;.\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 In the Spinnaker UI, Go to Pipelines and click on Start Manual Execution You will see the pipeline getting triggered and is in progress After few seconds, the pipleine is successful Clicking on execution details you can see the detail of deployment Go to Clusters and verify the deployment You can also go to Cloud9 terminal and verify the deployment\nkubectl get deployment nginx-deployment -n spinnaker kubectl get pods -l app=nginx -n spinnaker NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 1/1 1 1 108s NAME READY STATUS RESTARTS AGE nginx-deployment-6dc677db6-jchq8 1/1 Running 0 3m25s  Congratulations! You have successfully installed Spinnaker and created a test pipeline in Spinnaker and deployed the ngnix manifest to EKS cluster.\n"
},
{
	"uri": "/advanced/350_opentelemetry/traces/",
	"title": "Tracing",
	"tags": [],
	"description": "",
	"content": "The microservice demo we have installed is configured to generate traces and send them to Jaeger. The traces are generated using the OpenTelemetry specification and use X-Ray compatible IDs and propagators. This allows for using both Jaegor and X-Ray for storing and visualizing the trace across services.\nLet\u0026rsquo;s go setup our Open Telemetry collector to recieve these traces, batch the traces into bulk requests, and send the traces to AWS X-Ray.\n"
},
{
	"uri": "/advanced/430_emr_on_eks/spot_instances_2/",
	"title": "Using Spot Instances Part 2 - Run Sample Workload",
	"tags": [],
	"description": "",
	"content": "Spark Pod Template With Amazon EMR versions 5.33.0 and later, Amazon EMR on EKS supports pod template feature in Spark. Pod templates are specifications that determine how to run each pod. You can use pod template files to define the driver or executor pod‚Äôs configurations that Spark configurations do not support.\nFor more information about the pod templates support in EMR on EKS, see Pod Templates.\n To reduce costs, you can schedule Spark driver tasks to run on On-Demand instances while scheduling Spark executor tasks to run on Spot instances.\nWith pod templates you can define label eks.amazonaws.com/capacityType as a node selector, so that you can schedule Spark driver pods on On-demand Instances and Spark executor pods on the Spot Instances.\nNow, you will create a sample pod template for Spark Driver. Using nodeSelector eks.amazonaws.com/capacityType: ON_DEMAND this will run on On-demand Instances.\ncat \u0026gt; spark_driver_pod_template.yml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod spec: volumes: - name: source-data-volume emptyDir: {} - name: metrics-files-volume emptyDir: {} nodeSelector: eks.amazonaws.com/capacityType: ON_DEMAND containers: - name: spark-kubernetes-driver # This will be interpreted as Spark driver container EOF Next, you will create a sample pod template for Spark executors. Using nodeSelector eks.amazonaws.com/capacityType: SPOT this will run on Spot Instances.\ncat \u0026gt; spark_executor_pod_template.yml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod spec: volumes: - name: source-data-volume emptyDir: {} - name: metrics-files-volume emptyDir: {} nodeSelector: eks.amazonaws.com/capacityType: SPOT containers: - name: spark-kubernetes-executor # This will be interpreted as Spark executor container EOF Let\u0026rsquo;s upload sample pod templates and python script to s3 bucket.\naws s3 cp threadsleep.py ${s3DemoBucket} aws s3 cp spark_driver_pod_template.yml ${s3DemoBucket}/pod_templates/ aws s3 cp spark_executor_pod_template.yml ${s3DemoBucket}/pod_templates/ Next we submit the job.\n#Get required virtual cluster-id and role arn export VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query \u0026#34;virtualClusters[?state==\u0026#39;RUNNING\u0026#39;].id\u0026#34; --output text) export EMR_ROLE_ARN=$(aws iam get-role --role-name EMRContainers-JobExecutionRole --query Role.Arn --output text) #start spark job with start-job-run aws emr-containers start-job-run \\  --virtual-cluster-id $VIRTUAL_CLUSTER_ID \\  --name pi-spot \\  --execution-role-arn $EMR_ROLE_ARN \\  --release-label emr-5.33.0-latest \\  --job-driver \u0026#39;{ \u0026#34;sparkSubmitJobDriver\u0026#34;: { \u0026#34;entryPoint\u0026#34;: \u0026#34;\u0026#39;${s3DemoBucket}\u0026#39;/threadsleep.py\u0026#34;, \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.kubernetes.driver.podTemplateFile=\\\u0026#34;\u0026#39;${s3DemoBucket}\u0026#39;/pod_templates/spark_driver_pod_template.yml\\\u0026#34; --conf spark.kubernetes.executor.podTemplateFile=\\\u0026#34;\u0026#39;${s3DemoBucket}\u0026#39;/pod_templates/spark_executor_pod_template.yml\\\u0026#34; --conf spark.executor.instances=15 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\u0026#34;}}\u0026#39; \\  --configuration-overrides \u0026#39;{ \u0026#34;applicationConfiguration\u0026#34;: [ { \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;spark.dynamicAllocation.enabled\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;spark.kubernetes.executor.deleteOnTermination\u0026#34;: \u0026#34;true\u0026#34; } } ], \u0026#34;monitoringConfiguration\u0026#34;: { \u0026#34;cloudWatchMonitoringConfiguration\u0026#34;: { \u0026#34;logGroupName\u0026#34;: \u0026#34;/emr-on-eks/eksworkshop-eksctl\u0026#34;, \u0026#34;logStreamNamePrefix\u0026#34;: \u0026#34;pi\u0026#34; }, \u0026#34;s3MonitoringConfiguration\u0026#34;: { \u0026#34;logUri\u0026#34;: \u0026#34;\u0026#39;${s3DemoBucket}\u0026#39;/\u0026#34; } } }\u0026#39; You will be able to see the completed job in EMR console.\nLet\u0026rsquo;s check the pods deployed on On-Demand Instances and should now see Spark driver pods running on On-Demand instances.\nfor n in $(kubectl get nodes -l eks.amazonaws.com/capacityType=ON_DEMAND --no-headers | cut -d \u0026#34; \u0026#34; -f1); do echo \u0026#34;Pods on instance ${n}:\u0026#34;;kubectl get pods -n spark --no-headers --field-selector spec.nodeName=${n} ; echo ; done Let\u0026rsquo;s check the pods deployed on Spot Instances and should now see Spark executor pods running on Spot Instances.\nfor n in $(kubectl get nodes -l eks.amazonaws.com/capacityType=SPOT --no-headers | cut -d \u0026#34; \u0026#34; -f1); do echo \u0026#34;Pods on instance ${n}:\u0026#34;;kubectl get pods -n spark --no-headers --field-selector spec.nodeName=${n} ; echo ; done "
},
{
	"uri": "/advanced/350_opentelemetry/traces/adding_trace_configuration/",
	"title": "Adding trace Configuration",
	"tags": [],
	"description": "",
	"content": "Let‚Äôs update our Collector‚Äôs Manifest to collect traces and export them to x-ray. This involves un-commenting a few sections in kubernetes/adot/otel-container-insights-infra.yaml. The below sections will walk through the commented out sections and provide CLI commands to un-comment them out.\nThrift Service First let‚Äôs head to the end of the manifest and un-comment the Thrift service. This stands up an API endpoint to recieve thrift traffic over the Thrift Binary protocol on port 6832, HTTP-TCP on TCP port 14268, and gRPC on TCP 14250. Thrift supports all three of these protocols.\n#--- #apiVersion: v1 #kind: Service #metadata: # name: aws-otel-eks # namespace: aws-otel-eks # labels: # name: aws-otel-eks #spec: # ports: # - name: thrift-binary # port: 6832 # protocol: UDP # - name: thrift-http # port: 14268 # - name: grpc # port: 14250 # selector: # name: aws-otel-eks-ci  Uncomment these lines by running the following command:\nsed -i \u0026#39;275,296 s/#//g\u0026#39; kubernetes/adot/otel-container-insights-infra.yaml Jaeger receiver Now let\u0026rsquo;s head to lines 77 through 81 of the manifest. This defines how the Collector should receive Jaeger data. Notice how we configure it with both the binary, http, and gRPC protocols, which our Thrift Service was also configured to support.\n#jaeger: # protocols: # thrift_binary: # thrift_http: # grpc:  Uncomment these lines by running the following command:\nsed -i \u0026#39;77,81 s/#//g\u0026#39; kubernetes/adot/otel-container-insights-infra.yaml AWS X-Ray exporter Head to lines 159 \u0026amp; 160 in the manifest. This configures how to export data to AWS X-Ray, with us specifying the AWS Region to send data to.\n#awsxray: # region: ${AWS_REGION}  Uncomment these lines by running the following command:\nsed -i \u0026#39;159,160 s/#//g\u0026#39; kubernetes/adot/otel-container-insights-infra.yaml Trace Pipeline Now that we configured how to recieve traces, and how to export them to AWS X-Ray. Let\u0026rsquo;s tell the Collector to create a pipeline connecting these two components together.\nHead to lines 168 through 171. We defined a Pipeline named traces/applications that recives data from the Jaeger receiver we configured, and sends it to the AWS X-Ray exporter we configured.\n#traces/applications: # receivers: [jaeger] # processors: [] # exporters: [awsxray]  Uncomment these lines out by running the following command:\nsed -i \u0026#39;168,171 s/#//g\u0026#39; kubernetes/adot/otel-container-insights-infra.yaml Updating the Collector We defined a Jaeger receiver, X-Ray exporter, and a Pipeline to receive and export traces. Let\u0026rsquo;s update our Collector with this new configuration. Collectors will recurringly detect and update configuration, however to speed things up we will force Collector pod re-creation.\nLet\u0026rsquo;s run the following commands to update our Collector:\nkubectl apply -f kubernetes/adot/otel-container-insights-infra.yaml kubectl delete pods -n aws-otel-eks -l name=aws-otel-eks-ci "
},
{
	"uri": "/advanced/350_opentelemetry/traces/generating_traces/",
	"title": "Generating Traces",
	"tags": [],
	"description": "",
	"content": "By default microservice demo is not configured to send tracing data to the ADOT collector. Run the following commands to enable it:\nsed -i \u0026#39;s/#//\u0026#39; kubernetes/backend/*.yaml Then we apply these changes to our deployments\nkubectl apply -f kubernetes/backend/ Adding a new Employee Now that traces are enabled, let‚Äôs head back to our application and add a new employee. Let‚Äôs get our front-end URL:\necho http://${SERVICE_IP}/ Open that URL in your web browser, and create a new employee. We\u0026rsquo;ll name our employee \u0026ldquo;George Burdell\u0026rdquo; who\u0026rsquo;s occupation is \u0026ldquo;Software Engineer\u0026rdquo;. After clicking the \u0026lsquo;ADD EMPLOYEE\u0026rsquo; button, you should get a success model with the Employee ID. Let\u0026rsquo;s copy that employee Id. Then we\u0026rsquo;ll close the modal, and copy that Employee ID into the Get Employee field. After clicking \u0026lsquo;GET EMPLOYEE\u0026rsquo; we\u0026rsquo;ll get a modal with the Name \u0026amp; Occupation we provided earlier. Note: If you want to customize your employee. You\u0026rsquo;ll need to provide an occupation that is listed in the initdb.sql file.\n "
},
{
	"uri": "/advanced/350_opentelemetry/traces/viewing_traces_in_xray/",
	"title": "Viewing Traces in X-Ray &amp; CloudWatch",
	"tags": [],
	"description": "",
	"content": "X-Ray Now let‚Äôs head to the AWS X-Ray console and open the Service Map menu. We can get the URL for X-Ray Service Map by running the following command:\necho \u0026#34;https://${AWS_REGION}.console.aws.amazon.com/xray/home?region=${AWS_REGION}#/service-map\u0026#34; Selecting Trace on the left-hand menu in the AWS X-Ray console, we can view traces the Collector received and exported to X-Ray for our Create Employee API Call.\nNote, X-Ray by default is set to show traces from the Last 5 minutes, if the Employee was created more than 5 minutes ago, increase the window-size (top right).\nCloudWatch ServiceLens CloudWatch ServiceLens ties together CloudWatch metrics and logs, as well as traces from AWS X-Ray to give a complete view of your applications and their dependencies\nLet\u0026rsquo;s head to the CloudWatch ServiceLens console. The following command gives a direct link to the CloudWatch ServiceLens console:\necho \u0026#34;https://${AWS_REGION}.console.aws.amazon.com/cloudwatch/home?region=${AWS_REGION}#servicelens:service-map/map\u0026#34; The ServiceLens provides us a visualization of both the Container Insight metrics \u0026amp; X-Ray traces gathered by our OpenTelemetry Collector. "
},
{
	"uri": "/beginner/110_irsa/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To cleanup, follow these steps.\nkubectl delete -f ~/environment/irsa/job-s3.yaml kubectl delete -f ~/environment/irsa/job-ec2.yaml eksctl delete iamserviceaccount \\  --name iam-test \\  --namespace default \\  --cluster eksworkshop-eksctl \\  --wait rm -rf ~/environment/irsa/ aws s3 rb s3://eksworkshop-$ACCOUNT_ID-$AWS_REGION --region $AWS_REGION --force "
},
{
	"uri": "/advanced/310_servicemesh_with_istio/visualize/",
	"title": "Monitor &amp; Visualize",
	"tags": [],
	"description": "",
	"content": "Install Grafana and Prometheus Istio provides a basic sample installation to quickly get Prometheus and Grafana up and running, bundled with all of the Istio dashboards already installed:\nexport ISTIO_RELEASE=$(echo $ISTIO_VERSION |cut -d. -f1,2) # Install Prometheus kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_RELEASE}/samples/addons/prometheus.yaml # Install Grafana kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_RELEASE}/samples/addons/grafana.yaml We can now verify that they have been installed:\nkubectl -n istio-system get deploy grafana prometheus NAME READY UP-TO-DATE AVAILABLE AGE grafana 1/1 1 1 63s prometheus 1/1 1 1 64s  Install Jaeger and Kiali Jaeger is an open source end to end distributed tracing system, allowing users to monitor and troubleshoot complex distributed systems. Jaeger addresses issues with distributed transaction monitoring, performance and latency optimization, root cause and service dependency analysis etc.\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_RELEASE}/samples/addons/jaeger.yaml Kiali is a management console for an Istio-based service mesh. It provides dashboards, observability, and lets you operate your mesh with robust configuration and validation capabilities. It shows the structure of your service mesh by inferring traffic topology and displays the health of your mesh. Kiali provides detailed metrics, powerful validation, Grafana access, and strong integration for distributed tracing with Jaeger. You may visit official site to view features it offers.\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_RELEASE}/samples/addons/kiali.yaml We can now verify that they have been installed:\nkubectl -n istio-system get deploy jaeger kiali NAME READY UP-TO-DATE AVAILABLE AGE jaeger 1/1 1 1 63s kiali 1/1 1 1 64s  Generate traffic to collect telemetry data Open a new terminal tab and use these commands to send a traffic to the mesh\nexport GATEWAY_URL=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) watch --interval 1 curl -s -I -XGET \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; Next, we will launch Kiali to visualize application tracing and metrics.\nLaunch Kiali Open a new terminal tab and launch kiali dashboard by executing the following command\nkubectl -n istio-system port-forward \\ $(kubectl -n istio-system get pod -l app=kiali -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 8080:20001 Open Kiali dashboard. Click Preview / Preview Running Application in Cloud9 environment.\nClick the \u0026lsquo;Pop Out Into New Window\u0026rsquo; button\nNavigate to Graph from left panel to view graphical view of application\nNavigate Kiali interface to see powerful tracing and monitoring features\nLaunch Grafana Dashboard Currently Cloud9 IDE does not support previewing multiple running applications. Stop Kiali listener before launching Grafana.\n Next, we will visualize application metrics using Grafana. Open a new terminal tab and setup port-forwarding for Grafana by executing the following command\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 8080:3000 Open the Istio Dashboard via the Grafana UI\n In your Cloud9 environment, click Preview / Preview Running Application   Scroll to the end of the URL and append:  dashboard/db/istio-mesh-dashboard  Click the \u0026lsquo;Pop Out Into New Window\u0026rsquo; button  You will see that the traffic is evenly spread between reviews:v1and reviews:v3We encourage you to explore other Istio dashboards that are available by clicking the Istio Mesh Dashboard menu on top left of the page, and selecting a different dashboard.\n"
},
{
	"uri": "/advanced/430_emr_on_eks/fargate_1/",
	"title": "Serverless EMR job Part 1 - Setup",
	"tags": [],
	"description": "",
	"content": "Running applications on the serverless compute engine AWS Fargate, makes it easy for you to focus on deliverying business values, as it removes the need to provision, configure autoscaling, and manage the server.\nBefore we schedule a serverless EMR job on Amazon EKS, a Fargate profile is needed, that specifies which of your Spark pods should use Fargate when they are launched. For more information, see AWS Fargate profile and our previous lab Creating a Fargate Profile.\nCreate Fargate Profile Add your Fargate profile to EKS by the following command:\neksctl create fargateprofile --cluster eksworkshop-eksctl --name emr \\ --namespace spark --labels type=etl The labels setting provides your application a way to target a particular group of compute resources on EKS.\nTo ensure your job is picked up by Fargate not by the managed nodegroup on EC2, tag your Spark application by the same etl label.\nThe configuration looks like this:\n--conf spark.kubernetes.driver.label.type=etl  --conf spark.kubernetes.executor.label.type=etl Submit a job The sample job we will submit reads a public Amazon customer Reviews Dataset (~ 50GB), then counts the total number of words in reviews.\nFirstly, setup a permission for data source and target.\ncat \u0026lt;\u0026lt;EoF \u0026gt; review-data-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::amazon-reviews-pds/parquet/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${s3DemoBucket:5}/output/*\u0026#34; ] }] } EoF aws iam put-role-policy --role-name EMRContainers-JobExecutionRole --policy-name review-data-access --policy-document file://review-data-policy.json Secondly, upload the application code to S3.\n# create a pySpark job cat \u0026lt;\u0026lt; EOF \u0026gt;wordcount.py import sys from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026#39;Amazon reviews word count\u0026#39;).getOrCreate() df = spark.read.parquet(\u0026#34;s3://amazon-reviews-pds/parquet/\u0026#34;) df.selectExpr(\u0026#34;explode(split(lower(review_body), \u0026#39; \u0026#39;)) as words\u0026#34;).groupBy(\u0026#34;words\u0026#34;).count().write.mode(\u0026#34;overwrite\u0026#34;).parquet(sys.argv[1]) exit() EOF # upload the script aws s3 cp wordcount.py ${s3DemoBucket} Next, get existing EMR resources.\nexport VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query \u0026#34;virtualClusters[?state==\u0026#39;RUNNING\u0026#39;].id\u0026#34; --output text) export EMR_ROLE_ARN=$(aws iam get-role --role-name EMRContainers-JobExecutionRole --query Role.Arn --output text) Finally, start a serverless EMR job on EKS\naws emr-containers start-job-run \\  --virtual-cluster-id $VIRTUAL_CLUSTER_ID \\  --name word_count \\  --execution-role-arn $EMR_ROLE_ARN \\  --release-label emr-6.2.0-latest \\  --job-driver \u0026#39;{ \u0026#34;sparkSubmitJobDriver\u0026#34;: { \u0026#34;entryPoint\u0026#34;: \u0026#34;\u0026#39;$s3DemoBucket\u0026#39;/wordcount.py\u0026#34;, \u0026#34;entryPointArguments\u0026#34;:[\u0026#34;\u0026#39;$s3DemoBucket\u0026#39;/output/\u0026#34;], \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.kubernetes.driver.label.type=etl --conf spark.kubernetes.executor.label.type=etl --conf spark.executor.instances=8 --conf spark.executor.memory=2G --conf spark.driver.cores=1 --conf spark.executor.cores=3\u0026#34;}}\u0026#39; \\  --configuration-overrides \u0026#39;{ \u0026#34;applicationConfiguration\u0026#34;: [{ \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, \u0026#34;properties\u0026#34;: {\u0026#34;spark.kubernetes.allocation.batch.size\u0026#34;: \u0026#34;8\u0026#34;} }], \u0026#34;monitoringConfiguration\u0026#34;: { \u0026#34;s3MonitoringConfiguration\u0026#34;: { \u0026#34;logUri\u0026#34;: \u0026#34;\u0026#39;${s3DemoBucket}\u0026#39;/fargate-logs/\u0026#34;}} }\u0026#39; "
},
{
	"uri": "/advanced/430_emr_on_eks/fargate_2/",
	"title": "Serverless EMR job Part 2 - Monitor &amp; Troubleshoot",
	"tags": [],
	"description": "",
	"content": "Monitoring job status With zero manual effort, the number of Fargate instances change dynamically and distribute across mutliple availability zones. You can again open up couple of terminals and use watch command to see this change.\nWatch pod status:\nwatch kubectl get pod -n spark Watch node scaling activities:\nwatch kubectl get node \\ --label-columns=eks.amazonaws.com/capacityType,topology.kubernetes.io/zone Navigate to SparkUI to view job status:\necho -e \u0026#34;\\nNavigate to EMR virtual cluster console:\\n\\nhttps://console.aws.amazon.com/elasticmapreduce/home?\u0026#34;region=${AWS_REGION}\u0026#34;#virtual-cluster-jobs:\u0026#34;${VIRTUAL_CLUSTER_ID}\u0026#34;\\n\u0026#34; View output in S3, once it\u0026rsquo;s done (~ 10min):\naws s3 ls ${s3DemoBucket}/output/ --summarize --human-readable --recursive Troubleshooting Fargate has flexible configuration options to run your workloads. However, it supports up to 4 vCPU and 30 GB Memory per compute instance. It applies to each of your Spark executors or the driver. Check out the current supported configurations and limits here.\nLet\u0026rsquo;s submit the same job with 5 vCPUs that is over the limit to force the failure.\naws emr-containers start-job-run \\  --virtual-cluster-id $VIRTUAL_CLUSTER_ID \\  --name word_count \\  --execution-role-arn $EMR_ROLE_ARN \\  --release-label emr-6.2.0-latest \\  --job-driver \u0026#39;{ \u0026#34;sparkSubmitJobDriver\u0026#34;: { \u0026#34;entryPoint\u0026#34;: \u0026#34;\u0026#39;$s3DemoBucket\u0026#39;/wordcount.py\u0026#34;, \u0026#34;entryPointArguments\u0026#34;:[\u0026#34;\u0026#39;$s3DemoBucket\u0026#39;/output/\u0026#34;], \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.kubernetes.driver.label.type=etl --conf spark.kubernetes.executor.label.type=etl --conf spark.executor.instances=8 --conf spark.executor.memory=2G --conf spark.driver.cores=1 --conf spark.executor.cores=5\u0026#34; } }\u0026#39; Problem - the job is stuck with pending status.\nwatch kubectl get pod -n spark Wait for about 2 minutes. Press ctl+c to exit, as soon as the Spark driver is running. Check the driver‚Äôs log:\ndriver_name=$(kubectl get pod -n spark | grep \u0026#34;driver\u0026#34; | awk \u0026#39;{print $1}\u0026#39;) kubectl logs ${driver_name} -n spark -c spark-kubernetes-driver The job is not accepted by any resources: Investigate a hanging executor pod:\nexec_name=$(kubectl get pod -n spark | grep \u0026#34;exec-1\u0026#34; | awk \u0026#39;{print $1}\u0026#39;) kubectl describe pod ${exec_name} -n spark Congratulations! You found the root cause: Delete the driver pod to stop the hanging job:\nkubectl delete pod ${driver_name} -n spark # check job status kubectl get pod -n spark "
},
{
	"uri": "/advanced/340_appmesh_flagger/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Congratulations on using Flagger in AWS App Mesh to automate the deployment of a new version of the detail service!\nIn this workshop, we have gone through:\n Installing AppMesh in EKS cluster Setting up Flagger for AppMesh in EKS cluster Flagger Canary setup for backend service detail We exposed frontend service via AppMesh VirtualGateway, frontend service calls the backend service detail We did automated canary promotion from version 1 to version 2 We also created a scenario by injecting error for automated canary rollback while deploying version 3 And lastly we redeployed the version 3 using automated canary promotion  "
},
{
	"uri": "/advanced/340_appmesh_flagger/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Delete Flagger Resources kubectl delete canary detail -n flagger helm uninstall flagger-loadtester -n flagger kubectl delete HorizontalPodAutoscaler detail -n flagger kubectl delete deployment detail -n flagger Delete Flagger Namespace Namespace deletion may take few minutes, please wait till the process completes.\n kubectl delete namespace flagger Delete the Mesh kubectl delete meshes flagger Delete Policies and Service Accounts for flagger namespace aws iam delete-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/FlaggerEnvoyNamespaceIAMPolicy eksctl delete iamserviceaccount --cluster eksworkshop-eksctl --namespace flagger --name flagger-envoy-proxies Uninstall the Flagger Helm Charts helm -n appmesh-system delete flagger Delete Flagger CRDs for i in $(kubectl get crd | grep flagger | cut -d\u0026#34; \u0026#34; -f1) ; do kubectl delete crd $i done Uninstall Prometheus Helm Charts helm -n appmesh-system delete appmesh-prometheus Uninstall Metric Server kubectl delete -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.1/components.yaml Uninstall AppMesh helm -n appmesh-system delete appmesh-controller for i in $(kubectl get crd | grep appmesh | cut -d\u0026#34; \u0026#34; -f1) ; do kubectl delete crd $i done eksctl delete iamserviceaccount --cluster eksworkshop-eksctl --namespace appmesh-system --name appmesh-controller kubectl delete namespace appmesh-system "
},
{
	"uri": "/intermediate/",
	"title": "Intermediate",
	"tags": ["intermediate"],
	"description": "",
	"content": "Intermediate "
},
{
	"uri": "/intermediate/246_monitoring_amp_amg/amg_login/",
	"title": "Login to AMG workspace",
	"tags": [],
	"description": "",
	"content": "Login to AMG workspace Click on the Grafana workspace URL in the Summary section\nThis will take you to the AWS SSO login screen, where you can provide the UserId and Password that you created as part of prerequisites.\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/canary_deployment/deploy_canary/",
	"title": "Canary Deployment",
	"tags": [],
	"description": "",
	"content": "Now lets deploy a new version (version 2) of Catalog Product Detail backend service. And change the proddetail VirtualRouter to route traffic 90% to proddetail-v1 version 1 and 10% to proddetail-v2 version 2. And as we gain confidence in the proddetail-v2, we can increase the % in a linear fashion. Build Catalog Detail Version 2 service aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com PROJECT_NAME=eks-app-mesh-demo export APP_VERSION_2=2.0 for app in catalog_detail; do aws ecr describe-repositories --repository-name $PROJECT_NAME/$app \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || \\  aws ecr create-repository --repository-name $PROJECT_NAME/$app \u0026gt;/dev/null TARGET=$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$PROJECT_NAME/$app:$APP_VERSION_2 cd apps/$app docker build -t $TARGET -f version2/Dockerfile . docker push $TARGET done cd ../../. Deploy Catalog Detail Version 2 service resources Looking at the section of canary.yaml shown below, you can see we\u0026rsquo;ve added the route as 10% to new service proddetail-v2 and 90% to existing service proddetail-v1.\n--- apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualRouter metadata: name: proddetail-router namespace: prodcatalog-ns spec: listeners: - portMapping: port: 3000 protocol: http routes: - name: proddetail-route httpRoute: match: prefix: / action: weightedTargets: - virtualNodeRef: name: proddetail-v1 weight: 90 - virtualNodeRef: name: proddetail-v2 weight: 10 ---  Lets deploy the resources for proddetail service version v2\nenvsubst \u0026lt; ./deployment/canary.yaml | kubectl apply -f - virtualnode.appmesh.k8s.aws/proddetail-v2 created virtualrouter.appmesh.k8s.aws/proddetail-router configured deployment.apps/proddetail2 created service/proddetail2 created  Check the resources for proddetail service version 2\nkubectl get all -n prodcatalog-ns | grep \u0026#39;proddetail2\\|proddetail-v2\u0026#39; pod/proddetail2-9687989db-xxxx 3/3 Running 0 5m52s service/proddetail2 ClusterIP 10.100.XX.YYY \u0026lt;none\u0026gt; 3000/TCP 18m deployment.apps/proddetail2 1/1 1 1 5m52s replicaset.apps/proddetail2-96879yyyxx 1 1 1 5m52s virtualnode.appmesh.k8s.aws/proddetail-v2 arn:aws:appmesh:us-west-2:405710966773:mesh/prodcatalog-mesh/virtualNode/proddetail-v2_prodcatalog-ns 18m  "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/deploy_applications/create_the_app/",
	"title": "Create Product Catalog Application",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s create the Product Catalog Application!\nBuild Application Services Build and Push the Container images to ECR for all the three services\ncd eks-app-mesh-polyglot-demo aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com PROJECT_NAME=eks-app-mesh-demo export APP_VERSION=1.0 for app in catalog_detail product_catalog frontend_node; do aws ecr describe-repositories --repository-name $PROJECT_NAME/$app \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || \\  aws ecr create-repository --repository-name $PROJECT_NAME/$app \u0026gt;/dev/null TARGET=$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$PROJECT_NAME/$app:$APP_VERSION docker build -t $TARGET apps/$app docker push $TARGET done  Building/Pushing Container images first time to ECR may take around 3-5 minutes\n Once completed, you can confirm the images are in ECR by logging into the console Deploy the Application Services to EKS envsubst \u0026lt; ./deployment/base_app.yaml | kubectl apply -f - deployment.apps/prodcatalog created service/prodcatalog created deployment.apps/proddetail created service/proddetail created deployment.apps/frontend-node created service/frontend-node created  Fargate pod creation for prodcatalog service may take 3 to 4 minutes\n Get the deployment details kubectl get deployment,pods,svc -n prodcatalog-ns -o wide You can see that:\n  Product Catalog service was deployed to Fargate pod as it matched the configuration (namespace prodcatalog-ns and pod spec label as app= prodcatalog) that we had specified when creating fargate profile\n  And other services Frontend and Catalog Product Detail were deployed into Managed Nodegroup\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/frontend-node 1/1 1 1 44h frontend-node $ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/frontend-node:4.6 app=frontend-node deployment.apps/prodcatalog 1/1 1 1 22h prodcatalog $ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/product-catalog:1.2 app=prodcatalog deployment.apps/proddetail 1/1 1 1 44h proddetail $ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/product-detail:1.1 app=proddetail NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/frontend-node-77d64585d4-xxxx 1/1 Running 0 13h 192.168.X.6 ip-192-168-X-X.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prodcatalog-98f7c5f87-xxxxx 1/1 Running 0 13h 192.168.X.17 fargate-ip-192-168-X-X.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/proddetail-5b558df99d-xxxxx 1/1 Running 0 18h 192.168.24.X ip-192-168-X-X.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/frontend-node ClusterIP 10.100.X.X \u0026lt;none\u0026gt; 9000/TCP 44h app=frontend-node service/prodcatalog ClusterIP 10.100.X.X \u0026lt;none\u0026gt; 5000/TCP 41h app=prodcatalog service/proddetail ClusterIP 10.100.X.X \u0026lt;none\u0026gt; 3000/TCP 44h app=proddetail 3000/TCP 103m    Confirm that the fargate pod is using the Service Account role export BE_POD_NAME=$(kubectl get pods -n prodcatalog-ns -l app=prodcatalog -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl describe pod ${BE_POD_NAME} -n prodcatalog-ns | grep \u0026#39;AWS_ROLE_ARN\\|AWS_WEB_IDENTITY_TOKEN_FILE\\|serviceaccount\u0026#39; You should see the below output which has the same role that we had associated with the Service Account as part of Fargate setup. AWS_ROLE_ARN: arn:aws:iam::$ACCOUNT_ID:role/eksctl-eksworkshop-eksctl-addon-iamserviceac-Role1-1PWNQ4AJFMVBF AWS_WEB_IDENTITY_TOKEN_FILE: /var/run/secrets/eks.amazonaws.com/serviceaccount/token /var/run/secrets/eks.amazonaws.com/serviceaccount from aws-iam-token (ro) /var/run/secrets/kubernetes.io/serviceaccount from prodcatalog-envoy-proxies-token-69pql (ro)  Confirm that the fargate pod logging is enabled kubectl describe pod ${BE_POD_NAME} -n prodcatalog-ns | grep LoggingEnabled We can see the confirmation in the events that says Successfully enabled logging for pod. Normal LoggingEnabled 2m7s fargate-scheduler Successfully enabled logging for pod  "
},
{
	"uri": "/advanced/420_kubeflow/fairing/",
	"title": "Fairing",
	"tags": [],
	"description": "",
	"content": "Kubeflow Fairing   Jupyter notebooks are a great way to author your model creation. You can write the algorithms, train the model and if you need a way to publish the inference endpoint directly from this interface, you can use Kubeflow fairing to do so\nAssign S3 and ECR permissions For this chapter, we will make use of both S3 and ECR services. We\u0026rsquo;ll use S3 to store and access pipeline data. We\u0026rsquo;ll use ECR as our container registry for the training image. We need to add IAM policies to Worker nodes so that we can access both S3 and ECR. Run below commands in Cloud9 and assign desired permission\naws iam attach-role-policy --role-name $ROLE_NAME --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess aws iam attach-role-policy --role-name $ROLE_NAME --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess Create Jupyter notebook server Create new notebook server by following [Jupyter notebook chapter] (/advanced/420_kubeflow/jupyter). Before you jump to the link, take a note of custom image (527798164940.dkr.ecr.us-west-2.amazonaws.com/tensorflow-1.15.2-notebook-cpu:1.0.0) that you will use for eks-kubeflow-workshop notebook server. Below screenshot depicts how to use custom image\nClone the repo We will clone Github repo from a Jupyter notebook so that we can readily use authored notebooks in this chapter.\nCreate new Python 3 Notebook if one doesn\u0026rsquo;t exist. Run the command below to clone the repo\n!git clone https://github.com/aws-samples/eks-kubeflow-workshop.git Click Run. This will clone the repo into your notebook server\nClose the notebook tab, go back to the notebook server, select the notebook that we just used and click Shutdown.\nRun fairing introduction notebook Browse the \u0026ldquo;eks-kubeflow-workshop\u0026rdquo; repository and go to fairing introduction notebook (eks-kubeflow-workshop/notebooks/02_Fairing/02_01_fairing_introduction.ipynb). You can either click on the notebook to open or select and click View\nStarting from here, its important to read notebook instructions carefully. The info provided in the workshop is lightweight and you can use it to ensure desired result. You can complete the exercise by staying in the notebook\n Review the content and click first cell and click Run. This will let you install Fairing from Github repository\nWait till it finishes, go to next cell and click Run. Here is expected result Now that we have fairing installed, we will train a model authored in Python. The model will create a linear regression model that allows us to learn a function or relationship from a given set of continuous data. For example, we are given some data points of x and corresponding y and we need to learn the relationship between them that is called a hypothesis.\nIn case of linear regression, the hypothesis is a straight line i.e, h(x) = x * weight + b.\nRun cell 3. Once it completes, run cell 4. This will create a model locally on our notebook Now, lets use fairing and push the image into ECR which can be used for remote training\nBefore you run authenticate with ECR, change the region if needed. Run this cell and login so that you can perform ECR operations\nRun the next cell and create an ECR repository (fairing-job) in the same region. You should see similar output Let\u0026rsquo;s run next cell. Fairing pushes the image to ECR and then deploys the model remotely\nNow that we have demonstrated how to use Fairing to train locally and remotely, let\u0026rsquo;s train and deploy XGBoost model and review an end to end implementation\nRun fairing end to end deployment notebook For this exercise, we will use another notebook called 02_06_fairing_e2e.ipynb\nGo back to your notebook server and shutdown 02_01_fairing_introduction.ipynb notebook. Open the 02_06_fairing_e2e.ipynb\nLet\u0026rsquo;s install python dependencies by running first 2 cells, and then run next 2 cells under \u0026ldquo;Develop your model\u0026rdquo;\nRun the next cell to train the XGBoost model locally. Here is the expected result Now lets create an S3 bucket to store pipeline data. Remember to change HASH to a random value before running next cell\nRunning next two steps will setup Fairing backend. Remember to change S3 bucket name before running next cell\nNow lets submit the Trainjob. Here is the expected result Running next step will deploy prediction endpoint using Fairing. You will endpoint details at the bottom. Let\u0026rsquo;s call this prediction endpoint. Remember to replace with your endpoint before running next two cells This demonstrates how to build XGBoost model using Fairing and deploy it locally and to remote endpoint.\nRun the next steps to cleanup resources from this exercise\n"
},
{
	"uri": "/beginner/160_advanced-networking/secondary_cidr/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s cleanup this tutorial\nkubectl delete deployments --all kubectl delete service nginx Edit aws-node configmap and comment AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG and its value\nkubectl edit daemonset -n kube-system aws-node ... spec: containers: - env: #- name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG # value: \u0026quot;true\u0026quot; - name: AWS_VPC_K8S_CNI_LOGLEVEL value: DEBUG - name: MY_NODE_NAME ... Delete custom resource objects from ENIConfig CRD\nkubectl delete eniconfig/group1-pod-netconfig kubectl delete eniconfig/group2-pod-netconfig kubectl delete eniconfig/group3-pod-netconfig Terminate EC2 instances so that fresh instances are launched with default CNI configuration\nUse caution before you run the next command because it terminates all worker nodes including running pods in your workshop\n INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag-key,Values=eks:cluster-name\u0026quot; \u0026quot;Name=tag-value,Values=eksworkshop*\u0026quot; --output text` ) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;Terminating EC2 instance $i ...\u0026quot; aws ec2 terminate-instances --instance-ids $i done Delete secondary CIDR from your VPC\nVPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=eksctl-eksworkshop* | jq -r '.Vpcs[].VpcId') ASSOCIATION_ID=$(aws ec2 describe-vpcs --vpc-id $VPC_ID | jq -r '.Vpcs[].CidrBlockAssociationSet[] | select(.CidrBlock == \u0026quot;100.64.0.0/16\u0026quot;) | .AssociationId') aws ec2 delete-subnet --subnet-id $CGNAT_SNET1 aws ec2 delete-subnet --subnet-id $CGNAT_SNET2 aws ec2 delete-subnet --subnet-id $CGNAT_SNET3 aws ec2 disassociate-vpc-cidr-block --association-id $ASSOCIATION_ID "
},
{
	"uri": "/advanced/410_batch/workflow-simple/",
	"title": "Simple Batch Workflow",
	"tags": [],
	"description": "",
	"content": "Simple Batch Workflow Create the manifest workflow-whalesay.yaml and let\u0026rsquo;s deploy the whalesay example from before using Argo.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/batch_policy/workflow-whalesay.yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: whalesay- spec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay command: [cowsay] args: [\u0026#34;This is an Argo Workflow!\u0026#34;] EoF Now deploy the workflow using the argo CLI.\nYou can also run workflow specs directly using kubectl but the argo CLI provides syntax checking, nicer output, and requires less typing. For the equivalent kubectl commands, see Argo CLI.\n argo -n argo submit --watch ~/environment/batch_policy/workflow-whalesay.yaml Name: whalesay-rlssg Namespace: argo ServiceAccount: default Status: Succeeded Conditions: Completed True Created: Wed Jul 08 00:13:51 \u0026#43;0000 (3 seconds ago) Started: Wed Jul 08 00:13:51 \u0026#43;0000 (3 seconds ago) Finished: Wed Jul 08 00:13:54 \u0026#43;0000 (now) Duration: 3 seconds ResourcesDuration: 1s*(1 cpu),1s*(100Mi memory) STEP TEMPLATE PODNAME DURATION MESSAGE ‚úî whalesay-rlssg whalesay whalesay-rlssg 2s  Confirm the output by running the following command:\nargo -n argo logs $(argo -n argo list -o name) whalesay-rlssg: ___________________________ whalesay-rlssg: \u0026lt; This is an Argo Workflow! \u0026gt; whalesay-rlssg: --------------------------- whalesay-rlssg: \\ whalesay-rlssg: \\ whalesay-rlssg: \\ whalesay-rlssg: ## . whalesay-rlssg: ## ## ## == whalesay-rlssg: ## ## ## ## === whalesay-rlssg: /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;___/ === whalesay-rlssg: ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ whalesay-rlssg: \\______ o __/ whalesay-rlssg: \\ \\ __/ whalesay-rlssg: \\____\\______/  "
},
{
	"uri": "/010_introduction/basics/concepts_objects_details_1/",
	"title": "K8s Objects Detail (1/2)",
	"tags": [],
	"description": "",
	"content": "Pod  A thin wrapper around one or more containers  DaemonSet  Implements a single instance of a pod on a worker node  Deployment  Details how to roll out (or roll back) across versions of your application  "
},
{
	"uri": "/beginner/090_rbac/verify_user_role_binding/",
	"title": "Verify the Role and Binding",
	"tags": [],
	"description": "",
	"content": "Now that the user, Role, and RoleBinding are defined, lets switch back to rbac-user, and test.\nTo switch back to rbac-user, issue the following command that sources the rbac-user env vars, and verifies they\u0026rsquo;ve taken:\n. rbacuser_creds.sh; aws sts get-caller-identity You should see output reflecting that you are logged in as rbac-user.\nAs rbac-user, issue the following to get pods in the rbac namespace:\nkubectl get pods -n rbac-test The output should be similar to:\nNAME READY STATUS RESTARTS AGE nginx-55bd7c9fd-kmbkf 1/1 Running 0 23h  Try running the same command again, but outside of the rbac-test namespace:\nkubectl get pods -n kube-system You should get an error similar to: No resources found. Error from server (Forbidden): pods is forbidden: User \u0026#34;rbac-user\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;kube-system\u0026#34;  Because the role you are bound to does not give you access to any namespace other than rbac-test.\n"
},
{
	"uri": "/beginner/050_deploy/scalefrontend/",
	"title": "Scale the Frontend",
	"tags": [],
	"description": "",
	"content": "Challenge: Let\u0026rsquo;s also scale our frontend service!\n  Expand here to see the solution   kubectl get deployments kubectl scale deployment ecsdemo-frontend --replicas=3 kubectl get deployments    Check the browser tab where we can see our application running. You should now see traffic flowing to multiple frontend services.\n"
},
{
	"uri": "/beginner/060_helm/helm_micro/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To delete the workshop release, run:\nhelm uninstall workshop "
},
{
	"uri": "/intermediate/210_jenkins/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To uninstall Jenkins and cleanup the service account and CodeCommit repository run:\nhelm uninstall cicd aws codecommit delete-repository \\  --repository-name eksworkshop-app aws iam detach-user-policy \\  --user-name git-user \\  --policy-arn arn:aws:iam::aws:policy/AWSCodeCommitPowerUser aws iam delete-service-specific-credential \\  --user-name git-user \\  --service-specific-credential-id $CREDENTIAL_ID aws iam delete-user \\  --user-name git-user eksctl delete iamserviceaccount \\  --name jenkins \\  --namespace default \\  --cluster eksworkshop-eksctl rm -rf ~/environment/eksworkshop-app rm ~/environment/values.yaml sudo pip uninstall -y git-remote-codecommit "
},
{
	"uri": "/intermediate/330_app_mesh/deploy_dj_app/create_the_app/",
	"title": "Create DJ App",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s create the DJ App!\nThe application repo has all of the configuration YAML required to deploy the DJ App into its own prod namespace on your Kubernetes cluster.\nkubectl apply -f 1_base_application/base_app.yaml This will create the prod namespace as well as the Deployments and Services for the application. The output should be similar to:\nnamespace/prod created deployment.apps/dj created deployment.apps/metal-v1 created deployment.apps/jazz-v1 created service/dj created service/metal-v1 created service/jazz-v1 created  You can now verify that the objects were all created successfully and the application is up and running.\nkubectl -n prod get all NAME READY STATUS RESTARTS AGE pod/dj-6bf5fb7f45-qkhv7 1/1 Running 0 42s pod/jazz-v1-6f688dcbf9-djb9h 1/1 Running 0 41s pod/metal-v1-566756fbd6-8k2rs 1/1 Running 0 41s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dj ClusterIP 10.100.42.60 \u0026lt;none\u0026gt; 9080/TCP 41s service/jazz-v1 ClusterIP 10.100.192.113 \u0026lt;none\u0026gt; 9080/TCP 40s service/metal-v1 ClusterIP 10.100.238.120 \u0026lt;none\u0026gt; 9080/TCP 40s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/dj 1/1 1 1 42s deployment.apps/jazz-v1 1/1 1 1 41s deployment.apps/metal-v1 1/1 1 1 41s NAME DESIRED CURRENT READY AGE replicaset.apps/dj-6bf5fb7f45 1 1 1 43s replicaset.apps/jazz-v1-6f688dcbf9 1 1 1 42s replicaset.apps/metal-v1-566756fbd6 1 1 1 42s  Once you\u0026rsquo;ve verified everything is looking good in the prod namespace, you\u0026rsquo;re ready to test out this initial version of the DJ App.\n"
},
{
	"uri": "/beginner/060_helm/",
	"title": "Helm",
	"tags": ["beginner", "CON203", "CON205", "CON206"],
	"description": "",
	"content": "Helm   This tutorial has been updated for Helm v3. In version 3, the Tiller component was removed, which simplified operations and improved security.\n If you need to migrate from Helm v2 to v3 click here for the official documentation.\n Helm is a package manager for Kubernetes that packages multiple Kubernetes resources into a single logical deployment unit called a Chart. Charts are easy to create, version, share, and publish.\nIn this chapter, we\u0026rsquo;ll cover installing Helm. Once installed, we\u0026rsquo;ll demonstrate how Helm can be used to deploy a simple nginx webserver, and a more sophisticated microservice.\n"
},
{
	"uri": "/beginner/060_helm/helm_intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction Helm is a package manager and application management tool for Kubernetes that packages multiple Kubernetes resources into a single logical deployment unit called a Chart.\nHelm helps you to:\n Achieve a simple (one command) and repeatable deployment Manage application dependency, using specific versions of other application and services Manage multiple deployment configurations: test, staging, production and others Execute post/pre deployment jobs during application deployment Update/rollback and test application deployments  "
},
{
	"uri": "/intermediate/200_migrate_to_eks/deploy-counter-db-in-eks/",
	"title": "Deploy database to EKS",
	"tags": [],
	"description": "",
	"content": "The final step is to move the database from our kind cluster into EKS. There are lots of different options for how you might want to migrate application state. In many cases using an external database such as Amazon Relational Database Service (RDS) is a great fit.\nFor production data you\u0026rsquo;ll want to set up a way where you can verify correctness of your state or automatic syncing between environments. For this workshop we\u0026rsquo;re going to manually move our database state.\nThe first thing we need to do is create a Postgres database with hostPath persistent storage in Kubernetes. We\u0026rsquo;ll use the exact same ConfigMap from kind to generate an empty database first.\nAll of the config for Postgres is the same as it was for kind\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - --- apiVersion: v1 kind: ConfigMap metadata: name: postgres-config labels: app: postgres data: POSTGRES_PASSWORD: supersecret init: | CREATE TABLE importantdata ( id int4 PRIMARY KEY, count int4 NOT NULL ); INSERT INTO importantdata (id , count) VALUES (1, 0); --- kind: PersistentVolume apiVersion: v1 metadata: name: postgres-pv-volume labels: type: local app: postgres spec: storageClassName: manual capacity: storage: 5Gi accessModes: - ReadWriteMany hostPath: path: \u0026#34;/mnt/data\u0026#34; --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: postgres-pv-claim labels: app: postgres spec: storageClassName: manual accessModes: - ReadWriteMany resources: requests: storage: 5Gi EOF Deploy the postgres StatefulSet\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: StatefulSet metadata: name: postgres spec: replicas: 1 serviceName: postgres selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: terminationGracePeriodSeconds: 5 containers: - name: postgres image: postgres:13 imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; ports: - containerPort: 5432 envFrom: - configMapRef: name: postgres-config volumeMounts: - mountPath: /var/lib/postgresql/data name: postgredb - mountPath: /docker-entrypoint-initdb.d name: init resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; volumes: - name: postgredb persistentVolumeClaim: claimName: postgres-pv-claim - name: init configMap: name: postgres-config items: - key: init path: init.sql EOF Backup the data from our kind Postgres database and restore it to EKS using standard postgres tools.\nkubectl --context kind-kind exec -t postgres-0 -- pg_dumpall -c -U postgres \u0026gt; postgres_dump.sql Restore database\ncat postgres_dump.sql | kubectl exec -i postgres-0 -- psql -U postgres Now we can deploy a postgres service inside EKS to point to the new database endpoint. This is the exact same postgres service we deployed to kind.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - --- apiVersion: v1 kind: Service metadata: name: postgres labels: app: postgres spec: type: ClusterIP ports: - port: 5432 selector: app: postgres EOF Finally we need to update the counter application to remove the two environment variables we added for the external database.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - --- apiVersion: apps/v1 kind: Deployment metadata: name: counter labels: app: counter spec: replicas: 1 selector: matchLabels: app: counter template: metadata: labels: app: counter spec: containers: - name: counter image: public.ecr.aws/aws-containers/stateful-counter:latest ports: - containerPort: 8000 resources: requests: memory: \u0026#34;16Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; EOF Open your browser to this link\necho \u0026quot;http://\u0026quot;$(kubectl get svc counter --output jsonpath='{.status.loadBalancer.ingress[0].hostname}') "
},
{
	"uri": "/advanced/350_opentelemetry/kubernetes_metrics/",
	"title": "Kubernetes Metrics",
	"tags": [],
	"description": "",
	"content": "In this section of the workshop, we\u0026rsquo;ll configure the AWS Distro for OpenTelemetry Collector to scrape metrics from the Kubernetes Control Plane /metric endpoint.\nThis entails configuring a Collector to receive the EKS Control Plane metrics, and an exporter to send the metrics to AWS Managed Service for Prometheus.\nDue to a limitation with the current version of the Prometheus receiver, we will need to deploy an additional OpenTelemetry Collector as a single replica deployment.\n \u0026ldquo;When running multiple replicas of the collector with the same config, it will scrape the targets multiple times.\u0026rdquo; src\n "
},
{
	"uri": "/910_conclusion/survey/",
	"title": "Let us know what you think!",
	"tags": [],
	"description": "",
	"content": " Please take our survey!   "
},
{
	"uri": "/intermediate/265_spinnaker_eks/testing-helm/",
	"title": "Testing Helm-Based Pipeline",
	"tags": [],
	"description": "",
	"content": "Now lets deploy Helm Based Application to EKS using Spinnaker pipeline.\nSpinnaker UI Create application Click on Create Application and enter details as Product-Detail Create Pipeline Click on Pipelines under Product-Detail and click on Configure a new pipeline and add the name as below. Setup Trigger Click on Configuration under Pipelines and click on Add Trigger. This is the ECR registry we had setup in Spinnaker manifest in Configure Artifact chapter. Follow the below setup and click on \u0026ldquo;Save Changes\u0026rdquo;. Setup Bake Stage   Click on Add Stage and select Bake Manifest for Type from the dropdown\n  Select \u0026ldquo;Helm3\u0026rdquo; as Render Engine and enter name.\n  Select \u0026ldquo;Define a new artifact\u0026rdquo; for Expected Artifact and select your Git account that shows in dropdown. This is the Git account we had setup in Spinnaker manifest in Configure Artifact chapter. And then enter the below git location in the Content URL and add main as the Commit/Branch. This is to provide the the Helm template for the deployment.\nhttps://api.github.com/repos/aws-containers/eks-microservice-demo/contents/spinnaker/proddetail-0.1.0.tgz\n  Under the Overrides section, Select \u0026ldquo;create new artifact\u0026rdquo; for Expected Artifact and select your Git account that shows in dropdown. This is the Git account we had setup in Spinnaker manifest in Configure Artifact chapter. And then enter the below git location in the Content URL and add main as the Commit/Branch. This is to provide the overrides for the Helm template using values.yaml.\nhttps://api.github.com/repos/aws-containers/eks-microservice-demo/contents/spinnaker/helm-chart/values.yaml\n   Edit the Produces Artifact and change the name to helm-produced-artifact and click on Save Changes.  Setup Deploy Stage  Click on Add Stage and select Deploy (Manifest) from the dropdown for Type, and give a name as Bake proddetail. Select Type as spinnaker-workshop from the dropdown for Account. This is the EKS account we had setup in Spinnaker manifest in Add EKS Account chapter. Select helm-produced-artifact from the dropdown for Manifest Artifact and click on Save Changes.  Test Deployment Push new container image to ECR for testing trigger To ensure that the ECR trigger will work in Spinnaker UI:\n First change the content of the file to generate a new docker image digest. ECR trigger in Spinnaker does not work for same docker image digest. Go to ~/environment/eks-microservice-demo/apps/detail/app.js and add a comment to first line of the file like below  // commenting for file for docker image generation  Ensure that the image tag (APP_VERSION) you are adding below does not exist in the ECR repository eks-microservice-demo/test otherwise the trigger will not work. Spinnaker pipeline only triggers when a new version of image is added to ECR.\n And then run the below command in Cloud9 terminal.\ncd ~/environment/eks-microservice-demo aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com export APP_VERSION=1.0 export ECR_REPOSITORY=eks-microservice-demo/test TARGET=$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPOSITORY:$APP_VERSION docker build -t $TARGET apps/detail --no-cache docker push $TARGET  Building/Pushing Container images first time to ECR may take around 3-5 minutes\n Watch Pipleline getting triggered  You will see that docker push triggers a deployment in the pipeline.   Below are the Execution Details of pipeline  Get deploymet details  You can see the deployment of frontend and detail service below.   Click on the LoadBalancer link below and paste it on browser like http://a991d7csdsdsdsdsdsds-1949669176.XXXXX.elb.amazonaws.com:9000/   You should see the service up and running as below.   You can also go to Cloud9 terminal and confirm the deployment details  \tkubectl get all -n detail NAME READY STATUS RESTARTS AGE pod/frontend-677dd7d654-nzxbq 1/1 Running 0 152m pod/nginx-deployment-6dc677db6-jchq8 1/1 Running 0 22h pod/proddetail-65cf5d598c-h9l7s 1/1 Running 0 152m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/frontend LoadBalancer 10.100.73.76 a991d7csdsdsdsdsdsds-1949669176.XXXXX.elb.amazonaws.com 9000:30229/TCP 152m service/proddetail ClusterIP 10.100.37.158 \u0026lt;none\u0026gt; 3000/TCP 152m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/frontend 1/1 1 1 152m deployment.apps/nginx-deployment 1/1 1 1 22h deployment.apps/proddetail 1/1 1 1 152m NAME DESIRED CURRENT READY AGE replicaset.apps/frontend-677dd7d654 1 1 1 152m replicaset.apps/nginx-deployment-6dc677db6 1 1 1 22h replicaset.apps/proddetail-65cf5d598c 1 1 1 152m "
},
{
	"uri": "/advanced/430_emr_on_eks/eks_emr_using_node_selectors/",
	"title": "Using Node Selectors",
	"tags": [],
	"description": "",
	"content": "AWS EKS clusters can span multiple AZs in a VPC. A Spark application whose driver and executor pods are distributed across multiple AZs can incur inter-AZ data transfer costs. To minimize or eliminate inter-AZ data transfer costs, you can configure the application to only run on the nodes within a single AZ. In this example, we use the kubernetes node selector \u0026ldquo;topology.kubernetes.io/zone\u0026rdquo; to specify which AZ should the job run on.\nYou will use spark jobs running on EKS to analyze the New York City Taxi and Limousine Commision (TLC) Trip Record Data . For calendar years 2019 and 2020, you will calculate top-10 pickup locations in New York area based on total amount of distance travelled by the taxi\u0026rsquo;s. The data is available at Registry of Open Data on AWS at https://registry.opendata.aws/nyc-tlc-trip-records-pds/.\nCreating Nodegroup Amazon EKS managed node groups automate the provisioning and lifecycle management of nodes (Amazon EC2 instances) for Amazon EKS Kubernetes clusters. All managed nodes are provisioned as part of an Amazon EC2 Auto Scaling group that\u0026rsquo;s managed for you by Amazon EKS. All resources including the instances and Auto Scaling groups run within your AWS account. Each node group run across multiple Availability Zones that you define.\nWe will now create a managed node group for our example, let‚Äôs create a config file (addnodegroup-nytaxi.yaml) with details of a new managed node group.\ncat \u0026lt;\u0026lt; EOF \u0026gt; addnodegroup-nytaxi.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} managedNodeGroups: - name: emr-ny-taxi minSize: 6 desiredCapacity: 6 maxSize: 6 instanceType: m5.xlarge ssh: enableSsm: true EOF Create the new EKS managed nodegroup. This node group will have 6 EC2s running across three AZs.\neksctl create nodegroup --config-file=addnodegroup-nytaxi.yaml Spark Pod Template Next, you will create a pod template for Spark Executor. Here, we are specifying nodeSelector as eks.amazonaws.com/nodegroup: emr-ny-taxi and topology.kubernetes.io/zone: us-west-2a. This will ensure that spark executors are running in a single AZ (us-west-2 in this example) and are part of nodegroup which we created for analyzing New York taxi dataset.\nWe chose us-west-2 as this example was ran in Oregon. If you are running in a different region, make sure that the correct zone is selected.\ncat \u0026gt; spark_executor_nyc_taxi_template.yml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod spec: volumes: - name: source-data-volume emptyDir: {} - name: metrics-files-volume emptyDir: {} nodeSelector: eks.amazonaws.com/nodegroup: emr-ny-taxi topology.kubernetes.io/zone: us-west-2a containers: - name: spark-kubernetes-executor # This will be interpreted as Spark executor container EOF Next, you will create a pod template for Spark Driver. Here, we are specifying nodeSelector as eks.amazonaws.com/nodegroup: emr-ny-taxi and topology.kubernetes.io/zone: us-west-2a. The Spark Driver pods will run in the same AZ as Spark executor pods.\ncat \u0026gt; spark_driver_nyc_taxi_template.yml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod spec: volumes: - name: source-data-volume emptyDir: {} - name: metrics-files-volume emptyDir: {} nodeSelector: eks.amazonaws.com/nodegroup: emr-ny-taxi topology.kubernetes.io/zone: us-west-2a containers: - name: spark-kubernetes-driver # This will be interpreted as Spark driver container EOF Lets create a nytaxi.py file which will have spark code for analyzing the data set.\ncat \u0026lt;\u0026lt; EOF \u0026gt; nytaxi.py import sys from time import sleep from pyspark.sql import SparkSession from pyspark.sql import functions as F from pyspark.sql.window import Window spark = SparkSession.builder.appName(\u0026#39;uber_nyc_data\u0026#39;).getOrCreate() yellowtripdata_2020 = spark.read.format(\u0026#39;csv\u0026#39;).option(\u0026#39;header\u0026#39;, \u0026#39;true\u0026#39;).load(\u0026#39;s3://nyc-tlc/trip data/yellow_tripdata_2020*\u0026#39;) yellowtripdata_2020 = yellowtripdata_2020.withColumn(\u0026#39;year\u0026#39;, F.lit(2020)) yellowtripdata_2019 = spark.read.format(\u0026#39;csv\u0026#39;).option(\u0026#39;header\u0026#39;, \u0026#39;true\u0026#39;).load(\u0026#39;s3://nyc-tlc/trip data/yellow_tripdata_2019*\u0026#39;) yellowtripdata_2019 = yellowtripdata_2019.withColumn(\u0026#39;year\u0026#39;, F.lit(2019)) yellowtripdata = yellowtripdata_2020.union(yellowtripdata_2019.select(yellowtripdata_2020.columns)) yellowtripdata = yellowtripdata.withColumn(\u0026#39;total_amount\u0026#39;, F.col(\u0026#39;total_amount\u0026#39;).cast(\u0026#39;double\u0026#39;)) yellowtripdata = yellowtripdata.withColumn(\u0026#39;tip_amount\u0026#39;, F.col(\u0026#39;tip_amount\u0026#39;).cast(\u0026#39;double\u0026#39;)) yellowtripdata = yellowtripdata.withColumn(\u0026#39;trip_distance\u0026#39;, F.col(\u0026#39;trip_distance\u0026#39;).cast(\u0026#39;double\u0026#39;)) greentripdata_2020 = spark.read.format(\u0026#39;csv\u0026#39;).option(\u0026#39;header\u0026#39;, \u0026#39;true\u0026#39;).option(\u0026#39;inferschema\u0026#39;, \u0026#39;true\u0026#39;).load(\u0026#39;s3://nyc-tlc/trip data/green_tripdata_2020*\u0026#39;) greentripdata_2020 = greentripdata_2020.withColumn(\u0026#39;year\u0026#39;, F.lit(2020)) greentripdata_2019 = spark.read.format(\u0026#39;csv\u0026#39;).option(\u0026#39;header\u0026#39;, \u0026#39;true\u0026#39;).option(\u0026#39;inferschema\u0026#39;, \u0026#39;true\u0026#39;).load(\u0026#39;s3://nyc-tlc/trip data/green_tripdata_2019*\u0026#39;) greentripdata_2019 = greentripdata_2019.withColumn(\u0026#39;year\u0026#39;, F.lit(2019)) greentripdata = greentripdata_2020.union(greentripdata_2019.select(greentripdata_2020.columns)) greentripdata = greentripdata.withColumn(\u0026#39;total_amount\u0026#39;, F.col(\u0026#39;total_amount\u0026#39;).cast(\u0026#39;double\u0026#39;)) greentripdata = greentripdata.withColumn(\u0026#39;tip_amount\u0026#39;, F.col(\u0026#39;tip_amount\u0026#39;).cast(\u0026#39;double\u0026#39;)) greentripdata = greentripdata.withColumn(\u0026#39;trip_distance\u0026#39;, F.col(\u0026#39;trip_distance\u0026#39;).cast(\u0026#39;double\u0026#39;)) greentaxitrips_full_year = greentripdata.groupby(\u0026#39;PULocationID\u0026#39;, \u0026#39;year\u0026#39;).agg( F.sum(\u0026#39;trip_distance\u0026#39;).alias(\u0026#39;trip_distance\u0026#39;), F.sum(\u0026#39;total_amount\u0026#39;).alias(\u0026#39;total_amount\u0026#39;), F.sum(\u0026#39;tip_amount\u0026#39;).alias(\u0026#39;tip_amount\u0026#39;) ) yellowtaxitrips_full_year = yellowtripdata.groupby(\u0026#39;PULocationID\u0026#39;, \u0026#39;year\u0026#39;).agg( F.sum(\u0026#39;trip_distance\u0026#39;).alias(\u0026#39;trip_distance\u0026#39;), F.sum(\u0026#39;total_amount\u0026#39;).alias(\u0026#39;total_amount\u0026#39;), F.sum(\u0026#39;tip_amount\u0026#39;).alias(\u0026#39;tip_amount\u0026#39;) ) w = Window.partitionBy(\u0026#39;year\u0026#39;).orderBy(F.col(\u0026#39;trip_distance\u0026#39;).desc()) yellowtaxitrips_full_year = yellowtaxitrips_full_year.withColumn(\u0026#39;rank\u0026#39;,F.row_number().over(w)) w1 = Window.partitionBy(\u0026#39;year\u0026#39;).orderBy(F.col(\u0026#39;trip_distance\u0026#39;).desc()) greentaxitrips_full_year = greentaxitrips_full_year.withColumn(\u0026#39;rank\u0026#39;,F.row_number().over(w1)) print(yellowtaxitrips_full_year.filter(F.col(\u0026#39;rank\u0026#39;) \u0026lt;= 10).orderBy(\u0026#39;year\u0026#39;,\u0026#39;rank\u0026#39;).head(75)) print(greentaxitrips_full_year.filter(F.col(\u0026#39;rank\u0026#39;) \u0026lt;= 10).orderBy(\u0026#39;year\u0026#39;,\u0026#39;rank\u0026#39;).head(75)) spark.stop() EOF Let\u0026rsquo;s upload sample pod templates and python script to s3 bucket.\naws s3 cp nytaxi.py ${s3DemoBucket} aws s3 cp spark_driver_nyc_taxi_template.yml ${s3DemoBucket}/pod_templates/ aws s3 cp spark_executor_nyc_taxi_template.yml ${s3DemoBucket}/pod_templates/ Let\u0026rsquo;s create a json input file for emr-container cli.\ncat \u0026gt; request-nytaxi.json \u0026lt;\u0026lt;EOF { \u0026#34;name\u0026#34;: \u0026#34;nytaxi\u0026#34;, \u0026#34;virtualClusterId\u0026#34;: \u0026#34;${VIRTUAL_CLUSTER_ID}\u0026#34;, \u0026#34;executionRoleArn\u0026#34;: \u0026#34;${EMR_ROLE_ARN}\u0026#34;, \u0026#34;releaseLabel\u0026#34;: \u0026#34;emr-5.33.0-latest\u0026#34;, \u0026#34;jobDriver\u0026#34;: { \u0026#34;sparkSubmitJobDriver\u0026#34;: { \u0026#34;entryPoint\u0026#34;: \u0026#34;${s3DemoBucket}/nytaxi.py\u0026#34;, \u0026#34;sparkSubmitParameters\u0026#34;: \u0026#34;--conf spark.kubernetes.driver.podTemplateFile=${s3DemoBucket}/pod_templates/spark_driver_nyc_taxi_template.yml \\ --conf spark.kubernetes.executor.podTemplateFile=${s3DemoBucket}/pod_templates/spark_executor_nyc_taxi_template.yml \\ --conf spark.executor.instances=3 \\ --conf spark.executor.memory=2G \\ --conf spark.executor.cores=2 \\ --conf spark.driver.cores=1\u0026#34; } }, \u0026#34;configurationOverrides\u0026#34;: { \u0026#34;applicationConfiguration\u0026#34;: [ { \u0026#34;classification\u0026#34;: \u0026#34;spark-defaults\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;spark.dynamicAllocation.enabled\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;spark.kubernetes.executor.deleteOnTermination\u0026#34;: \u0026#34;true\u0026#34; } } ], \u0026#34;monitoringConfiguration\u0026#34;: { \u0026#34;cloudWatchMonitoringConfiguration\u0026#34;: { \u0026#34;logGroupName\u0026#34;: \u0026#34;/emr-on-eks/eksworkshop-eksctl\u0026#34;, \u0026#34;logStreamNamePrefix\u0026#34;: \u0026#34;nytaxi\u0026#34; }, \u0026#34;s3MonitoringConfiguration\u0026#34;: { \u0026#34;logUri\u0026#34;: \u0026#34;${s3DemoBucket}/\u0026#34; } } } } EOF Finally, let\u0026rsquo;s trigger the Spark job\naws emr-containers start-job-run --cli-input-json file://request-nytaxi.json The spark job will run for around 5-7 minutes. You can watch the EKS pods using below command\nwatch kubectl get pods -n spark You will be able to see the completed job in EMR console. It should look like below:\nAnalyzing Results of Spark Job The results of the spark job are avilable in CloudWatch under log group \u0026ldquo;/emr-on-eks/eksworkshop-eksctl\u0026rdquo; in a spark driver log stream ending in \u0026ldquo;stdout\u0026rdquo;. For example the results of our spark job can be seen at \u0026ldquo;nytaxi/ftuls1q50staavpu9914lln00/jobs/00000002uc2sglcmtm4/containers/spark-797747e3709842048d8fa6d1feda3cb2/spark-00000002uc2sglcmtm4-driver/stdout\u0026rdquo;\nAs we can see, for yellow cabs, the most famous pickup locations are \u0026ldquo;132\u0026rdquo; and \u0026ldquo;138\u0026rdquo; for both 2019 and 2020. \u0026ldquo;132\u0026rdquo; and \u0026ldquo;138\u0026rdquo; are the codes for JFK and LaGuardia airports. The code to location mapping is defined at https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page under \u0026ldquo;Taxi Zone Lookup Table\u0026rdquo;.\n\u0026ldquo;132\u0026rdquo;/JFK is the busiest location for both 2019 and 2020 and we can see the significant drop in traffic due to covid-19. For 2019, the cumulative distance travelled by all yellow cabs from JFK as a pickup location was 41.1 million miles. The number dropped to 9 million miles for 2020.\nSimilarly, the total distance travelled for second most busiest pick up location (138/LaGuardia) dropped from 19.8 million miles to 3.6 million miles.\nFor Green Cabs, top two locations in 2019 were \u0026ldquo;74\u0026rdquo; and \u0026ldquo;75\u0026rdquo; in 2019. This changed to \u0026ldquo;254\u0026rdquo; and \u0026ldquo;235\u0026rdquo; in 2020.\n"
},
{
	"uri": "/advanced/350_opentelemetry/kubernetes_metrics/receiver/",
	"title": "Receiver",
	"tags": [],
	"description": "",
	"content": "Deploying the Prometheus Metrics Scraper Let\u0026rsquo;s deploy a single OpenTelemetry Collector which scrapes prometheus metrics from the EKS Control Plane and sends them to CloudWatch.\nFirst let\u0026rsquo;s create a temporary environment variable place holder.\nexport WORKSPACE_ID=\u0026#39;${WORKSPACE_ID}\u0026#39; Now we\u0026rsquo;ll update the manifest for this Prometheus metric scraper with our environment variables, and deploy the singleton collector.\nenvsubst \u0026lt; kubernetes/adot/otel-prometheus.yaml | sponge kubernetes/adot/otel-prometheus.yaml kubectl apply -f kubernetes/adot/otel-prometheus.yaml Let\u0026rsquo;s check that the scraper is deployed and running. Wait until READY shows 1/1. $ kubectl get pods -n aws-otel-eks -l name=aws-otel-eks-prometheus NAME READY STATUS RESTARTS AGE aws-otel-eks-prometheus-5bb78cb869-hrmf4 1/1 Running 0 5m  Viewing Kubernetes Metrics in CloudWatch The Prometheus collector is now deployed, let\u0026rsquo;s open the CloudWatch console to see the metrics pulled in. A direct link to the CloudWatch console can be generated with this command:\necho -e \u0026#34;https://${AWS_REGION}.console.aws.amazon.com/cloudwatch/home?region=${AWS_REGION}\u0026#34; Open the URL, and head to Metrics \u0026gt; All Metrics. You will see a namespace of ContainerInsights/Prometheus, which contains the scraped Kubernetes Metrics.\nIt can take a few minutes before you start seeing the Kubernetes metrics in CloudWatch.\n "
},
{
	"uri": "/beginner/120_network-policies/calico/stars_policy_demo/",
	"title": "Stars Policy Demo",
	"tags": [],
	"description": "",
	"content": "Stars Policy Demo In this sub-chapter we create frontend, backend, client and UI services on the EKS cluster and define network policies to allow or block communication between these services. This demo also has a management UI that shows the available ingress and egress paths between each service.\n"
},
{
	"uri": "/advanced/350_opentelemetry/kubernetes_metrics/export_to_amp/",
	"title": "Export to AMP",
	"tags": [],
	"description": "",
	"content": "In previous section we learned how to receive metrics from the Kubernetes Control Plane and export them to CloudWatch using the OpenTelemetry collector. In this section we are going to demonstrate a core benefit of the OpenTelemetry Collector pattern, by configuring it to send the Kubernetes Control Plane metrics to an additional destination, AWS Managed Service for Prometheus (AMP).\nLet\u0026rsquo;s create an AMP workspace:\naws amp create-workspace --alias eks-workshop --region $AWS_REGION IAM Policy \u0026amp; IAM Role Now we create a new IAM Policy (AWSManagedPrometheusWriteAccessPolicy) which grants AMP read and write permission:\nread -r -d \u0026#39;\u0026#39; PERMISSION_POLICY \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;:[ { \u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;:[ \u0026#34;aps:RemoteWrite\u0026#34;, \u0026#34;aps:QueryMetrics\u0026#34;, \u0026#34;aps:GetSeries\u0026#34;, \u0026#34;aps:GetLabels\u0026#34;, \u0026#34;aps:GetMetricMetadata\u0026#34; ], \u0026#34;Resource\u0026#34;:\u0026#34;*\u0026#34; } ] } EOF echo \u0026#34;${PERMISSION_POLICY}\u0026#34; \u0026gt; AMPPolicy.json export SERVICE_ACCOUNT_IAM_POLICY_ARN=\u0026#34;arn:aws:iam::${ACCOUNT_ID}:policy/AWSManagedPrometheusWriteAccessPolicy\u0026#34; aws iam create-policy --policy-name AWSManagedPrometheusWriteAccessPolicy --policy-document file://AMPPolicy.json Then the AWSManagedPrometheusWriteAccessPolicy IAM Policy is attached to the IAM Role (AWSDistroOpenTelemetryRole) we previously created for our OpenTelemetry collector.\naws iam attach-role-policy --role-name AWSDistroOpenTelemetryRole --policy-arn=${SERVICE_ACCOUNT_IAM_POLICY_ARN} Exporter Open kubernetes/adot/otel-prometheus.yaml, and view lines 63-67. This describes an exporter which sends metrics to an AMP workspace. exporters: #awsprometheusremotewrite: # endpoint: https://aps-workspaces.eu-central-1.amazonaws.com/workspaces/${WORKSPACE_ID}/api/v1/remote_write # aws_auth: # service: \u0026#34;aps\u0026#34; # region: ${AWS_REGION}  Let\u0026rsquo;s un-comment lines 63-67 with the following command:\nsed -i \u0026#39;63,67 s/#//g\u0026#39; kubernetes/adot/otel-prometheus.yaml Pipeline Now in kubernetes/adot/otel-prometheus.yaml let\u0026rsquo;s view lines 116-119. This outlines a new pipeline, which takes in the same prometheus metrics we send to CloudWatch, processes the metrics into batched segments, then exports the metrics to our AMP workspace. service: pipelines: metrics: receivers: [prometheus] processors: [resourcedetection, resource] exporters: [awsemf] #metrics/prom: # receivers: [prometheus] # processors: [batch] # exporters: [awsprometheusremotewrite]  Let\u0026rsquo;s un-comment lines 116-119 with the following command:\nsed -i \u0026#39;116,119 s/#//g\u0026#39; kubernetes/adot/otel-prometheus.yaml Updating Prometheus Metrics Collector Now that the Prometheus Metrics Collector\u0026rsquo;s configuration was updated to send metrics to Prometheus, let\u0026rsquo;s update our deployment.\nFirst, let\u0026rsquo;s grab our AMP Workspace ID:\nexport WORKSPACE_ID=$(aws amp --region=$AWS_REGION list-workspaces --alias eks-workshop | jq .workspaces[0].workspaceId -r) echo \u0026#34;AMP Workspace ID: ${WORKSPACE_ID}\u0026#34; Now we\u0026rsquo;ll update the manifest with our environment variables, deploy the new manifest, and like before force-recreate the collector to speed up the rollout.\nenvsubst \u0026lt; kubernetes/adot/otel-prometheus.yaml | sponge kubernetes/adot/otel-prometheus.yaml kubectl apply -f kubernetes/adot/otel-prometheus.yaml kubectl delete pod -n aws-otel-eks -l name=aws-otel-eks-prometheus Now our OpenTelemetry Collector is sending Kubernetes Control Plane metrics to both CloudWatch and Prometheus.\n"
},
{
	"uri": "/advanced/350_opentelemetry/kubernetes_metrics/setup_grafana/",
	"title": "Setup Grafana ",
	"tags": [],
	"description": "",
	"content": "In the previous section we exported Kubernetes Control Plane metrics to our AMP workspace. Now let\u0026rsquo;s visualize this data using Grafana. This workshop uses open source Grafana. This is due to a limitation on the AWS EventEngine tool not yet supporting AWS Managed Grafana (AMG). If you are doing this lab on your own account, you can use AWS Managed Grafana.\n Deploying Grafana First install Helm\ncurl -fsSL -o /tmp/get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod +x /tmp/get_helm.sh /tmp/get_helm.sh helm version Then, update the Grafana configuration with our AWS Account specific configuration\nenvsubst \u0026lt; kubernetes/adot/grafana.values | sponge kubernetes/adot/grafana.values Now install Grafana onto our EKS Cluster using Helm\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update helm upgrade --install -n aws-otel-eks -f kubernetes/adot/grafana.values grafana grafana/grafana Configuring AMP Data Source in Grafana Grab the Grafana UI URL\nexport GRAFANA_SVC=$(kubectl -n aws-otel-eks get svc grafana --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\u0026#34;) echo $GRAFANA_SVC In a different browser tab, open the above URL. Note, it can take a few minutes for DNS to propegate. Credentials to login listed below. user: admin password: eksworkshop  In the Grafana console select Configurations, and choose Data sources in the sub-menu. Select \u0026ldquo;Add Data source\u0026rdquo; Choose Prometheus Get the endpoint for our AMP instance, and copy \u0026amp; paste it into Grafana for the HTTP \u0026gt; URL configuration\naws amp describe-workspace --workspace-id $WORKSPACE_ID | jq \u0026#39;.workspace.prometheusEndpoint\u0026#39; -r Scroll down to the Auth section, and select the toggle to enable SigV4 Auth. You‚Äôll then have a SigV4 Auth Details configuration section appear. Set Default Region to the same AWS region of your AMP Workspace. You can get the AWS Region by running $ echo $AWS_REGION.\nScroll down to the bottom of the page, and select Save \u0026amp; Test. After testing is finished, you should see a green check-mark stating the Data source is working.\nGrafana is now configured to read metrics from our AMP instance. In the next section we\u0026rsquo;ll cover visualizing these metrics with Grafana.\n"
},
{
	"uri": "/advanced/350_opentelemetry/kubernetes_metrics/visualizing_with_grafana/",
	"title": "Vizualizing with Grafana",
	"tags": [],
	"description": "",
	"content": "Now that Grafana is setup to read metrics from Prometheus, let\u0026rsquo;s create a dashboard on our Kubernetes Cluster\u0026rsquo;s health.\nEnd to End test AMP \u0026amp; Grafana In the Grafana web-console, let\u0026rsquo;s head to Explore Let\u0026rsquo;s run a test PromQL query against our AMP workspace. In the query bar type in etcd_object_counts, then click the Run Query button on the top right. If this query returns data, we can confirm that we are 1) Ingesting Kubernetes Control Plane metrics to our AMP workspace. 2) Grafana is able to connect to our AMP workspace.\nCreating Kubernetes Grafana Dashboard In the Nav select Create, and choose Import. Enter 12006 in the field for the Grafana.com dashboard ID, and then click the Load button Click on Import Now we can see a pre-made dashboard outlining our Kubernetes Control Plane metrics Congratulations! You\u0026rsquo;ve now used the AWS Distro for OpenTelemetry to\n receive metrics from Container Insights receive traces from the deployed microservices store the telemetry data using CloudWatch, X-Ray, and AWS Managed Service for Prometheus Visualize using X-Ray, CloudWatch, and Grafana.  "
},
{
	"uri": "/advanced/",
	"title": "Advanced",
	"tags": ["advanced"],
	"description": "",
	"content": "Advanced "
},
{
	"uri": "/intermediate/246_monitoring_amp_amg/query_metrics/",
	"title": "Query Metrics",
	"tags": [],
	"description": "",
	"content": "Configure AMP data source Select AWS services from the AWS logo on the left navigation bar, which will take you to the screen as shown below showing all the AWS data sources available for you to choose from.\nSelect Prometheus from the list, select the AWS Region where you created the AMP workspace. This will automatically populate the AMP workspaces available in that Region as shown below.\nSimply select the AMP workspace from the list and click Add data sources. Once added you will able to see that the AMP data source is authenticated through SigV4 protocol. Grafana (7.3.5 and above) has the AWS SigV4 proxy built-in as a plugin which makes this possible.\nQuery Metrics In this section we will be importing a public Grafana dashboard that allows us to visualize metrics from a Kubernetes environment.\nGo to the plus sign on the left navigation bar and select Import. In the Import screen, type 3119 in Import via grafana.com textbox and click Load\nSelect the AMP data source in the drop down at the bottom and click on Import\nOnce complete, you will be able to see the Grafana dashboard showing metrics from the EKS cluster through AMP data source as shown below.\nYou can also create your own custom dashboard using PromQL by creating a custom dashboard and adding a panel connecting AMP as the data source.\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/port_to_app_mesh/trigger_sidecar_injection/",
	"title": "Sidecar Injection",
	"tags": [],
	"description": "",
	"content": "For a pod in your application to join a mesh, it must have an Envoy proxy container running sidecar within the pod. This establishes the data plane that AWS App Mesh controls. So we must run an Envoy container within each pod of the Product Catalog App deployment. For example:\nThis can be accomplished a few different ways:\n  Before installing the application, you can modify the Product Catalog App Deployment container specs to include App Mesh sidecar containers and set a few required configuration elements and environment variables. When pods are deployed, they would run the sidecar.\n  After installing the application, you can patch each Deployment to include the sidecar container specs. Upon applying this patch, the old pods would be torn down, and the new pods would come up with the sidecar.\n  You can enable the AWS App Mesh Sidecar Injector in the meshed namespace, which watches for new pods to be created and automatically adds the sidecar container and required configuration to the pods as they are deployed.\n  In this tutorial, we will use the third option and enable automatic sidecar injection for our meshed pods. We have enabled automatic sidecar injection by adding label Labels: appmesh.k8s.aws/sidecarInjectorWebhook=enabled on the prodcatalog-ns namespace when we created the mesh resources in previous chapter, but this was done after initial pod creation. Currently, our pods each have one container running.\nkubectl get pods -n prodcatalog-ns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/frontend-node-77d64585d4-xxxx 1/1 Running 0 13h 192.168.X.6 ip-192-168-X-X.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prodcatalog-98f7c5f87-xxxxx 1/1 Running 0 13h 192.168.X.17 fargate-ip-192-168-X-X.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/proddetail-5b558df99d-xxxxx 1/1 Running 0 18h 192.168.24.X ip-192-168-X-X.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  To inject sidecar proxies for these pods, simply restart the deployments. The controller will handle the rest.\nkubectl -n prodcatalog-ns rollout restart deployment prodcatalog kubectl -n prodcatalog-ns rollout restart deployment proddetail kubectl -n prodcatalog-ns rollout restart deployment frontend-node Get the Pod details. You should see 3 containers in each pod main application container, envoy sidecar container and xray sidecar container\nIt takes 4 to 6 minutes to restart the Fargate Pod\n kubectl get pods -n prodcatalog-ns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/frontend-node-77d64585d4-xxxx 3/3 Running 0 13h 192.168.X.6 ip-192-168-X-X.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/prodcatalog-98f7c5f87-xxxxx 3/3 Running 0 13h 192.168.X.17 fargate-ip-192-168-X-X.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/proddetail-5b558df99d-xxxxx 3/3 Running 0 18h 192.168.24.X ip-192-168-X-X.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 3000/TCP 44h app=proddetail,version=v1  Get Running containers from pod We can see that there are two sidecar containers envoy and xray-daemon along with application container frontend-node\nPOD=$(kubectl -n prodcatalog-ns get pods -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) kubectl -n prodcatalog-ns get pods ${POD} -o jsonpath=\u0026#39;{.spec.containers[*].name}\u0026#39;; echo frontend-node envoy xray-daemon  "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/deploy_applications/test_the_app/",
	"title": "Test the Application",
	"tags": [],
	"description": "",
	"content": "Testing the Connectivity between Fargate and Nodegroup pods To test if our ported Product Catalog App is working as expected, we\u0026rsquo;ll first exec into the frontend-node container.\nexport FE_POD_NAME=$(kubectl get pods -n prodcatalog-ns -l app=frontend-node -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n prodcatalog-ns exec -it ${FE_POD_NAME} -c frontend-node bash You will see a prompt from within the frontend-node container.\nroot@frontend-node-9d46cb55-XXX:/usr/src/app#  curl to Fargate prodcatalog backend endpoint and you should see the below response\ncurl http://prodcatalog.prodcatalog-ns.svc.cluster.local:5000/products/ { \u0026#34;products\u0026#34;: {}, \u0026#34;details\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;vendors\u0026#34;: [ \u0026#34;ABC.com\u0026#34; ] } }  Exit from the frontend-node container exec bash. Now, To test the connectivity from Fargate service prodcatalog to Nodegroup service proddetail, we\u0026rsquo;ll first exec into the prodcatalog container.\nexport BE_POD_NAME=$(kubectl get pods -n prodcatalog-ns -l app=prodcatalog -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n prodcatalog-ns exec -it ${BE_POD_NAME} -c prodcatalog bash You will see a prompt from within the prodcatalog container.\nroot@prodcatalog-98f7c5f87-xxxxx:/usr/src/app#  curl to Nodegroup proddetail backend endpoint and you should see the below response. You can now exit from prodcatalog exec bash.\ncurl http://proddetail.prodcatalog-ns.svc.cluster.local:3000/catalogDetail {\u0026#34;version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;vendors\u0026#34;:[\u0026#34;ABC.com\u0026#34;]}  Congratulations on deploying the initial Product Catalog Application architecture!\nBefore we create the App Mesh-enabled versions of Product Catalog App, we\u0026rsquo;ll first install the AWS App Mesh Controller for Kubernetes into our cluster.\n"
},
{
	"uri": "/advanced/310_servicemesh_with_istio/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To cleanup, follow the below steps.\nTerminate any kubectl port-forward or watch processes\nkillall kubectl killall watch When you‚Äôre finished experimenting with the Bookinfo sample, uninstall and clean it up using the following instructions\nexport ISTIO_RELEASE=$(echo $ISTIO_VERSION |cut -d. -f1,2) kubectl delete -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_RELEASE}/samples/addons/jaeger.yaml kubectl delete -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_RELEASE}/samples/addons/kiali.yaml kubectl delete -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_RELEASE}/samples/addons/prometheus.yaml kubectl delete -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_RELEASE}/samples/addons/grafana.yaml export NAMESPACE=\u0026#34;bookinfo\u0026#34; ${HOME}/environment/istio-${ISTIO_VERSION}/samples/bookinfo/platform/kube/cleanup.sh istioctl manifest generate --set profile=demo | kubectl delete -f - kubectl delete ns bookinfo kubectl delete ns istio-system  You can ignore the errors for non-existent resources because they may have been deleted hierarchically.\n Finally, we can delete the istio folder and clean up the ~/.bash_profile.\ncd ~/environment rm -rf istio-${ISTIO_VERSION} sed -i \u0026#39;/ISTIO_VERSION/d\u0026#39; ${HOME}/.bash_profile unset ISTIO_VERSION "
},
{
	"uri": "/advanced/410_batch/workflow-advanced/",
	"title": "Advanced Batch Workflow",
	"tags": [],
	"description": "",
	"content": "Advanced Batch Workflow Let\u0026rsquo;s take a look at a more complex workflow, involving passing artifacts between jobs, multiple dependencies, etc.\nCreate teardrop.yaml using the command below:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/batch_policy/teardrop.yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: teardrop- spec: entrypoint: teardrop templates: - name: create-chain container: image: alpine:latest command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;] args: [\u0026#34;echo \u0026#39;\u0026#39; \u0026gt;\u0026gt; /tmp/message\u0026#34;] outputs: artifacts: - name: chain path: /tmp/message - name: whalesay inputs: parameters: - name: message artifacts: - name: chain path: /tmp/message container: image: docker/whalesay command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;] args: [\u0026#34;echo Chain: ; cat /tmp/message* | sort | uniq | tee /tmp/message; cowsay This is Job {{inputs.parameters.message}}! ; echo {{inputs.parameters.message}} \u0026gt;\u0026gt; /tmp/message\u0026#34;] outputs: artifacts: - name: chain path: /tmp/message - name: whalesay-reduce inputs: parameters: - name: message artifacts: - name: chain-0 path: /tmp/message.0 - name: chain-1 path: /tmp/message.1 container: image: docker/whalesay command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;] args: [\u0026#34;echo Chain: ; cat /tmp/message* | sort | uniq | tee /tmp/message; cowsay This is Job {{inputs.parameters.message}}! ; echo {{inputs.parameters.message}} \u0026gt;\u0026gt; /tmp/message\u0026#34;] outputs: artifacts: - name: chain path: /tmp/message - name: teardrop dag: tasks: - name: create-chain template: create-chain - name: Alpha dependencies: [create-chain] template: whalesay arguments: parameters: [{name: message, value: Alpha}] artifacts: - name: chain from: \u0026#34;{{tasks.create-chain.outputs.artifacts.chain}}\u0026#34; - name: Bravo dependencies: [Alpha] template: whalesay arguments: parameters: [{name: message, value: Bravo}] artifacts: - name: chain from: \u0026#34;{{tasks.Alpha.outputs.artifacts.chain}}\u0026#34; - name: Charlie dependencies: [Alpha] template: whalesay arguments: parameters: [{name: message, value: Charlie}] artifacts: - name: chain from: \u0026#34;{{tasks.Alpha.outputs.artifacts.chain}}\u0026#34; - name: Delta dependencies: [Bravo] template: whalesay arguments: parameters: [{name: message, value: Delta}] artifacts: - name: chain from: \u0026#34;{{tasks.Bravo.outputs.artifacts.chain}}\u0026#34; - name: Echo dependencies: [Bravo, Charlie] template: whalesay-reduce arguments: parameters: [{name: message, value: Echo}] artifacts: - name: chain-0 from: \u0026#34;{{tasks.Bravo.outputs.artifacts.chain}}\u0026#34; - name: chain-1 from: \u0026#34;{{tasks.Charlie.outputs.artifacts.chain}}\u0026#34; - name: Foxtrot dependencies: [Charlie] template: whalesay arguments: parameters: [{name: message, value: Foxtrot}] artifacts: - name: chain from: \u0026#34;{{tasks.create-chain.outputs.artifacts.chain}}\u0026#34; - name: Golf dependencies: [Delta, Echo] template: whalesay-reduce arguments: parameters: [{name: message, value: Golf}] artifacts: - name: chain-0 from: \u0026#34;{{tasks.Delta.outputs.artifacts.chain}}\u0026#34; - name: chain-1 from: \u0026#34;{{tasks.Echo.outputs.artifacts.chain}}\u0026#34; - name: Hotel dependencies: [Echo, Foxtrot] template: whalesay-reduce arguments: parameters: [{name: message, value: Hotel}] artifacts: - name: chain-0 from: \u0026#34;{{tasks.Echo.outputs.artifacts.chain}}\u0026#34; - name: chain-1 from: \u0026#34;{{tasks.Foxtrot.outputs.artifacts.chain}}\u0026#34; EoF This workflow uses a Directed Acyclic Graph (DAG) to explicitly define job dependencies. Each job in the workflow calls a whalesay template and passes a parameter with a unique name. Some jobs call a whalesay-reduce template which accepts multiple artifacts and combines them into a single artifact.\nEach job in the workflow pulls the artifact(s) and lists them in the \u0026ldquo;Chain\u0026rdquo;, then calls whalesay for the current job. Each job will then have a list of the previous job dependency chain (list of all jobs that had to complete before current job could run).\nRun the workflow.\nargo -n argo submit --watch ~/environment/batch_policy/teardrop.yaml Name: teardrop-vqbmb Namespace: argo ServiceAccount: default Status: Succeeded Conditions: Completed True Created: Tue Jul 07 20:32:12 \u0026#43;0000 (42 seconds ago) Started: Tue Jul 07 20:32:12 \u0026#43;0000 (42 seconds ago) Finished: Tue Jul 07 20:32:54 \u0026#43;0000 (now) Duration: 42 seconds ResourcesDuration: 32s*(1 cpu),32s*(100Mi memory) STEP TEMPLATE PODNAME DURATION MESSAGE ‚úî teardrop-vqbmb teardrop ‚îú-‚úî create-chain create-chain teardrop-vqbmb-1083106731 3s ‚îú-‚úî Alpha whalesay teardrop-vqbmb-2236987393 3s ‚îú-‚úî Bravo whalesay teardrop-vqbmb-1872757121 4s ‚îú-‚úî Charlie whalesay teardrop-vqbmb-2266260663 4s ‚îú-‚úî Delta whalesay teardrop-vqbmb-2802530727 18s ‚îú-‚úî Echo whalesay-reduce teardrop-vqbmb-2599957478 4s ‚îú-‚úî Foxtrot whalesay teardrop-vqbmb-1298400165 4s ‚îú-‚úî Hotel whalesay-reduce teardrop-vqbmb-3381869223 8s ‚îî-‚úî Golf whalesay-reduce teardrop-vqbmb-1766004759 8s  Continue to the Argo Dashboard to explore this model further.\n"
},
{
	"uri": "/010_introduction/basics/concepts_objects_details_2/",
	"title": "K8s Objects Detail (2/2)",
	"tags": [],
	"description": "",
	"content": "ReplicaSet  Ensures a defined number of pods are always running  Job  Ensures a pod properly runs to completion  Service  Maps a fixed IP address to a logical group of pods  Label  Key/Value pairs used for association and filtering  "
},
{
	"uri": "/beginner/090_rbac/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Once you have completed this chapter, you can cleanup the files and resources you created by issuing the following commands:\nunset AWS_SECRET_ACCESS_KEY unset AWS_ACCESS_KEY_ID kubectl delete namespace rbac-test rm rbacuser_creds.sh rm rbacuser-role.yaml rm rbacuser-role-binding.yaml aws iam delete-access-key --user-name=rbac-user --access-key-id=$(jq -r .AccessKey.AccessKeyId /tmp/create_output.json) aws iam delete-user --user-name rbac-user rm /tmp/create_output.json Next remove the rbac-user mapping from the existing configMap by editing the existing aws-auth.yaml file:\ndata: mapUsers: | [] And apply the ConfigMap and delete the aws-auth.yaml file\nkubectl apply -f aws-auth.yaml rm aws-auth.yaml "
},
{
	"uri": "/beginner/091_iam-groups/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Once you have completed this chapter, you can cleanup the files and resources you created by issuing the following commands:\nunset KUBECONFIG kubectl delete namespace development integration kubectl delete pod nginx-admin eksctl delete iamidentitymapping --cluster eksworkshop-eksctl --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sAdmin eksctl delete iamidentitymapping --cluster eksworkshop-eksctl --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sDev eksctl delete iamidentitymapping --cluster eksworkshop-eksctl --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sInteg aws iam remove-user-from-group --group-name k8sAdmin --user-name PaulAdmin aws iam remove-user-from-group --group-name k8sDev --user-name JeanDev aws iam remove-user-from-group --group-name k8sInteg --user-name PierreInteg aws iam delete-group-policy --group-name k8sAdmin --policy-name k8sAdmin-policy aws iam delete-group-policy --group-name k8sDev --policy-name k8sDev-policy aws iam delete-group-policy --group-name k8sInteg --policy-name k8sInteg-policy aws iam delete-group --group-name k8sAdmin aws iam delete-group --group-name k8sDev aws iam delete-group --group-name k8sInteg aws iam delete-access-key --user-name PaulAdmin --access-key-id=$(jq -r .AccessKey.AccessKeyId /tmp/PaulAdmin.json) aws iam delete-access-key --user-name JeanDev --access-key-id=$(jq -r .AccessKey.AccessKeyId /tmp/JeanDev.json) aws iam delete-access-key --user-name PierreInteg --access-key-id=$(jq -r .AccessKey.AccessKeyId /tmp/PierreInteg.json) aws iam delete-user --user-name PaulAdmin aws iam delete-user --user-name JeanDev aws iam delete-user --user-name PierreInteg aws iam delete-role --role-name k8sAdmin aws iam delete-role --role-name k8sDev aws iam delete-role --role-name k8sInteg rm /tmp/*.json rm /tmp/kubeconfig* # reset aws credentials and config files rm ~/.aws/{config,credentials} aws configure set default.region ${AWS_REGION} "
},
{
	"uri": "/intermediate/330_app_mesh/install_app_mesh_controller/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Congratulations on installing the AWS App Mesh Controller for Kubernetes in your cluster!\nNext, we\u0026rsquo;ll enable observability, analytics, and routing functionality into our DJ App by porting it to run within App Mesh, and we\u0026rsquo;ll demonstrate how to simplify various types of deployments.\n"
},
{
	"uri": "/beginner/060_helm/helm_nginx/",
	"title": "Deploy nginx With Helm",
	"tags": [],
	"description": "",
	"content": "Deploy nginx With Helm In this Chapter, we will dig deeper with Helm and demonstrate how to install the nginx web server via the following steps:\n Update the Chart Repository   Search Chart Repositories   Add the Bitnami Repository   Install bitnami/nginx   Clean Up   "
},
{
	"uri": "/intermediate/330_app_mesh/port_to_app_mesh/trigger_sidecar_injection/",
	"title": "Sidecar Injection",
	"tags": [],
	"description": "",
	"content": "Recall that to join the mesh, each pod will need an Envoy proxy sidecar container. To stream configuration, those proxies will need some minimal permissions in IAM. We can use IRSA here again, granting only the required permissions to our application namespace.\nNote you can scope the policy actions to only specific resources within your namespace, if you wish. For the purposes of our demo, we\u0026rsquo;ll use the default policy and apply to all resources in the namespace.\n# Download the IAM policy document for the Envoy proxies curl -o envoy-iam-policy.json https://raw.githubusercontent.com/aws/aws-app-mesh-controller-for-k8s/master/config/iam/envoy-iam-policy.json # Create an IAM policy for the proxies from the policy document aws iam create-policy \\ --policy-name AWSAppMeshEnvoySidecarIAMPolicy \\ --policy-document file://envoy-iam-policy.json # Create an IAM role and service account for the application namespace eksctl create iamserviceaccount \\ --cluster eksworkshop-eksctl \\ --namespace prod \\ --name prod-proxies \\ --attach-policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSAppMeshEnvoySidecarIAMPolicy \\ --override-existing-serviceaccounts \\ --approve Now that\u0026rsquo;s sorted, you can start the proxies. We have enabled automatic sidecar injection on the prod namespace, but this was done after initial pod creation. Currently, your pods each have one container running.\nkubectl get pods -n prod NAME READY STATUS RESTARTS AGE NAME READY STATUS RESTARTS AGE dj-6bf5fb7f45-qkhv7 1/1 Running 0 7m21s jazz-v1-6f688dcbf9-djb9h 1/1 Running 0 7m21s metal-v1-566756fbd6-8k2rs 1/1 Running 0 7m21s  To inject sidecar proxies for these pods, simply restart the deployments. The controller will handle the rest, and will inject sidecar proxies in any new pods as well.\nkubectl -n prod rollout restart deployment dj jazz-v1 metal-v1 deployment.apps/dj restarted deployment.apps/jazz-v1 restarted deployment.apps/metal-v1 restarted  You should now see 2 containers in each pod. It might take a few seconds for the new configuration to settle.\nkubectl -n prod get pods NAME READY STATUS RESTARTS AGE dj-6544487b5f-7glpg 2/2 Running 0 14s jazz-v1-9bfb9b78c-qjmvj 2/2 Running 0 14s metal-v1-9f78fb8c8-67mqc 2/2 Running 0 14s  Now you can see that 2 containers are running in each pod, verify that they are the application service and the Envoy proxy. Examine the pod and confirm these are the containers running within it.\nexport DJ_POD_NAME=$(kubectl get pods -n prod -l app=dj -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n prod get pods $DJ_POD_NAME -o jsonpath=\u0026#39;{.spec.containers[*].name}\u0026#39; dj envoy  Here you see both the application service as well as the sidecar proxy container. Any new pods created in this namespace will have the proxy injected automatically.\n"
},
{
	"uri": "/intermediate/330_app_mesh/deploy_dj_app/test_the_app/",
	"title": "Test DJ App",
	"tags": [],
	"description": "",
	"content": "To test what we\u0026rsquo;ve just created, we will:\n Exec into the dj pod curl out to the jazz-v1 and metal-v1 backends  First we will exec into the dj pod\nexport DJ_POD_NAME=$(kubectl get pods -n prod -l app=dj -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl exec -n prod -it ${DJ_POD_NAME} bash You will see a prompt from within the dj container.\nroot@dj-5b445fbdf4-8xkwp:/usr/src/app#  Now from the dj container, we\u0026rsquo;ll issue a curl request to the jazz-v1 backend service:\ncurl -s jazz-v1:9080 | json_pp Output should be similar to: [ \u0026#34;Astrud Gilberto\u0026#34;, \u0026#34;Miles Davis\u0026#34; ]  Try it again, but issue the command to the metal-v1 backend:\ncurl -s metal-v1:9080 | json_pp You should get a list of heavy metal bands back:\n[ \u0026#34;Megadeth\u0026#34;, \u0026#34;Judas Priest\u0026#34; ]  When you\u0026rsquo;re done exploring this vast world of music, hit CTRL-D, or type exit to quit the container\u0026rsquo;s shell.\n"
},
{
	"uri": "/intermediate/265_spinnaker_eks/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Delete Spinnaker artifacts Namespace deletion may take few minutes, please wait till the process completes.\n for i in $(kubectl get crd | grep spinnaker | cut -d\u0026#34; \u0026#34; -f1) ; do kubectl delete crd $i done kubectl delete ns spinnaker-operator kubectl delete ns spinnaker kubectl delete ns detail (Optional) Create Old Nodegroup In case you have existing workloads to evit to this nodegroup before we delete the nodegroup created for this chapter Nodegroup creation will take few minutes.\n eksctl create nodegroup --cluster=eksworkshop-eksctl --name=nodegroup --nodes=3 --node-type=t3.small --enable-ssm --managed Delete Nodegroup eksctl delete nodegroup --cluster=eksworkshop-eksctl --name=spinnaker "
},
{
	"uri": "/intermediate/200_migrate_to_eks/cleanup/",
	"title": "Cleanup resources",
	"tags": [],
	"description": "",
	"content": "Delete EKS resources\nkubectl delete svc/counter svc/postgres deploy/counter statefulset/postgres Delete ALB\naws elbv2 delete-listener \\  --listener-arn $ALB_LISTENER aws elbv2 delete-load-balancer \\  --load-balancer-arn $ALB_ARN aws elbv2 delete-target-group \\  --target-group-arn $TG_ARN Delete VPC peering\naws ec2 delete-vpc-peering-connection \\  --vpc-peering-connection-id $PEERING_ID Delete EKS cluster\neksctl delete cluster --name $CLUSTER "
},
{
	"uri": "/beginner/070_healthchecks/",
	"title": "Health Checks",
	"tags": ["beginner", "CON206"],
	"description": "",
	"content": "Health Checks   By default, Kubernetes will restart a container if it crashes for any reason. It uses Liveness and Readiness probes which can be configured for running a robust application by identifying the healthy containers to send traffic to and restarting the ones when required.\nIn this section, we will understand how liveness and readiness probes are defined and test the same against different states of a pod. Below is the high level description of how these probes work.\nLiveness probes are used in Kubernetes to know when a pod is alive or dead. A pod can be in a dead state for a variety of reasons; Kubernetes will kill and recreate the pod when a liveness probe does not pass.\nReadiness probes are used in Kubernetes to know when a pod is ready to serve traffic. Only when the readiness probe passes will a pod receive traffic from the service; if a readiness probe fails traffic will not be sent to the pod.\nWe will review some examples in this module to understand different options for configuring liveness and readiness probes.\n"
},
{
	"uri": "/beginner/080_scaling/",
	"title": "Autoscaling our Applications and Clusters",
	"tags": ["beginner", "CON205"],
	"description": "",
	"content": "Implement AutoScaling with HPA and CA   In this Chapter, we will show patterns for scaling your worker nodes and applications deployments automatically.\nAutomatic scaling in K8s comes in two forms:\n  Horizontal Pod Autoscaler (HPA) scales the pods in a deployment or replica set. It is implemented as a K8s API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).\n  Cluster Autoscaler (CA) a component that automatically adjusts the size of a Kubernetes Cluster so that all pods have a place to run and there are no unneeded nodes.\n  "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/port_to_app_mesh/testing_v1/",
	"title": "Testing the Application",
	"tags": [],
	"description": "",
	"content": "Now it\u0026rsquo;s time to test. To test if our ported Product Catalog App is working as expected, we\u0026rsquo;ll first exec into the frontend-node container.\nexport FE_POD_NAME=$(kubectl get pods -n prodcatalog-ns -l app=frontend-node -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n prodcatalog-ns exec -it ${FE_POD_NAME} -c frontend-node bash You will see a prompt from within the frontend-node container.\nroot@frontend-node-9d46cb55-XXX:/usr/src/app#  Test the confiuration by issuing a curl request to the virtual service prodcatalog on port 5000, simulating what would happen if code running in the same pod made a request to the prodcatalog backend:\ncurl -v http://prodcatalog.prodcatalog-ns.svc.cluster.local:5000/products/ Output should be similar to below. You can see that the request to backend service prodcatalog is going via envoy proxy.\n* Trying 10.100.163.192... * TCP_NODELAY set * Connected to prodcatalog.prodcatalog-ns.svc.cluster.local (10.100.xx.yyy) port 5000 (#0) \u0026gt; GET /products/ HTTP/1.1 \u0026gt; Host: prodcatalog.prodcatalog-ns.svc.cluster.local:5000 \u0026gt; User-Agent: curl/7.52.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: application/json \u0026lt; content-length: 51 \u0026lt; x-amzn-trace-id: Root=1-600925c6-e2c7bec92b824ddc9969d1b5 \u0026lt; access-control-allow-origin: * \u0026lt; server: envoy \u0026lt; date: Thu, 21 Jan 2021 06:57:10 GMT \u0026lt; x-envoy-upstream-service-time: 19 \u0026lt; { \u0026#34;products\u0026#34;: {}, \u0026#34;details\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;vendors\u0026#34;: [ \u0026#34;ABC.com\u0026#34; ] } } * Curl_http_done: called premature == 0 * Connection #0 to host prodcatalog.prodcatalog-ns.svc.cluster.local left intact  Exit from the frontend-node exec bash. Now, To test the connectivity from Fargate service prodcatalog to Nodegroup service proddetail, we\u0026rsquo;ll first exec into the prodcatalog container.\nexport BE_POD_NAME=$(kubectl get pods -n prodcatalog-ns -l app=prodcatalog -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n prodcatalog-ns exec -it ${BE_POD_NAME} -c prodcatalog bash You will see a prompt from within the prodcatalog container.\nroot@prodcatalog-98f7c5f87-xxxxx:/usr/src/app#  Test the confiuration by issuing a curl request to the virtual service proddetail on port 3000, simulating what would happen if code running in the same pod made a request to the proddetail backend:\ncurl -v http://proddetail.prodcatalog-ns.svc.cluster.local:3000/catalogDetail You should see the below response. You can see that the request to backend service proddetail-v1 is going via envoy proxy. You can exit now from the prodcatalog exec bash.\n..... ..... \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: application/json \u0026lt; content-length: 51 \u0026lt; x-amzn-trace-id: Root=1-600925c6-e2c7bec92b824ddc9969d1b5 \u0026lt; access-control-allow-origin: * \u0026lt; server: envoy \u0026lt; date: Thu, 21 Jan 2021 06:57:10 GMT \u0026lt; x-envoy-upstream-service-time: 19 .... .... {\u0026#34;version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;vendors\u0026#34;:[\u0026#34;ABC.com\u0026#34;]}  Congrats! You\u0026rsquo;ve migrated the initial architecture to provide the same functionality. Now lets expose the frontend-node to external users to access the UI using App Mesh Virtual Gateway.\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/virtual_gateway_setup/testing_virtualgateway/",
	"title": "Testing Virtual Gateway",
	"tags": [],
	"description": "",
	"content": "Now it\u0026rsquo;s time to test. Get the Loadbalancer endpoint that Virtual Gateway is exposed.\nIt takes 3 to 5 minutes to set up the Load Balancer. You can go to Console and Go to Load Balancer and check if the state is Active. Do not proceed to next step until Load Balancer is Active.\n export LB_NAME=$(kubectl get svc ingress-gw -n prodcatalog-ns -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[*].hostname}\u0026#34;) curl -v --silent ${LB_NAME} | grep x-envoy echo $LB_NAME Check if the request to the Ingress Gateway is going from envoy by curl to the above Loadbalancer url \u0026gt; GET / HTTP/1.1 \u0026gt; Host: db13be460b8648c4bXXXf.elb.us-west-2.amazonaws.com \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: text/html; charset=utf-8 \u0026lt; content-length: 3783 \u0026lt; x-amzn-trace-id: Root=1-5ff4c10e-71cca9a19486406b80eaa475 \u0026lt; server: envoy \u0026lt; date: Tue, 05 Jan 2021 19:42:06 GMT \u0026lt; x-envoy-upstream-service-time: 3 \u0026lt; server: envoy { [1079 bytes data] * Connection #0 to host db13be460b8648c4bXXXf.elb.us-west-2.amazonaws.com left intact workshop:~/environment $ echo $LB_NAME db13be460b8648c4bXXXf.elb.us-west-2.amazonaws.com  Copy paste the above Loadbalancer endpoint in your browser and you should see the frontend application loaded as below. Add Product e.g. Table with ID as 1 to Product Catalog and click Add button. You should see the new product Table added in the Product Catalog table. You can also see the Catalog Detail about Vendor information has been fetched from proddetail backend service. Congratulations on exposing the Product Catalog Application via App Mesh Virtual Gateway!\nLet‚Äôs try out Canary Deployment by adding a new version of the proddetail-v2 and adding some new configuration to our virtual routers to shift traffic between the different versions of Catalog Detail service proddetail-v1 and proddetail-v2.\n"
},
{
	"uri": "/advanced/420_kubeflow/pipelines/",
	"title": "Kubeflow pipeline",
	"tags": [],
	"description": "",
	"content": "Kubeflow Pipelines   Kubeflow Pipeline is one the core components of the toolkit and gets deployed automatically when you install Kubeflow. Kubeflow Pipelines consists of:\n A user interface (UI) for managing and tracking experiments, jobs, and runs. An engine for scheduling multi-step ML workflows. An SDK for defining and manipulating pipelines and components. Notebooks for interacting with the system using the SDK.  Amazon Sagemaker is a managed service that enables data scientists and developers to quickly and easily build, train, and deploy machine learning models.\nFor this exercise, we will build Mnist classification pipeline using Amazon Sagemaker.\nAssign IAM permissions In order to run this exercise, we need three levels of IAM permissions. 1) create Kubernetes secrets aws-secret with Sagemaker policies. We\u0026rsquo;ll use this during pipeline execution to make calls to AWS API\u0026rsquo;s. 2) create an IAM execution role for Sagemaker so that the job can assume this role in order to perform Sagemaker actions. Typically in a production environment, you would assign fine-grained permissions depending on the nature of actions you take and leverage tools like IAM Role for Service Account for securing access to AWS resources but for simplicity we will assign AmazonSageMakerFullAccess IAM policy to both. You can read more about granular policies here. 3) Assign sagemaker:InvokeEndpoint permission to Worker node IAM role so that we can use this to make predictions once Sagemaker creates the endpoint\nRun this command from your Cloud9 to create these IAM permissions\naws iam create-user --user-name sagemakeruser aws iam attach-user-policy --user-name sagemakeruser --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess aws iam create-access-key --user-name sagemakeruser \u0026gt; /tmp/create_output.json Next, record the new user\u0026rsquo;s credentials into environment variables:\nexport AWS_ACCESS_KEY_ID_VALUE=$(jq -j .AccessKey.AccessKeyId /tmp/create_output.json | base64) export AWS_SECRET_ACCESS_KEY_VALUE=$(jq -j .AccessKey.SecretAccessKey /tmp/create_output.json | base64) Apply to EKS cluster:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aws-secret namespace: kubeflow type: Opaque data: AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID_VALUE AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY_VALUE EOF Run this command to create Sagemaker execution role\nTRUST=\u0026quot;{ \\\u0026quot;Version\\\u0026quot;: \\\u0026quot;2012-10-17\\\u0026quot;, \\\u0026quot;Statement\\\u0026quot;: [ { \\\u0026quot;Effect\\\u0026quot;: \\\u0026quot;Allow\\\u0026quot;, \\\u0026quot;Principal\\\u0026quot;: { \\\u0026quot;Service\\\u0026quot;: \\\u0026quot;sagemaker.amazonaws.com\\\u0026quot; }, \\\u0026quot;Action\\\u0026quot;: \\\u0026quot;sts:AssumeRole\\\u0026quot; } ] }\u0026quot; aws iam create-role --role-name eksworkshop-sagemaker-kfp-role --assume-role-policy-document \u0026quot;$TRUST\u0026quot; aws iam attach-role-policy --role-name eksworkshop-sagemaker-kfp-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess aws iam attach-role-policy --role-name eksworkshop-sagemaker-kfp-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess aws iam get-role --role-name eksworkshop-sagemaker-kfp-role --output text --query 'Role.Arn' At the end of the script, you will get the arn of IAM role generated. Make a note of this role as you will need it during pipeline creation step\nHere is an example of the output TeamRole:~/environment $ aws iam get-role --role-name eksworkshop-sagemaker-kfp-role --output text --query \u0026#39;Role.Arn\u0026#39; arn:aws:iam::371348455981:role/eksworkshop-sagemaker-kfp-role  Lastly, let\u0026rsquo;s assign sagemaker:InvokeEndpoint permission to Worker node IAM role\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/sagemaker-invoke.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;sagemaker:InvokeEndpoint\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name sagemaker-invoke-for-worker --policy-document file://~/environment/sagemaker-invoke.json Run Sagemaker pipeline notebook Go to your eks-kubeflow-workshop notebook server and browse for Sagemaker pipeline notebook (eks-workshop-notebook/notebooks/05_Kubeflow_Pipeline/05_04_Pipeline_SageMaker.ipynb). If you haven\u0026rsquo;t installed notebook server, review fairing chapter and finish the clone the repo instructions.\nOpen Sagemaker pipeline notebook\nStarting from here, its important to read notebook instructions carefully. The info provided in the workshop is lightweight and you can use it to ensure desired result. You can complete the exercise by staying in the notebook\n Review the introduction and go through the prerequisites for 1) creating an S3 bucket and 2) copying the pipeline data. You can skip step 3 because you have already created Kubernetes secrets and Sagemaker execution role earlier. Continue with step 4 by installing Kubeflow pipeline SDK\nOnce all the prerequisites steps are completed, let\u0026rsquo;s go through building our pipeline.\nRun step 1 to load Kubeflow pipeline SDK. Once that is complete, run step 2 to load sagemaker components\nBefore you run step 3 - create pipeline, replace the value of SAGEMAKER_ROLE_ARN with Sagemaker execution role that we created during Assign IAM permissions After this, go and run next two steps to compile and deploy your pipelines\nYou will receive 2 links, one is for experiments and other is pipeline run. Click on here next to Experiment link. You can click on View pipeline to view details of the pipeline You can click on Experiments, All runs, mnist-classification-pipeline to examine each steps in the pipeline. Click on sagemaker training job and then click logs tab to see details After few minutes you\u0026rsquo;ll see the training job completes and then creates a model. After this step, batch transformation runs and then finally model is deployed using Sagemaker inference. Make a note of Sagemaker endpoint so that we can run prediction to validate our model Now, let\u0026rsquo;s run prediction against this endpoint. You can safely ignore the permissions note because we have already taken care of this earlier. Install boto3 and then change the endpoint name to the name you received in previous step You\u0026rsquo;ll receive predictions as depicted above\n"
},
{
	"uri": "/advanced/410_batch/dashboard/",
	"title": "Argo Dashboard",
	"tags": [],
	"description": "",
	"content": "Argo Dashboard Since V2.5, Argo UI has been replace by Argo Server. The new UI is not read-only ‚Äî it also comes with the ability to create and update data directly in your browser. Click here for more information\n Access the Argo Server To access the dashboard we will expose the argo-server service by adding a LoadBalancer.\nkubectl -n argo patch svc argo-server \\  -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39;  It will take several minutes for the ELB to become healthy and start passing traffic to the pods.\n To access the Argo Dashboard, click on the URL generated by the these commands:\nARGO_URL=$(kubectl -n argo get svc argo-server --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\u0026#34;) echo ARGO DASHBOARD: http://${ARGO_URL}:2746 You will see the teardrop workflow from Advanced Batch Workflow. Click on it to see a visualization of the workflow.\nThe workflow should relatively look like a teardrop, and provide a live status for each job. Click on Hotel to see a summary of the Hotel job.\nThis details basic information about the job, and includes a link to the Logs. The Hotel job logs list the job dependency chain and the current whalesay, and should look similar to:\nExplore the other jobs in the workflow to see each job\u0026rsquo;s status and logs.\n"
},
{
	"uri": "/010_introduction/architecture/",
	"title": "Kubernetes Architecture",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n Architectural Overview   Control Plane   Data Plane   Kubernetes Cluster Setup   "
},
{
	"uri": "/intermediate/330_app_mesh/deploy_dj_app/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Congratulations on deploying the initial DJ App architecture!\nBefore we create the App Mesh-enabled versions of DJ App, we\u0026rsquo;ll first install the AWS App Mesh Controller for Kubernetes into our cluster.\n"
},
{
	"uri": "/beginner/060_helm/helm_micro/",
	"title": "Deploy Example Microservices Using Helm",
	"tags": [],
	"description": "",
	"content": "Deploy Example Microservices Using Helm In this chapter, we will demonstrate how to deploy microservices using a custom Helm Chart, instead of doing everything manually using kubectl.\nFor detailed information on working with chart templates, refer to the Helm docs.\n"
},
{
	"uri": "/intermediate/330_app_mesh/port_to_app_mesh/testing_v1/",
	"title": "Testing the Application",
	"tags": [],
	"description": "",
	"content": "Now it\u0026rsquo;s time to test. Calls can be made to metal or jazz from within the dj pod and they are routed to either the metal-v1 or jazz-v1 endpoints, respectively.\nTo test if our ported DJ App is working as expected, we\u0026rsquo;ll first exec into the dj container.\nexport DJ_POD_NAME=$(kubectl get pods -n prod -l app=dj -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n prod exec -it ${DJ_POD_NAME} -c dj bash You will see a prompt from within the dj container.\nroot@dj-5b445fbdf4-8xkwp:/usr/src/app#  Test the confiuration by issuing a curl request to the virtual service jazz on port 9080, simulating what would happen if code running in the same pod made a request to the jazz backend:\ncurl -s jazz.prod.svc.cluster.local:9080 | json_pp Output should be similar to:\n[ \u0026#34;Astrud Gilberto\u0026#34;, \u0026#34;Miles Davis\u0026#34; ]  Now test the metal service:\ncurl -s metal.prod.svc.cluster.local:9080 | json_pp You should get a list of heavy metal bands back:\n[ \u0026#34;Megadeth\u0026#34;, \u0026#34;Judas Priest\u0026#34; ]  When you\u0026rsquo;re done exploring this vast, now mesh-enabled world of music, hit CTRL-D, or type exit to exit the container\u0026rsquo;s shell\nCongrats! You\u0026rsquo;ve migrated the initial architecture to provide the same functionality, but now with the wide array of AWS App Mesh features at your disposal.\nLet\u0026rsquo;s try out one of those features by adding a new version of the metal and jazz backend services, and adding some new configuration to our virtual routers to shift traffic between the different versions.\n"
},
{
	"uri": "/intermediate/246_monitoring_amp_amg/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the Monitoring using Amazon Managed Service for Prometheus / Grafana module.\nPlease cleanup your environment before going to a new one:\nCleanup Helm environment:\nhelm uninstall prometheus-for-amp -n prometheus kubectl delete ns prometheus Delete IAM ressources:\nSERVICE_ACCOUNT_IAM_ROLE=EKS-AMP-ServiceAccount-Role SERVICE_ACCOUNT_IAM_POLICY=AWSManagedPrometheusWriteAccessPolicy AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026quot;Account\u0026quot; --output text) SERVICE_ACCOUNT_IAM_POLICY_ARN=arn:aws:iam::$AWS_ACCOUNT_ID:policy/$SERVICE_ACCOUNT_IAM_POLICY aws iam detach-role-policy --role-name $SERVICE_ACCOUNT_IAM_ROLE --policy-arn $SERVICE_ACCOUNT_IAM_POLICY_ARN aws iam delete-role --role-name $SERVICE_ACCOUNT_IAM_ROLE Cleanup AMP Workspace:\naws amp delete-workspace --workspace-id $WORKSPACE_ID "
},
{
	"uri": "/beginner/115_sg-per-pod/90_cleanup/",
	"title": "Cleanup",
	"tags": ["beginner"],
	"description": "",
	"content": "export VPC_ID=$(aws eks describe-cluster \\  --name eksworkshop-eksctl \\  --query \u0026#34;cluster.resourcesVpcConfig.vpcId\u0026#34; \\  --output text) export RDS_SG=$(aws ec2 describe-security-groups \\  --filters Name=group-name,Values=RDS_SG Name=vpc-id,Values=${VPC_ID} \\  --query \u0026#34;SecurityGroups[0].GroupId\u0026#34; --output text) export POD_SG=$(aws ec2 describe-security-groups \\  --filters Name=group-name,Values=POD_SG Name=vpc-id,Values=${VPC_ID} \\  --query \u0026#34;SecurityGroups[0].GroupId\u0026#34; --output text) export C9_IP=$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4) export NODE_GROUP_SG=$(aws ec2 describe-security-groups \\  --filters Name=tag:Name,Values=eks-cluster-sg-eksworkshop-eksctl-* Name=vpc-id,Values=${VPC_ID} \\  --query \u0026#34;SecurityGroups[0].GroupId\u0026#34; \\  --output text) # uninstall the RPM package sudo yum remove -y $(sudo yum list installed | grep amzn2extra-postgresql12 | awk \u0026#39;{ print $1}\u0026#39;) # delete database aws rds delete-db-instance \\  --db-instance-identifier rds-eksworkshop \\  --delete-automated-backups \\  --skip-final-snapshot # delete kubernetes element kubectl -n sg-per-pod delete -f ~/environment/sg-per-pod/green-pod.yaml kubectl -n sg-per-pod delete -f ~/environment/sg-per-pod/red-pod.yaml kubectl -n sg-per-pod delete -f ~/environment/sg-per-pod/sg-policy.yaml kubectl -n sg-per-pod delete secret rds # delete the namespace kubectl delete ns sg-per-pod # disable ENI trunking kubectl -n kube-system set env daemonset aws-node ENABLE_POD_ENI=false kubectl -n kube-system rollout status ds aws-node # detach the IAM policy aws iam detach-role-policy \\  --policy-arn arn:aws:iam::aws:policy/AmazonEKSVPCResourceController \\  --role-name ${ROLE_NAME} # remove the security groups rules aws ec2 revoke-security-group-ingress \\  --group-id ${RDS_SG} \\  --protocol tcp \\  --port 5432 \\  --source-group ${POD_SG} aws ec2 revoke-security-group-ingress \\  --group-id ${RDS_SG} \\  --protocol tcp \\  --port 5432 \\  --cidr ${C9_IP}/32 aws ec2 revoke-security-group-ingress \\  --group-id ${NODE_GROUP_SG} \\  --protocol tcp \\  --port 53 \\  --source-group ${POD_SG} aws ec2 revoke-security-group-ingress \\  --group-id ${NODE_GROUP_SG} \\  --protocol udp \\  --port 53 \\  --source-group ${POD_SG} # delete POD security group aws ec2 delete-security-group \\  --group-id ${POD_SG} Verify the RDS instance has been deleted.\naws rds describe-db-instances \\  --db-instance-identifier rds-eksworkshop \\  --query \u0026#34;DBInstances[].DBInstanceStatus\u0026#34; \\  --output text Expected output\nAn error occurred (DBInstanceNotFound) when calling the DescribeDBInstances operation: DBInstance rds-eksworkshop not found.  We can now safely delete the DB security group and the DB subnet group.\n# delete RDS SG aws ec2 delete-security-group \\  --group-id ${RDS_SG} # delete DB subnet group aws rds delete-db-subnet-group \\  --db-subnet-group-name rds-eksworkshop Finally, we will delete the EKS Nodegroup\n# delete the nodegroup eksctl delete nodegroup -f ${HOME}/environment/sg-per-pod/nodegroup-sec-group.yaml --approve # remove the trunk label kubectl label node --all \u0026#39;vpc.amazonaws.com/has-trunk-attached\u0026#39;- cd ~/environment rm -rf sg-per-pod "
},
{
	"uri": "/beginner/040_dashboard/cleanup/",
	"title": "Cleanup",
	"tags": ["beginner", "CON203"],
	"description": "",
	"content": "Stop the proxy and delete the dashboard deployment\n# kill proxy pkill -f \u0026#39;kubectl proxy --port=8080\u0026#39; # delete dashboard kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/${DASHBOARD_VERSION}/aio/deploy/recommended.yaml unset DASHBOARD_VERSION "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/integrate_observability/",
	"title": "Observability",
	"tags": [],
	"description": "",
	"content": "In this chapter, we will dive into key operational data and tools such as CloudWatch Container Insights, Cloudwatch Logs and Prometheus that you can leverage within the AWS environment to supercharge your ability to monitor metrics, collect logs, trigger alerts, and trace distributed services.\nObservability includes the use of various signals (metrics, traces, logs) to monitor the overall health of your application. And in this section, we\u0026rsquo;ll use the following data and tools to get the end to end vissibility into our Product Catalog Application deployed in EKS.\n Container Insights Cloudwatch Container logs Prometheus App Mesh Metrics Fargate Container logs AWS X-Ray Tracing  Make sure that you have completed the Observability Setup chapter before proceeding to the next section.\n "
},
{
	"uri": "/advanced/420_kubeflow/distributed/",
	"title": "Kubeflow Distributed Training",
	"tags": [],
	"description": "",
	"content": "Distributed Training using tf-operator and pytorch-operator TFJob is a Kubernetes custom resource that you can use to run TensorFlow training jobs on Kubernetes. The Kubeflow implementation of TFJob is in tf-operator. Similarly, you can create PyTorch Job by defining a PyTorchJob config file and pytorch-operator will help create PyTorch job, monitor and keep track of the job.\nGo to your eks-kubeflow-workshop notebook server and browse for distributed training notebooks (eks-workshop-notebook/notebooks/03_Distributed_Training/03_01_Distributed_Training_ParameterServer.ipynb). If you haven‚Äôt installed notebook server, review fairing chapter and finish the clone the repo instructions.\nYou can go over basic concepts of distributed training. In addition, we prepare distributed-tensorflow-job.yaml and distributed-pytorch-job.yaml for you to run distributed training jobs on EKS. You can follow guidance to check job specs, create the jobs and monitor the jobs.\nStarting from here, its important to read notebook instructions carefully. The info provided in the workshop is lightweight and you can use it to ensure desired result. You can complete the exercise by staying in the notebook\n "
},
{
	"uri": "/advanced/420_kubeflow/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Uninstall Kubeflow Delete IAM users, S3 bucket and Kubernetes secret\n# delete s3user aws iam detach-user-policy --user-name s3user --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess aws iam delete-access-key --access-key-id `echo $AWS_ACCESS_KEY_ID_VALUE | base64 --decode` --user-name s3user aws iam delete-user --user-name s3user # delete sagemakeruser aws iam detach-user-policy --user-name sagemakeruser --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess aws iam delete-access-key --access-key-id `echo $AWS_ACCESS_KEY_ID_VALUE | base64 --decode` --user-name sagemakeruser aws iam delete-user --user-name sagemakeruser # delete S3 bucket aws s3 rb s3://$S3_BUCKET --force --region $AWS_REGION # delete aws-secret kubectl delete secret/aws-secret kubectl delete secret/aws-secret -n kubeflow Run these commands to uninstall Kubeflow from your EKS cluster\ncd ${KF_DIR} kfctl delete -V -f ${CONFIG_FILE} Scale the cluster back to previous size\neksctl scale nodegroup --cluster eksworkshop-eksctl --name $NODEGROUP_NAME --nodes 3 "
},
{
	"uri": "/advanced/410_batch/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Cleanup Delete all workflows argo -n argo delete --all Remove Artifact Repository Bucket aws s3 rb s3://batch-artifact-repository-${ACCOUNT_ID}/ --force Undeploy Argo kubectl delete -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/${ARGO_VERSION}/manifests/install.yaml kubectl delete namespace argo Cleanup Kubernetes Job kubectl delete job/whalesay delete the inline policy aws iam delete-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker delete the folder and all the files in it rm -rf ~/environment/batch_policy "
},
{
	"uri": "/010_introduction/architecture/architecture_control_and_data_overview/",
	"title": "Architectural Overview",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 kubectl--api controller--api scheduler--api api--kubelet1 api--etcd kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;  "
},
{
	"uri": "/beginner/090_rbac/",
	"title": "Intro to RBAC",
	"tags": ["beginner", "CON205"],
	"description": "",
	"content": "Intro to RBAC   In this chapter, we\u0026rsquo;ll learn about how role based access control (RBAC) works in kubernetes.\n"
},
{
	"uri": "/010_introduction/eks/",
	"title": "Amazon EKS",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n EKS Cluster Creation Workflow   What happens when you create your EKS cluster   EKS Architecture for Control plane and Worker node communication   High Level   Amazon EKS!   "
},
{
	"uri": "/beginner/050_deploy/cleanup/",
	"title": "Cleanup the applications",
	"tags": [],
	"description": "",
	"content": "To delete the resources created by the applications, we should delete the application deployments:\nUndeploy the applications:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml "
},
{
	"uri": "/intermediate/330_app_mesh/port_to_app_mesh/canary_testing/",
	"title": "Canary Testing with a v2",
	"tags": [],
	"description": "",
	"content": "A canary release is a method of slowly exposing a new version of software. The theory behind it is that by serving the new version of the software initially to say, 5% of requests, if there is a problem, the problem only impacts a very small percentage of users before its discovered and rolled back.\nSo now back to our DJ App scenario, metal-v2 and jazz-v2 services are out, and they now include the city each artist is from in the response.\nLet\u0026rsquo;s see how we can release these new versions in a canary fashion using AWS App Mesh.\nWhen we\u0026rsquo;re complete, requests to metal and jazz will be distributed in a weighted fashion to both the v1 and v2 versions, with 95% going to the current prod v1 and 5% going to our release candidate v2.\nv2 Services YAML for this step can be found in the 3_canary_new_version directory. First, let\u0026rsquo;s look at the new v2 deployments and services. Below is jazz-v2, for example.\n--- apiVersion: apps/v1 kind: Deployment metadata: name: jazz-v2 spec: replicas: 1 selector: matchLabels: app: jazz version: v2 template: metadata: labels: app: jazz version: v2 spec: containers: - name: jazz image: \u0026#34;672518094988.dkr.ecr.us-west-2.amazonaws.com/hello-world:v1.0\u0026#34; ports: - containerPort: 9080 env: - name: \u0026#34;HW_RESPONSE\u0026#34; value: \u0026#34;[\\\u0026#34;Astrud Gilberto (Bahia, Brazil)\\\u0026#34;,\\\u0026#34;Miles Davis (Alton, Illinois)\\\u0026#34;]\u0026#34; --- apiVersion: v1 kind: Service metadata: name: jazz-v2 labels: app: jazz version: v2 spec: ports: - port: 9080 name: http selector: app: jazz version: v2 ---  You can see our exciting new feature enhancement is ready to deploy.\nCanary for v2 If we deploy our new service versions into prod right now, we will not have any traffic routed to them. To achieve this, we need to define VirtualNodes for each new version, as well as apply an update to our existing VirtualRouters to send 5% of traffic to the new version.\nWe will use App Mesh\u0026rsquo;s weightedTargets route feature to configure this logic.\n--- apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualNode metadata: name: jazz-v2 namespace: prod spec: podSelector: matchLabels: app: jazz version: v2 listeners: - portMapping: port: 9080 protocol: http healthCheck: protocol: http path: \u0026#39;/ping\u0026#39; healthyThreshold: 2 unhealthyThreshold: 2 timeoutMillis: 2000 intervalMillis: 5000 serviceDiscovery: dns: hostname: jazz-v2.prod.svc.cluster.local --- apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualRouter metadata: name: jazz-router namespace: prod spec: listeners: - portMapping: port: 9080 protocol: http routes: - name: jazz-route httpRoute: match: prefix: / action: weightedTargets: - virtualNodeRef: name: jazz-v1 weight: 95 - virtualNodeRef: name: jazz-v2 weight: 5 ---  With this, our configuration changes are complete and ready to deploy. Apply these changes now with kubectl.\nkubectl apply -f 3_canary_new_version/v2_app.yaml virtualrouter.appmesh.k8s.aws/jazz-router configured virtualrouter.appmesh.k8s.aws/metal-router configured virtualnode.appmesh.k8s.aws/jazz-v2 created virtualnode.appmesh.k8s.aws/metal-v2 created deployment.apps/jazz-v2 created deployment.apps/metal-v2 created service/jazz-v2 created service/metal-v2 created  Verify the status of your new and existing Kubernetes objects.\nkubectl -n prod get deployments,services,virtualnodes,virtualrouters NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/dj 1/1 1 1 139m deployment.apps/jazz-v1 1/1 1 1 139m deployment.apps/metal-v1 1/1 1 1 139m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dj ClusterIP 10.100.42.60 \u0026lt;none\u0026gt; 9080/TCP 139m service/jazz ClusterIP 10.100.134.233 \u0026lt;none\u0026gt; 9080/TCP 121m service/jazz-v1 ClusterIP 10.100.192.113 \u0026lt;none\u0026gt; 9080/TCP 139m service/metal ClusterIP 10.100.175.238 \u0026lt;none\u0026gt; 9080/TCP 121m service/metal-v1 ClusterIP 10.100.238.120 \u0026lt;none\u0026gt; 9080/TCP 139m NAME ARN AGE virtualnode.appmesh.k8s.aws/dj arn:aws:appmesh:us-west-2:510431938379:mesh/dj-app/virtualNode/dj_prod 121m virtualnode.appmesh.k8s.aws/jazz-v1 arn:aws:appmesh:us-west-2:510431938379:mesh/dj-app/virtualNode/jazz-v1_prod 121m virtualnode.appmesh.k8s.aws/jazz-v2 arn:aws:appmesh:us-west-2:510431938379:mesh/dj-app/virtualNode/jazz-v2_prod 37s virtualnode.appmesh.k8s.aws/metal-v1 arn:aws:appmesh:us-west-2:510431938379:mesh/dj-app/virtualNode/metal-v1_prod 121m virtualnode.appmesh.k8s.aws/metal-v2 arn:aws:appmesh:us-west-2:510431938379:mesh/dj-app/virtualNode/metal-v2_prod 37s NAME ARN AGE virtualrouter.appmesh.k8s.aws/jazz-router arn:aws:appmesh:us-west-2:510431938379:mesh/dj-app/virtualRouter/jazz-router_prod 121m virtualrouter.appmesh.k8s.aws/metal-router arn:aws:appmesh:us-west-2:510431938379:mesh/dj-app/virtualRouter/metal-router_prod 121m  Dig in a little deeper to see the updated Route configuration for the \u0026lsquo;jazz-router\u0026rsquo; virtual router.\nkubectl -n prod describe virtualrouters jazz-router .. Routes: Http Route: Action: Weighted Targets: Virtual Node Ref: Name: jazz-v1 Weight: 95 Virtual Node Ref: Name: jazz-v2 Weight: 5 Match: Prefix: / Name: jazz-route .. { Here you can see that it is configured to route 95% of traffic to the v1 version of the service, and 5% to v2 for canary testing. Let\u0026rsquo;s test this out.\n"
},
{
	"uri": "/beginner/091_iam-groups/",
	"title": "Using IAM Groups to manage Kubernetes access",
	"tags": ["beginner"],
	"description": "",
	"content": "Using IAM Groups to manage Kubernetes cluster access   In this chapter, we\u0026rsquo;ll learn about how to simplify access to different parts of the kubernetes clusters depending on IAM Groups\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/integrate_observability/container_insights/",
	"title": "Container Insights",
	"tags": [],
	"description": "",
	"content": "Container Insights CloudWatch Container Insights is a fully managed service that collects, aggregates, and summarizes Amazon EKS metrics and logs. The CloudWatch Container Insights dashboard gives you access to the following information:\n CPU and memory utilization Task and service counts Read/write storage Network Rx/Tx Container instance counts for clusters, services, and tasks  Log into console, navigate to Cloudwatch -\u0026gt; Container Insights -\u0026gt; Performance monitoring, you can see the EKS Cluster insight You can change the dropdown to EKS Nodes to see the Nodes insight "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/integrate_observability/cloudwatch_logs/",
	"title": "Cloudwatch Logs",
	"tags": [],
	"description": "",
	"content": "Cloudwatch Console Logs are collected by the fluentd daemonset running in the EKS nodes and also by Fluentbit daemonset that collect logs from Fargate and send them to Cloudwatch.\nThe following CloudWatch log groups are created by default when Container Insights is setup:\n /aws/containerinsights/cluster-name/application /aws/containerinsights/cluster-name/dataplane /aws/containerinsights/cluster-name/hostNodegroup Container Logs /aws/containerinsights/cluster-name/performance /aws/eks/eksworkshop-eksctl/cluster fluent-bit-cloudwatch  Log into console, navigate to Cloudwatch -\u0026gt; LogGroups, you should see below log groups Nodegroup Container Logs Click on the application LogGroup, and click on Search All Now, Type Catalog Detail Version in the Search box and Click enter, you should see the below logs from proddetail backend service Fargate Container Logs Log into console, navigate to Cloudwatch -\u0026gt; LogGroups -\u0026gt; Click on fluent-bit-cloudwatch LogGroup -\u0026gt; Click on Search All and type Get Request succeeded in the search box and enter, you should see below logs (Optional) Control Plane Logging If you want to enable control plane logging, follow this link\nThen, Log into console, navigate to Cloudwatch -\u0026gt; LogGroups, you should see log group /aws/eks/eksworkshop-eksctl/cluster for Control Plane as below "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/integrate_observability/prometheus_metrics/",
	"title": "Prometheus Metrics",
	"tags": [],
	"description": "",
	"content": "CloudWatch Container Insights monitoring for Prometheus automates the discovery of Prometheus metrics from containerized systems and workloads. Prometheus is an open-source systems monitoring and alerting toolkit.\nPrometheus Metrics Logs The CloudWatch agent supports the standard Prometheus scrape configurations as documented in scrape_config in the Prometheus documentation. The CloudWatch agent YAML that we set up in the chapter enable-prometheus-metrics-in-cloudwatch have jobs configured that are scraped, and the metrics are sent to CloudWatch.\nLog events from Amazon EKS and Kubernetes clusters are stored in the /aws/containerinsights/cluster_name/prometheus LogGroup in Amazon CloudWatch Logs.\nLog into console, navigate to Cloudwatch -\u0026gt; LogGroups, you should see /aws/containerinsights/eksworkshop-eksctl/prometheus LogGroup, select this and you should be able to see the metrics for all the containers logged here. Prometheus Metrics The CloudWatch agent with Prometheus support automatically collects metrics from services and workloads. Prometheus metrics collected from Amazon EKS and Kubernetes clusters are in the ContainerInsights/Prometheus namespace.\nLog into console, navigate to Cloudwatch -\u0026gt; Metrics -\u0026gt; ContainerInsights/Prometheus -\u0026gt; \u0026ldquo;ClusterName, Namespace\u0026rdquo; , you should see the below metrics for the namespace prodcatalog-ns. Select any of the metrics (as in below example envoy_cluster_upstream_cx_rx_bytes_total) to add to the graph. You can find the complete list of Prometheus Metrics for App Mesh here. Prometheus Report In the CloudWatch console, Container Insights provides pre-built reports for App Mesh in Amazon EKS and Kubernetes clusters.\nTo see the pre-built reports on App Mesh Prometheus metrics, Log into console, navigate to Cloudwatch -\u0026gt; \u0026ldquo;Performance monitoring\u0026rdquo; -\u0026gt; \u0026ldquo;Container Insights\u0026rdquo;, select EKS Prometheus App Mesh in first dropdown and select the EKS cluster in second dropdown. "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/integrate_observability/xray_trace/",
	"title": "XRay Trace",
	"tags": [],
	"description": "",
	"content": "XRay Trace AWS X-Ray helps developers and DevOps engineers quickly understand how an application and its underlying services are performing. When it‚Äôs integrated with AWS App Mesh, the combination makes for a powerful analytical tool.\nTo instrument your application code, use the X-Ray SDK. The SDK records data about incoming and outgoing requests and sends it to the X-Ray daemon, which relays the data in batches to X-Ray. See the examples in the code below for our Product Catalog demo application.\n Frontend Product Catalog Catalog Detail  Service Map Log into console, navigate to X-Ray, you should see the below in the Service Map. AWS X-Ray service maps show information about call from the client to its downstream services. The service graph arrows show the request workflow, which helps to understand the relationships between services. Below graph shows the traces when we access the Product Catalog application from the Load Balancer endpoint:\n First, the Envoy proxy prodcatalog-mesh/ingress-gw of VirtualGateway received the request and routed it to the Envoy proxy prodcatalog-mesh/frontend-node. Then, the Envoy proxy prodcatalog-mesh/frontend-node routed it to the server Frontend Node. Then, Frontend Node made a request to server Product Catalog to retrieve the products. Instead of directly calling the Product Catalog server, the request went to the frontend-node Envoy proxy and the proxy routed the call to Product Catalog server. Then, the Envoy proxy prodcatalog-mesh/prodcatalog received the request and routed it to the server Product Catalog. Then, Product Catalog made a request to server Product Detail V1 to retrieve the catalog detail information for version 1. Instead of directly calling the Product Detail V1 server, the request went to the prodcatalog Envoy proxy and the proxy routed the call to Product Detail V1. Then, the Envoy proxy prodcatalog-mesh/prodetail-v1 received the request and routed it to the server Product Detail V1. Similar steps of workflow happens when Product Detail V2 is accessed when we click on Canary Deployment button.  Trace Details Frontend Node service to Product Catalog service\nLog into console, navigate to X-Ray -\u0026gt; Service Map -\u0026gt; Click on prodcatalog-mesh/frontend-node_prodcatalog-ns, you should see the below page. Now Click on View Traces in above page, you should see below which shows all the requests routed from prodcatalog-mesh/frontend-node_prodcatalog-ns Now Click on http://prodcatalog.prodcatalog-ns.svc.cluster.local:5000/products/ and then click on any trace from the Trace List table to see the detailed traces for this request. In this page, you see the number of request, latency for each hop in the trace between prodcatalog-mesh/frontend-node_prodcatalog-ns and the Product Catalog server. Product Catalog service to Product Detail V1 service\nLog into console, navigate to X-Ray -\u0026gt; Service Map -\u0026gt; Click on prodcatalog-mesh/prodcatalog_prodcatalog-ns, you should see the below page. Now Click on View Traces in above page, you should see below which shows all the requests routed from prodcatalog-mesh/prodcatalog_prodcatalog-ns Now Click on http://prodetail.prodcatalog-ns.svc.cluster.local:5000/catalogDetail/ and then click on any trace from the Trace List table to see the detailed traces for this request. In this page, you see the number of request, latency for each hop in the trace between prodcatalog-mesh/prodcatalog_prodcatalog-ns and the Product Detail V1 server. "
},
{
	"uri": "/advanced/430_emr_on_eks/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "For more hands-on experience, see the dedicated EMR on EKS Workshop.\nEmpty and delete S3 buckets aws s3 rm $s3DemoBucket --recursive aws s3 rb $s3DemoBucket --force Delete IAM Role and policiess aws iam delete-role-policy --role-name EMRContainers-JobExecutionRole --policy-name review-data-access aws iam delete-role-policy --role-name EMRContainers-JobExecutionRole --policy-name EMR-Containers-Job-Execution aws iam delete-role --role-name EMRContainers-JobExecutionRole Delete Virtual Cluster Cluster cannot be deleted unless the pods in pending state are cleaned up. Lets find out running jobs and cancel them.\nfor Job_id in $(aws emr-containers list-job-runs --states RUNNING --virtual-cluster-id ${VIRTUAL_CLUSTER_ID} --query \u0026#34;jobRuns[?state==\u0026#39;RUNNING\u0026#39;].id\u0026#34; --output text ); do aws emr-containers cancel-job-run --id ${Job_id} --virtual-cluster-id ${VIRTUAL_CLUSTER_ID}; done We can now delete the cluster\naws emr-containers delete-virtual-cluster --id ${VIRTUAL_CLUSTER_ID} To delete the namespace, the node group and the Fargate profile created by this module, run the following commands\nkubectl delete namespace spark eksctl delete fargateprofile --cluster=eksworkshop-eksctl --name emr --wait eksctl delete nodegroup --config-file=addnodegroup.yaml --approve eksctl delete nodegroup --config-file=addnodegroup-spot.yaml --approve eksctl delete nodegroup --config-file=addnodegroup-nytaxi.yaml --approve "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Namespace deletion may take few minutes, please wait till the process completes.\n Delete Product Catalog apps kubectl delete namespace prodcatalog-ns Delete ECR images aws ecr delete-repository --repository-name eks-app-mesh-demo/catalog_detail --force aws ecr delete-repository --repository-name eks-app-mesh-demo/frontend_node --force aws ecr delete-repository --repository-name eks-app-mesh-demo/product_catalog --force (Only if you enabled for this workshop) Disable Amazon EKS Control Pane Logs eksctl utils update-cluster-logging --disable-types all \\  --region ${AWS_REGION} \\  --cluster eksworkshop-eksctl \\  --approve Delete Cloudwatch namespace kubectl delete namespace amazon-cloudwatch Delete Observability namespace kubectl delete namespace aws-observability Delete the Product Catalog mesh kubectl delete meshes prodcatalog-mesh Uninstall the Helm Charts helm -n appmesh-system delete appmesh-controller Delete AWS App Mesh CRDs for i in $(kubectl get crd | grep appmesh | cut -d\u0026#34; \u0026#34; -f1) ; do kubectl delete crd $i done Delete the AppMesh Controller service account eksctl delete iamserviceaccount --cluster eksworkshop-eksctl --namespace appmesh-system --name appmesh-controller Delete the AWS App Mesh namespace kubectl delete namespace appmesh-system Delete Fargate Logging Policy export PodRole=$(aws eks describe-fargate-profile --cluster-name eksworkshop-eksctl --fargate-profile-name fargate-productcatalog --query \u0026#39;fargateProfile.podExecutionRoleArn\u0026#39; | sed -n \u0026#39;s/^.*role\\/\\(.*\\)\u0026#34;.*$/\\1/ p\u0026#39;) aws iam detach-role-policy \\  --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/FluentBitEKSFargate \\  --role-name ${PodRole} aws iam delete-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/FluentBitEKSFargate Delete Fargate profile eksctl delete fargateprofile \\  --name fargate-productcatalog \\  --cluster eksworkshop-eksctl Delete the policy and IRSA eksctl delete iamserviceaccount --cluster eksworkshop-eksctl --namespace prodcatalog-ns --name prodcatalog-envoy-proxies aws iam delete-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/ProdEnvoyNamespaceIAMPolicy "
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/canary_deployment/testing_v2/",
	"title": "Testing Canary",
	"tags": [],
	"description": "",
	"content": "Get the Loadbalancer endpoint\nexport LB_NAME=$(kubectl get svc ingress-gw -n prodcatalog-ns -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[*].hostname}\u0026#34;) echo $LB_NAME Now, back in your browser, you should see below screen which shows that Catalog Detail proddetail-v1 Version 1 is being used.\nNow click on the button Canary Deployment few times as we set the route weight as 10% to proddetail V2 and 90% to proddetail V1, you should see information coming from Product Catalog proddetail-v2 V2 after few clicks. You can see the XYZ.com vendor in the detail for proddetail V2.\nOnce we gain confidence in our new service version 2, and we see no error or latency issue, then we can decide to divert more traffic gradually to version V2. In below example we shift the traffic weight to 50% for both proddetail service versions in VirualRouter.\n--- apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualRouter metadata: name: proddetail-router namespace: prodcatalog-ns spec: ..... routes: - name: proddetail-route httpRoute: match: prefix: / action: weightedTargets: - virtualNodeRef: name: proddetail-v1 weight: 50 - virtualNodeRef: name: proddetail-v2 weight: 50 ---  If anything were to go wrong, you can simply rollback to the known-good v1 version of the services by changing the weight in VirtualRouter to 100% to version 1. Once you\u0026rsquo;ve verified things are good with the new versions, you can shift all traffic to them and deprecate v1.\nCongrats on rolling out your new feature! Now lets see the logs and traces of our Application Services to get the end to end visibility in terms of observability.\n"
},
{
	"uri": "/intermediate/330_app_mesh/cleanup/",
	"title": "App Mesh Cleanup",
	"tags": [],
	"description": "",
	"content": "Delete DJ App kubectl delete namespace prod Delete the dj-app mesh kubectl delete meshes dj-app Uninstall the Helm Charts helm -n appmesh-system delete appmesh-controller Delete AWS App Mesh CRDs for i in $(kubectl get crd | grep appmesh | cut -d\u0026#34; \u0026#34; -f1) ; do kubectl delete crd $i done Delete the AWS App Mesh namespace kubectl delete namespace appmesh-system Delete the appmesh-controller service account eksctl delete iamserviceaccount --cluster eksworkshop-eksctl --namespace appmesh-system --name appmesh-controller "
},
{
	"uri": "/010_introduction/architecture/architecture_control/",
	"title": "Control Plane",
	"tags": [],
	"description": "",
	"content": "graph TB kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end kubectl--api controller--api scheduler--api api--kubelet api--etcd classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;    One or More API Servers: Entry point for REST / kubectl\n  etcd: Distributed key/value store\n  Controller-manager: Always evaluating current vs desired state\n  Scheduler: Schedules pods to worker nodes\n  Check out the official Kubernetes documentation for a more in-depth explanation of control plane components.\n"
},
{
	"uri": "/intermediate/330_app_mesh/port_to_app_mesh/testing_v2/",
	"title": "Testing DJ App v2",
	"tags": [],
	"description": "",
	"content": "To test if our canary is working as expected, once again exec into the dj container and send some requests.\nexport DJ_POD_NAME=$(kubectl get pods -n prod -l app=dj -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n prod exec -it ${DJ_POD_NAME} -c dj bash Once at the container\u0026rsquo;s prompt, issue a series of curl\u0026rsquo;s to test the traffic distribution.\nroot@dj-5b445fbdf4-8xkwp:/usr/src/app#  while true; do curl http://jazz.prod.svc.cluster.local:9080/ echo sleep .5 done Output should be similar to this, with about 5% of our traffic getting the new version.\n... [\u0026#34;Astrud Gilberto\u0026#34;,\u0026#34;Miles Davis\u0026#34;] [\u0026#34;Astrud Gilberto\u0026#34;,\u0026#34;Miles Davis\u0026#34;] [\u0026#34;Astrud Gilberto\u0026#34;,\u0026#34;Miles Davis\u0026#34;] [\u0026#34;Astrud Gilberto\u0026#34;,\u0026#34;Miles Davis\u0026#34;] [\u0026#34;Astrud Gilberto (Bahia, Brazil)\u0026#34;,\u0026#34;Miles Davis (Alton, Illinois)\u0026#34;] [\u0026#34;Astrud Gilberto\u0026#34;,\u0026#34;Miles Davis\u0026#34;] [\u0026#34;Astrud Gilberto\u0026#34;,\u0026#34;Miles Davis\u0026#34;] [\u0026#34;Astrud Gilberto\u0026#34;,\u0026#34;Miles Davis\u0026#34;] [\u0026#34;Astrud Gilberto\u0026#34;,\u0026#34;Miles Davis\u0026#34;] [\u0026#34;Astrud Gilberto\u0026#34;,\u0026#34;Miles Davis\u0026#34;] ...  Hit CTRL-C to stop the looping.\nAt this point, You would typically monitor metrics from the new version of the service, and slowly roll out more traffic to it.\nLet\u0026rsquo;s live dangerously and shift 50% of our traffic to v2. Use your favorite editor and modify the VirtualRouter configurations for both jazz-router and metal-router, then apply your changes and re-test.\nHint: look for the weight values specified in the weightedTargets specification.\nOnce you\u0026rsquo;ve modified the router configurations in YAML, apply your changes.\nkubectl apply -f 3_canary_new_version/v2_app.yaml virtualrouter.appmesh.k8s.aws/jazz-router configured virtualrouter.appmesh.k8s.aws/metal-router configured virtualnode.appmesh.k8s.aws/jazz-v2 unchanged virtualnode.appmesh.k8s.aws/metal-v2 unchanged deployment.apps/jazz-v2 unchanged deployment.apps/metal-v2 unchanged service/jazz-v2 unchanged service/metal-v2 unchanged  Now jump back into the dj container and send some requests.\nkubectl -n prod exec -it ${DJ_POD_NAME} -c dj bash root@dj-5b445fbdf4-8xkwp:/usr/src/app#  while true; do curl http://metal.prod.svc.cluster.local:9080/ echo sleep .5 done You should output similar to this, with about 50% of your traffic routing to the new version.\n[\u0026#34;Megadeth\u0026#34;,\u0026#34;Judas Priest\u0026#34;] [\u0026#34;Megadeth\u0026#34;,\u0026#34;Judas Priest\u0026#34;] [\u0026#34;Megadeth (Los Angeles, California)\u0026#34;,\u0026#34;Judas Priest (West Bromwich, England)\u0026#34;] [\u0026#34;Megadeth\u0026#34;,\u0026#34;Judas Priest\u0026#34;] [\u0026#34;Megadeth (Los Angeles, California)\u0026#34;,\u0026#34;Judas Priest (West Bromwich, England)\u0026#34;] [\u0026#34;Megadeth (Los Angeles, California)\u0026#34;,\u0026#34;Judas Priest (West Bromwich, England)\u0026#34;] [\u0026#34;Megadeth\u0026#34;,\u0026#34;Judas Priest\u0026#34;] [\u0026#34;Megadeth\u0026#34;,\u0026#34;Judas Priest\u0026#34;] [\u0026#34;Megadeth (Los Angeles, California)\u0026#34;,\u0026#34;Judas Priest (West Bromwich, England)\u0026#34;]  If anything were to go wrong, you can simply rollback to the known-good v1 version of the services. Once you\u0026rsquo;ve verified things are good with the new versions, you can shift all traffic to them and deprecate v1.\nCongrats on rolling out your new feature!\n"
},
{
	"uri": "/beginner/060_helm/helm_nginx/updatecharts/",
	"title": "Update the Chart Repository",
	"tags": [],
	"description": "",
	"content": "Helm uses a packaging format called Charts. A Chart is a collection of files and templates that describes Kubernetes resources.\nCharts can be simple, describing something like a standalone web server (which is what we are going to create), but they can also be more complex, for example, a chart that represents a full web application stack, including web servers, databases, proxies, etc.\nInstead of installing Kubernetes resources manually via kubectl, one can use Helm to install pre-defined Charts faster, with less chance of typos or other operator errors.\nChart repositories change frequently due to updates and new additions. To keep Helm\u0026rsquo;s local list updated with all these changes, we need to occasionally run the repository update command.\nTo update Helm\u0026rsquo;s local list of Charts, run:\n# first, add the default repository, then update helm repo add stable https://charts.helm.sh/stable helm repo update And you should see something similar to:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. ‚éà Happy Helming!‚éà  Next, we\u0026rsquo;ll search for the nginx web server Chart.\n"
},
{
	"uri": "/beginner/110_irsa/",
	"title": "IAM Roles for Service Accounts",
	"tags": ["intermediate", "CON205"],
	"description": "",
	"content": "Fine-Grained IAM Roles for Service Accounts   In Kubernetes version 1.12, support was added for a new¬†ProjectedServiceAccountToken¬†feature, which is an OIDC JSON web token that also contains the service account identity, and supports a configurable audience.\nAmazon EKS now hosts a public OIDC discovery endpoint per cluster containing the signing keys for the¬†ProjectedServiceAccountToken¬†JSON web tokens so external systems, like IAM, can validate and accept the Kubernetes-issued OIDC tokens.\nOIDC federation access¬†allows you to assume IAM roles via the Secure Token Service (STS), enabling authentication with an OIDC provider, receiving a JSON Web Token (JWT), which in turn can be used to assume an IAM role. Kubernetes, on the other hand, can issue so-called¬†projected service account tokens, which happen to be valid OIDC JWTs for pods. Our setup equips each pod with a cryptographically-signed token that can be verified by STS against the OIDC provider of your choice to establish the pod‚Äôs identity.\nnew credential provider ‚Äùsts:AssumeRoleWithWebIdentity‚Äù\n"
},
{
	"uri": "/010_introduction/architecture/architecture_worker/",
	"title": "Data Plane",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 api--kubelet1 kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;    Made up of worker nodes\n  kubelet: Acts as a conduit between the API server and the node\n  kube-proxy: Manages IP translation and routing\n  Check out the official Kubernetes documentation for a more in-depth explanation of data plane components.\n"
},
{
	"uri": "/beginner/115_sg-per-pod/",
	"title": "Security groups for pods",
	"tags": ["beginner"],
	"description": "",
	"content": "Introduction Containerized applications frequently require access to other services running within the cluster as well as external AWS services, such as Amazon Relational Database Service (Amazon RDS).\nOn AWS, controlling network level access between services is often accomplished via security groups.\nBefore the release of this new functionality, you could only assign security groups at the node level. And because all nodes inside a Node group share the security group, by attaching the security group to access the RDS instance to the Node group, all the pods running on theses nodes would have access the database even if only the green pod should have access.\nSecurity groups for pods integrate Amazon EC2 security groups with Kubernetes pods. You can use Amazon EC2 security groups to define rules that allow inbound and outbound network traffic to and from pods that you deploy to nodes running on many Amazon EC2 instance types. For a detailed explanation of this capability, see the Introducing security groups for pods blog post and the official documentation.\nObjectives During this section of the workshop:\n We will create an Amazon RDS database protected by a security group called RDS_SG. We will create a security group called POD_SG that will be allowed to connect to the RDS instance. Then we will deploy a SecurityGroupPolicy that will automatically attach the POD_SG security group to a pod with the correct metadata. Finally we will deploy two pods (green and red) using the same image and verify that only one of them (green) can connect to the Amazon RDS database.  "
},
{
	"uri": "/beginner/120_network-policies/",
	"title": "Securing Your Cluster with Network Policies",
	"tags": ["intermediate"],
	"description": "",
	"content": "Securing your cluster with network policies In this chapter, we are going to use two tools to secure our cluster by using network policies and then integrating our cluster\u0026rsquo;s network policies with EKS security groups.\nFirst we will use Project Calico to enforce Kubernetes network policies in our cluster, protecting our various microservices.\nAfter that, we will use Calico Enterprise to\n Implement Egress Access Controls to enable your EKS workloads to communicate with other Amazon services (for example: RDS or EC2 instances) or other API endpoints. Troubleshoot microservices that are unable to communicate with each other Implement and report on Enterprise Security Controls in EKS  "
},
{
	"uri": "/010_introduction/architecture/cluster_setup_options/",
	"title": "Kubernetes Cluster Setup",
	"tags": [],
	"description": "",
	"content": "In addition to the managed Amazon EKS solution, there are many tools available to help bootstrap and configure a self-managed Kubernetes cluster. They include:\n Minikube ‚Äì Development and Learning Kops ‚Äì Learning, Development, Production Kubeadm ‚Äì Learning, Development, Production Docker for Mac - Learning, Development Kubernetes IN Docker - Learning, Development  Alongside these open source solutions, there are also many commercial options available.\nLet\u0026rsquo;s take a look at Amazon EKS!\n"
},
{
	"uri": "/beginner/130_exposing-service/",
	"title": "Exposing a Service",
	"tags": ["beginner", "CON203"],
	"description": "",
	"content": "Introduction   In this Chapter, we will review how to configure a Service, Deployment or Pod to be exposed outside our cluster. We will also review the different ways to do so.\n"
},
{
	"uri": "/010_introduction/eks/eks_customers/",
	"title": "EKS Cluster Creation Workflow",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/beginner/140_assigning_pods/",
	"title": "Assigning Pods to Nodes",
	"tags": ["intermediate", "CON203"],
	"description": "",
	"content": "Assigning Pods to Nodes   Introduction In this Chapter, we will review how the strategy of assigning Pods works, alternatives and recommended approaches.\nYou can constrain a pod to only be able to run on particular nodes or to prefer to run on particular nodes.\nGenerally such constraints are unnecessary, as the scheduler will automatically do a reasonable placement (e.g. spread your pods across nodes, not place the pod on a node with insufficient free resources, etc.) but there are some circumstances where you may want more control on a node where a pod lands, e.g. to ensure that a pod ends up on a machine with an SSD attached to it, or to co-locate pods from two different services that communicate a lot into the same availability zone.\n"
},
{
	"uri": "/010_introduction/eks/eks_control_plane/",
	"title": "What happens when you create your EKS cluster",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/beginner/150_spotnodegroups/",
	"title": "Using Spot Instances with EKS",
	"tags": ["beginner", "CON206"],
	"description": "",
	"content": "Using Spot Instances with EKS   In this module, you will learn how to provision, manage, and maintain your Kubernetes clusters with Amazon EKS on EC2 Spot instances using Spot managed node groups to optimize cost and scale. Click here for a deep-dive blog post on Kubernetes and EC2 Spot Instances in managed node groups.\n"
},
{
	"uri": "/010_introduction/eks/eks_high_architecture/",
	"title": "EKS Architecture for Control plane and Worker node communication",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/010_introduction/eks/eks_high_level/",
	"title": "High Level",
	"tags": [],
	"description": "",
	"content": "Once your EKS cluster is ready, you get an API endpoint and you\u0026rsquo;d use Kubectl, community developed tool to interact with your cluster.\n"
},
{
	"uri": "/beginner/160_advanced-networking/",
	"title": "Advanced VPC Networking with EKS",
	"tags": ["intermediate"],
	"description": "",
	"content": "Advanced VPC Networking with EKS   In this Chapter, we will review some of the advanced VPC networking features with EKS.\n"
},
{
	"uri": "/010_introduction/eks/stay_tuned/",
	"title": "Amazon EKS!",
	"tags": [],
	"description": "",
	"content": "Stay tuned as we continue the journey with EKS in the next module!\nAlways ask questions! Feel free to ask them in person during this workshop, or any time on the official Kubernetes Slack channel accessible via http://slack.k8s.io/.\n"
},
{
	"uri": "/beginner/170_statefulset/",
	"title": "Stateful containers using StatefulSets",
	"tags": ["beginner", "CON206"],
	"description": "",
	"content": "Stateful containers using StatefulSets StatefulSet manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods, suitable for applications that require one or more of the following.\n Stable, unique network identifiers Stable, persistent storage Ordered, graceful deployment and scaling Ordered, automated rolling updates  In this Chapter, we will review how to deploy a MySQL database using StatefulSet and Amazon Elastic Block Store (EBS) as PersistentVolume. The example is a MySQL single leader topology with a follower running asynchronous replication.\n"
},
{
	"uri": "/beginner/185_bottlerocket/",
	"title": "Deploy Bottlerocket nodes for additional security",
	"tags": ["beginner"],
	"description": "",
	"content": "Deploying Bottlerocket nodes to your cluster Bottlerocket is a Linux-based open-source operating system that is purpose-built by Amazon Web Services for running containers. Bottlerocket includes only the essential software required to run containers, and ensures that the underlying software is always secure. With Bottlerocket, customers can reduce maintenance overhead and automate their workflows by applying configuration settings consistently as nodes are upgraded or replaced. Bottlerocket is now generally available at no cost as an Amazon Machine Image (AMI) for Amazon Elastic Compute Cloud (EC2).\nIn this Chapter, we will deploy three Bottlerocket-based nodes and deploy an nginx pod on one of them.\n"
},
{
	"uri": "/beginner/180_fargate/",
	"title": "Deploying Microservices to EKS Fargate",
	"tags": ["beginner", "CON206"],
	"description": "",
	"content": "Deploying Microservices to EKS Fargate   AWS Fargate is a technology that provides on-demand, right-sized compute capacity for containers. With AWS Fargate, you no longer have to provision, configure, or scale groups of virtual machines to run containers. This removes the need to choose server types, decide when to scale your node groups, or optimize cluster packing. You can control which pods start on Fargate and how they run with Fargate profiles, which are defined as part of your Amazon EKS cluster.\nIn this Chapter, we will deploy the game 2048 game on EKS Fargate and expose it to the Internet using an Application Load balancer.\n"
},
{
	"uri": "/beginner/190_fsx_lustre/",
	"title": "Deploying Stateful Microservices with Amazon FSx Lustre",
	"tags": ["beginner", "CON206"],
	"description": "",
	"content": "Deploying Stateful Microservices with Amazon FSx Lustre   Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance storage for compute workloads. Many workloads such as machine learning, high performance computing (HPC), video rendering, and financial simulations depend on compute instances accessing the same set of data through high-performance shared storage.\nPowered by Lustre, the world\u0026rsquo;s most popular high-performance file system, FSx for Lustre offers sub-millisecond latencies, up to hundreds of gigabytes per second of throughput, and millions of IOPS. It provides multiple deployment options and storage types to optimize cost and performance for your workload requirements.\nFSx for Lustre file systems can also be linked to Amazon S3 buckets, allowing you to access and process data concurrently from both a high-performance file system and from the S3 API.\n"
},
{
	"uri": "/beginner/190_efs/",
	"title": "Deploying Stateful Microservices with AWS EFS",
	"tags": ["beginner", "CON206"],
	"description": "",
	"content": "Deploying Stateful Microservices with AWS EFS   Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.\nAmazon EFS supports the Network File System version 4 (NFSv4.1 and NFSv4.0) protocol, so the applications and tools that you use today work seamlessly with Amazon EFS. Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time, providing a common data source for workloads and applications running on more than one instance or server. For an Amazon EFS technical overview, see Amazon EFS: How It Works.\nIn this Chapter, you will create an EFS file system and share it across a set of stateful microservices deployed on AWS EKS which can all have concurrent access to the file system.\n"
},
{
	"uri": "/beginner/190_ocean/",
	"title": "Optimized Worker Node Management with Ocean by Spot.io",
	"tags": ["beginner"],
	"description": "",
	"content": "Optimized Worker Node Management with Ocean by Spot.io Introduction Amazon EC2 Spot Instances offer AWS customers up to 90% cost savings in comparison to On-Demand Instances. However, they can be interrupted with a 2 minute warning when EC2 needs the capacity back. While this itself does not pose any issue for stateless workloads such as those typically running on Amazon EKS, managing larger clusters running Spot Instances as worker nodes on your own, does require a large amount of manual configuration, setup and maintenance.\nFor AWS customers looking for a turn-key solution that doesn‚Äôt require significant time and effort, Ocean abstracts the nitty-gritty, EKS infrastructure management and provides an enterprise-level SLA for high availability, and data persistence.\nThe result is that your EKS cluster will automatically run on an optimal blend of Spot Instances, Savings Plans and Reserved Instances as well as On-Demand when needed, so you can focus on higher value activities.\nPrerequisites  An AWS Account. An existing EKS cluster. An installed and configured kubectl.  What you will achieve After concluding this module, you will be set up with Ocean by Spot.io (previously Spotinst), understand the benefits it brings to EKS users and will be able to run a fully optimized EKS cluster, with ease and confidence.\nThroughout this module we will be working with the Console UI, and use of the AWS vended version of eksctl is assumed. However, you should know that there is an Ocean integrated version of eksctl, which allows you to streamline and optimize the entire process. If you wish to learn more about the Spot.io vended version of eksctl click here.\n "
},
{
	"uri": "/beginner/191_secrets/",
	"title": "Encrypting Secrets with AWS Key Management Service (KMS) Keys",
	"tags": ["beginner"],
	"description": "",
	"content": "Encrypting Kubernetes Secrets   Kubernetes can store secrets that pods can access via a mounted volume. Today, Kubernetes secrets are stored with Base64 encoding, but security teams would prefer a stronger approach. Amazon EKS clusters version 1.13 and higher support the capability of encrypting your Kubernetes secrets using AWS Key Management Service (KMS) Customer Managed Keys (CMK). No changes in the way you are using secrets are required. The only requirement is to enable the encryption provider support during EKS cluster creation.\nThe workflow is as follows:\n The user (typically in an admin role) creates a secret. The Kubernetes API server in the EKS control plane generates a Data Encryption Key (DEK) locally and uses it to encrypt the plaintext payload in the secret. Note that the control plane generates a unique DEK for every single write, and the plaintext DEK is never saved to disk. The Kubernetes API server calls kms:Encrypt to encrypt the DEK with the CMK. This key is the root of the key hierarchy, and, in the case of KMS, it creates the CMK on a hardware security module (HSM). In this step, the API server uses the CMK to encrypt the DEK and also caches the base64 of the encrypted DEK. The API server stores the DEK-encrypted secret in etcd. If one now wants to use the secret in, say a pod via a volume (read-path), the reverse process takes place. That is, the API server reads the encrypted secret from etcd and decrypts the secret with the DEK. The application, running in a pod on either EC2 or Fargate, can then consume the secret as usual.  "
},
{
	"uri": "/beginner/200_secrets/",
	"title": "Securing Secrets using SealedSecrets",
	"tags": ["beginner", "CON206"],
	"description": "",
	"content": "Securing Secrets using SealedSecrets Kubernetes Secret is a resource that helps cluster operators manage the deployment of sensitive information such as passwords, OAuth tokens, and ssh keys etc. These Secrets can be mounted as data volumes or exposed as environment variables to the containers in a Pod, thus decoupling Pod deployment from managing sensitive data needed by the containerized applications within a Pod.\nIt is a common practice for a DevOps Team to manage the YAML manifests for various Kubernetes resources and version control them using a Git repository. Additionally, they can integrate a Git repository with a GitOps workflow to do Continuous Delivery of such resources to an EKS cluster. The challenge here is about managing the YAML manifests for Kubernetes Secrets outside the cluster. The sensitive data in a Secret is obfuscated by using merely base64 encoding. Storing such files in a Git repository is extremely insecure as it is trivial to decode the base64 encoded data.\nSealed Secrets provides a mechanism to encrypt a Secret object so that it is safe to store - even to a public repository. A SealedSecret can be decrypted only by the controller running in the Kubernetes cluster and nobody else is able to obtain the original Secret from a SealedSecret. In this Chapter, you will use SealedSecrets to encrypt YAML manifests pertaining to Kubernetes Secrets as well as be able deploy these encrypted Secrets to your EKS clusters using normal workflows with tools such as kubectl.\n"
},
{
	"uri": "/beginner/060_helm/helm_nginx/searchchart/",
	"title": "Search Chart Repositories",
	"tags": [],
	"description": "",
	"content": "Now that our repository Chart list has been updated, we can search for Charts.\nTo list all Charts:\nhelm search repo That should output something similar to: NAME CHART VERSION APP VERSION DESCRIPTION stable/acs-engine-autoscaler 2.2.2 2.1.1 Scales worker... stable/aerospike 0.3.2 v4.5.0.5 A Helm chart... ...  You can see from the output that it dumped the list of all Charts we have added. In some cases that may be useful, but an even more useful search would involve a keyword argument. So next, we\u0026rsquo;ll search just for nginx:\nhelm search repo nginx That results in: NAME CHART VERSION APP VERSION DESCRIPTION stable/nginx-ingress 1.41.3 v0.34.1 DEPRECATED! An nginx Ingress controller ... stable/nginx-ldapauth-proxy 0.1.6 1.13.5 DEPRECATED - nginx proxy with ldapauth ... stable/nginx-lego 0.3.1 Chart for... stable/gcloud-endpoints 0.1.2 1 DEPRECATED Develop... ...  This new list of Charts are specific to nginx, because we passed the nginx argument to the helm search repo command.\nFurther information on the command can be found here.\n"
},
{
	"uri": "/intermediate/200_migrate_to_eks/",
	"title": "Migrate to EKS",
	"tags": ["intermediate"],
	"description": "",
	"content": "Migrate Workloads to EKS In this chapter we will migrate a workload from a self managed kind cluster to an EKS cluster. The workload will have a stateless frontend and a stateful database backend. You\u0026rsquo;ll need to follow the steps to create a Cloud9 workspace. Make sure you update your IAM permissions with an eksworkshop-admin role.\nWhen you create your Cloud9 instance you should select an instance size with at least 8 GB of memory (eg m5.large) because we are going to create a kind cluster we will migrate workloads from.\n We\u0026rsquo;ll need a few environment variables throughout this section so let\u0026rsquo;s set those up now. Unless specified all commands should be run from your Cloud9 instance.\nexport CLUSTER=eksworkshop export AWS_ZONE=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone) export AWS_REGION=${AWS_ZONE::-1} export AWS_DEFAULT_REGION=${AWS_REGION} export ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#39;Account\u0026#39; --output text) export INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id) export MAC=$(curl -s http://169.254.169.254/latest/meta-data/mac) export SECURITY_GROUP=$(curl -s http://169.254.169.254/latest/meta-data/network/interfaces/macs/${MAC}/security-group-ids) export SUBNET=$(curl -s http://169.254.169.254/latest/meta-data/network/interfaces/macs/${MAC}/subnet-id) export VPC=$(curl -s http://169.254.169.254/latest/meta-data/network/interfaces/macs/${MAC}/vpc-id) export IP=$(ip -4 addr show eth0 | grep -oP \u0026#39;(?\u0026lt;=inet\\s)\\d+(\\.\\d+){3}\u0026#39;) printf \u0026#34;export CLUSTER=$CLUSTER\\nexport ACCOUNT_ID=$ACCOUNT_ID\\nexport AWS_REGION=$AWS_REGION\\nexport AWS_DEFAULT_REGION=${AWS_REGION}\\nexport AWS_ZONE=$AWS_ZONE\\nexport INSTANCE_ID=$INSTANCE_ID\\nexport MAC=$MAC\\nexport SECURITY_GROUP=$SECURITY_GROUP\\nexport SUBNET=$SUBNET\\nexport VPC=$VPC\\nexport IP=$IP\u0026#34; | tee -a ~/.bash_profile . ~/.bash_profile Now we can expand the Cloud9 root volume\ncurl -sL \u0026#39;https://eksworkshop.com/intermediate/200_migrate_to_eks/resize-ebs.sh\u0026#39; | bash Install kubectl, kind, aws-iam-authenticator, eksctl and update aws\n# Install kubectl curl -sLO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl rm -f ./kubectl # install eksctl curl -sLO \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; tar xz -C /tmp -f \u0026#34;eksctl_$(uname -s)_amd64.tar.gz\u0026#34; sudo install -o root -g root -m 0755 /tmp/eksctl /usr/local/bin/eksctl rm -f ./\u0026#34;eksctl_$(uname -s)_amd64.tar.gz\u0026#34; # install aws-iam-authenticator curl -sLO \u0026#34;https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/aws-iam-authenticator\u0026#34; sudo install -o root -g root -m 0755 aws-iam-authenticator /usr/local/bin/aws-iam-authenticator rm -f ./aws-iam-authenticator # install kind curl -sLo kind \u0026#34;https://kind.sigs.k8s.io/dl/v0.11.0/kind-linux-amd64\u0026#34; sudo install -o root -g root -m 0755 kind /usr/local/bin/kind rm -f ./kind # install awscliv2 curl -sLo \u0026#34;awscliv2.zip\u0026#34; \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install rm -rf ./awscliv2.zip ./aws # setup tab completion /usr/local/bin/kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl \u0026gt;/dev/null /usr/local/bin/eksctl completion bash | sudo tee /etc/bash_completion.d/eksctl \u0026gt;/dev/null echo \u0026#39;source /usr/share/bash-completion/bash_completion\u0026#39; \u0026gt;\u0026gt; $HOME/.bashrc . $HOME/.bashrc Once you have everything done use eksctl to create an EKS cluster with the following command\neksctl create cluster --name $CLUSTER \\  --managed --enable-ssm "
},
{
	"uri": "/intermediate/201_resource_management/",
	"title": "Resource Management",
	"tags": ["intermediate"],
	"description": "",
	"content": "Resource Management   Kubernetes Request is used to ensure a Pod has enough defined resources available. It is possible for the Pod to use more than what is specified. This is considered a soft limit.\nKubernetes Limit is a used to ensure a Pod does not use above what is specified. This is considered a hard limit.\nKubernetes Resource Quotas is used to limit resource usage per namespace.\nKubernetes Pod Priority and Preemption is a used to apply priorities to pods relative to other pods. If a pod cannot be placed on a node, it may preempt or evict lower priority pods.\n"
},
{
	"uri": "/beginner/120_network-policies/calico-enterprise/lab-1/",
	"title": "Policy Automation and External Access",
	"tags": [],
	"description": "",
	"content": "Getting Started Before you go any further, please remember to type lab1 set_up in the terminal to set up the environment for this lab.\nIntroduction In this lab, you will learn to leverage Calico Enterprise to implement pod-level access controls and enable a self-service model that makes it easy for development teams to define and deploy Calico and Kubernetes network policies for their applications. This allows your organization to more easily adopt best practices around microsegmentation and a zero-trust approach to network security in your Kubernetes platform.\nYou will learn the following:  How to use Policy Tiers and Network Sets to define network policies that define the ‚Äúguard rails‚Äù for your entire Kubernetes platform How to use DNS policy rules to enable external access for applications in specific namespaces How to use the Flow Visualizer to gain visibility into existing traffic flows within the cluster and select workloads to recommend policies using Policy Recommendations How to use Policy Impact Preview and Staged Network Policies to evaluate the impact of new network policies and changes to existing policies before they are enforced within the cluster  Setup For this lab, we will be using a fairly simple microservices application called storefront to walk through the implementation of Network Policy - both as someone from platform engineering and as a storefront application developer.\nStorefront (Fig 1) is a fairly simple microservices application running in Kubernetes. It has a frontend service that handles end-user requests, communicates with two business logic services which in turn make requests to a backend service. All of these containers communicate with a logging service and one of the business logic services, microservice 2, makes external requests to Twilio to provide some telephony for this application.\nLet‚Äôs quickly take a look at the pods running in the storefront namespace that are using to run these microservices:\nkubectl get pods -n storefront NAME READY STATUS RESTARTS AGE backend-6cfbdd589f-xxlkn 2/2 Running 0 22h frontend-864f4fcdfd-2hhv4 4/4 Running 0 22h logging-684747d7cd-bbjwx 1/1 Running 0 22h microservice1-794cf77b9d-c27qr 4/4 Running 0 22h microservice2-7bb79d9f4f-f7h6f 5/5 Running 0 22h Using the Flow Visualizer Next, let‚Äôs login to the Calico Enterprise UI and use the Flow Visualizer to understand how these services are communicating with each other. Login to Calico Enterprise and select the Flow Visualizations from the left navigation menu\nThe Flow Visualizer is a powerful tool to gain visibility into network traffic within the cluster and troubleshoot issues. Later in this lab, we will also use it to recommend network policies for our storefront application.\nThe outer ring of the Flow Visualizer can be used to select the various namespaces within our cluster. Mousing over each subsection, you‚Äôll see the name of the namespace appear in the dropdown filters on the right-hand side.\nFind the storefront namespace and select it, then use the magnifying glass button in the upper right to zoom in on these network flows.\nMousing over subsections of the next inner ring (grey), you will see the pod prefixes for each of the microservices that make up our storefront application. Note that Calico Enterprise aggregates network flows so it is easy to make sense of traffic in your cluster that may be backed by Kubernetes resources like replica sets.\nThe innermost ring in the Flow Visualizer allows you to select specific network flows that are associated with each of our storefront microservices. Mousing over each subsection, you will see each of these flows on the right-hand side dropdown - these should correspond to the diagram at the beginning of this lab (Fig 1).\nUnderstanding Policy Tiers Now that we have a better understanding of the microservices that make up our storefront application and how they are communicating with each other, let‚Äôs assume the role of someone from platform engineering to start defining network policies for our cluster.\nCalico Enterprise makes it easy to define the ‚Äúguard rails‚Äù for your Kubernetes platform through the use of Policy Tiers. Policy Tiers allow platform engineering and security teams to enforce network policies that take precedence over those defined for specific applications like storefront.\nLogin to the Calico Enterprise UI and select Policies from the left navigation menu. Here you will see all of the network policies defined within the cluster and how they map to policy tiers.\nTiers are evaluated from left to right, and network policies within tiers are evaluated from top to bottom. This effectively means that a network policy in the Security tier (Fig. 4) needs to evaluate and pass traffic before any policy below it or to the right can see that same traffic. Tiers are tied to RBAC and provide a powerful way to implement security and platform controls for your entire cluster without having to involve application teams. Your lab environment already has a few policies in the Platform and Security tiers that provide some examples of some common use cases.\nEnabling access to Twilio APIs Putting on our platform engineering hat, let‚Äôs use policy tiers to define a network policy that will be used to control traffic leaving our cluster - in this case, applications like storefront that need access to the Twilio APIs.\nOn the Policies Board, click the +ADD POLICY link in our platform tier. Let‚Äôs call this policy ‚Äútwilio-integration‚Äù, add it after global.logging, and make the scope of the policy Global.\nBy default, a GlobalNetworkPolicy applies to the entire cluster. In this case, we want to enable specific namespaces (i.e. different applications) to access the Twilio APIs. Add a namespace selector with a key of ‚Äútwilio‚Äù and a value of ‚Äúallowed.‚Äù This policy will only apply to namespaces that have been granted permission to access Twilio with this label - the storefront namespace already has this label.\nUnder Type, select the Egress rule checkbox and deselect the Ingress checkbox. Now let‚Äôs add an Allow egress rule that will whitelist traffic to the Twilio API endpoint. Here we can leverage DNS rules with a wildcard match to ensure that applications will not break if the subdomain for the endpoint changes.\nWhile this egress rule allows traffic to *.twilio.com, we also want to restrict any other egress traffic that is leaving the cluster. We can accomplish this by adding another egress Deny rule that makes use of a Network Set.\nNetwork Sets allow you to create long lists of CIDRs, IPs, or domains and abstract them with a label that can be referenced in network policies. The lab setup has an existing Network Set for all public IP CIDRs. Add it to the egress Deny rule by using the label ‚Äútype=public‚Äù as shown in Fig. 6 below.\nSave and deploy the policy by selecting Enforce. Check out some of the other Network Sets that are available in the lab environment by choosing Network Sets from the left navigation menu.\nGenerating policies for storefront Now that we have defined a policy to enable external access for our k8s platform, let‚Äôs put on our application developer hat and begin to define policies that whitelist only what is required by our storefront application - i.e. a zero-trust and least privileged approach to network security.\nWhile application developers are often familiar with Kubernetes, they may not have the domain expertise to define their own k8s or Calico network policies. Policy Recommendations is a key capability of Calico Enterprise that makes this process easier by automatically generating policies for specific workloads based on their labels and associated network flows.\nReturn to the Flow Visualizer and select the storefront namespace and zoom in on this view. As you mouse over the pod prefixes, notice the ‚Äúmagic wand‚Äù icon on the right - this can be used to generate policy recommendations.\nSelect the frontend-* pod prefix and click the magic wand to generate a network policy for this workload. This will bring you to a Staged Network Policy that has been recommended by looking at the labels and network flows for frontend.\nWe will cover Staged Network Policies in the next section, but for now, go ahead and select STAGE for our frontend policy and repeat this process for microservice1, microservice2, and backend.\nPreviewing and staging policy changes In the last section, we generated network policies for our storefront application using Policy Recommendations. The output of this process was a set of Staged Network Policies. Staged policies are a Calico Enterprise resource that lets you evaluate the impact of your network policies before going to production. Specifically, you can verify that correct flows are being allowed or denied. Let‚Äôs do this for the storefront policies we just generated.\nVerify there are some staged policies for storefront:\nkubectl get stagednetworkpolicies.projectcalico.org -n storefront NAME CREATED AT default.backend-55f7dc6b68 2020-01-10T01:08:09Z default.frontend-6d996fbd97 2020-01-10T01:05:46Z default.microservice1-7599d8d8f 2020-01-10T01:09:25Z default.microservice2-fddf7cff8 2020-01-10T01:10:11Z Note: Even though we have been making heavy use of the Calico Enterprise UI, most of the features in Calico Enterprise are available via an API/command-line to make it easy to integrate these workflows into your CI/CD pipeline.\nLet‚Äôs go back to the Policies Board - you should be able to see the set of staged policies that were listed in the last kubectl command. There is a toggle in the top-level filter to hide or show these policies.\nIn the upper right corner of the Policies Board, find the menu ‚ÄúToggle stats‚Äù icon and check all of the boxes. Now you should have a birds-eye view of how traffic is flowing across tiers and policies - including the ones you have staged. The stats show you that traffic would be allowed by each of these staged policies if and when they are enforced. Along with the Flow Visualizer, the Policies Board is another valuable tool to quickly make sense of traffic within your cluster.\nPolicy impact preview Before we wrap things up, let‚Äôs take a look at another tool in the Calico Enterprise toolbox - Policy Impact Preview. While staged policies allow you to evaluate network policies over a longer period of time, policy impact preview provides a quick sanity check on any change to an existing policy using historical traffic flows.\nGoing back to the default tier, open up the staged policy you just created with the storefront.default.backend prefix and select EDIT. We‚Äôll do something reckless and delete the Egress rule. Now in the upper right select PREVIEW.\nHello again, Flow Visualizer! This time the flow visualizer is highlighting the flows that would be impacted with the policy change we just made - this is indicated by any flows flashing red. Mouse over this flow to see the details on the right-hand side.\nHitting the back arrow in the upper right takes you back to the storefront.default.backend policy, where you can cancel this edit.\nWrapping up As someone from platform engineering, you should have a better understanding of the basics of policy tiers to stand up guard rails and external access for application teams. And as a developer, you learned how Calico Enterprise can help jump start efforts to implement microsegmentation of east-west traffic with policy recommendations specific to your application. Along the way you also touched on some of the visibility and analytics that Calico Enterprise provides.\nStick around and feel free to explore Calico Enterprise in your lab setup.\n"
},
{
	"uri": "/intermediate/210_jenkins/",
	"title": "Deploying Jenkins",
	"tags": ["intermediate", "operations", "ci/cd"],
	"description": "",
	"content": "Deploy Jenkins In this Chapter, we will deploy Jenkins using the Helm package manager we installed in the Helm module and the OIDC identity provider we setup in the [IAM Roles for Service Accounts module]({{ ref \u0026ldquo;beginner/110_irsa\u0026rdquo;}}).\n"
},
{
	"uri": "/beginner/120_network-policies/calico-enterprise/lab-2/",
	"title": "Visibility and Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Getting Started Before you go any further, please remember to type lab2 set_up in the terminal to set up the environment for this lab.\nIntroduction In this lab, you will learn how Calico Enterprise makes it easy for platform engineering and development teams to gain visibility into everything that is happening from a networking perspective within your cluster, quickly diagnose issues, and understand how policies are being evaluated in real-time. The visibility that Calico Enterprise provides is a cornerstone to improving the operations of your container platform and adopting network security best practices.\nYou will learn the following:  How to use the Flow Visualizer to gain visibility into existing traffic flows within the cluster and quickly identify sources of denied traffic How to use the Policy Board and Policy Editor to understand how policies are being evaluated in real-time across Policy Tiers How to use network Flow Logs to look ‚Äúunder the hood‚Äù of your cluster and get a detailed view into how workloads are communicating with all of Kubernetes context to troubleshoot and diagnose issues  Setup For this lab, we will be using a fairly simple microservices application called storefront to understand the visibility and troubleshooting features of Calico Enterprise - if you are already familiar with storefront from another lab then you may want to skip to the next section.\nStorefront (Fig 1) is a fairly simple microservices application running in Kubernetes. It has a frontend service that handles end-user requests, communicates with two business logic services which in turn make requests to a backend service. All of these containers communicate with a logging service and one of the business logic services, microservice 2, makes external requests to Twilio to provide some telephony for this application.\nLet‚Äôs quickly take a look at the pods running in the storefront namespace that are using to run these microservices:\nkubectl get pods -n storefront NAME READY STATUS RESTARTS AGE backend-6cfbdd589f-xxlkn 2/2 Running 0 22h frontend-864f4fcdfd-2hhv4 4/4 Running 0 22h logging-684747d7cd-bbjwx 1/1 Running 0 22h microservice1-794cf77b9d-c27qr 4/4 Running 0 22h microservice2-7bb79d9f4f-f7h6f 5/5 Running 0 22h Using the Flow Visualizer Next, let‚Äôs login to the Calico Enterprise UI and use the Flow Visualizer to understand how these services are communicating with each other. Login to Calico Enterprise and select the Flow Visualizer from the left navigation menu\nThe Flow Visualizer is a powerful tool to gain visibility into network traffic within the cluster and troubleshoot issues. We will look at several different views that the Flow Visualizer provides to understand traffic flows between cluster endpoints that are represented in a circle. Traffic flows are represented as arcs within this circle and the thickness of each arc represents the volume of traffic for that flow.\nNamespace View The default view for the Flow Visualizer is the namespaces view. In this view, the different colors of the Flow Visualizer represent traffic from different namespaces. The outer ring of the Flow Visualizer can be used to select the various namespaces within our cluster. Mousing over each subsection, you‚Äôll see the name of the namespace appear in the dropdown filters on the right-hand side.\n Find the storefront namespace in the outer ring and select it, then use the magnifying glass button in the upper right to zoom in on these network flows. Mousing over subsections of the next inner ring (grey), you will see the pod prefixes for each of the microservices that make up our storefront application. Note that Calico Enterprise aggregates network flows so it is easy to make sense of traffic in your cluster that may be backed by Kubernetes resources like replica sets.  The innermost ring in the Flow Visualizer allows you to select specific network flows that are associated with each of our storefront microservices. Mousing over each subsection, you will see each of these flows on the right-hand side dropdown - these should correspond to the diagram at the beginning of this lab (Fig 1).\nNames View The names view is nearly identical to the namespaces view except that each color within the Flow Visualizer represents different pod prefix names within your cluster. Used in combination with namespace and other filters, this view can be helpful in diagnosing issues within a specific application.\nStatus View The status view differs from other views for the Flow Visualizer in that it only represents flows in two colors - green for allowed traffic and red for denied traffic. The status view is the best way to quickly identify the source of denied traffic that may be unexpected in a dev, test, or production cluster.\n Let‚Äôs see this view in action with a simple example. Run the command lab2 deploy_rogue in your which will deploy a new pod into the cluster. This pod could represent any number of issues within your environment - a misconfiguration or something that went wrong in your CI/CD pipeline, or something that may be indicative of a compromise.  Coming back to the flow visualizer in the status view, you should start to see some denied traffic represented as red flows as the rogue pod begins to reach out to various endpoints within the storefront application.\n Use the right-hand filter to select Status = Denied to view only these flows, and then mouse over the middle ring to the specific endpoints involved in each of these flows.  Filtering The Flow Visualizer can be used in combination with useful filtering capabilities. In addition to some of the drop-down filters we just used in the last section, there are additional filtering capabilities in the top filter bar.\nExplore the top filter bar and understand the effects of different filters for time range, source and destination labels, and different endpoint types.\nEndpoint types include the following:\n Network - public and private IP addresses Network Set - Calico resources that allows you to abstract long lists of IPs, domains, CIDR ranges, etc. with labels (which can also be used in Calico network policies) HEP - host endpoints that use host networking WEP - workload endpoints; VMs and pods that use virtual networking  Using the Policy Board Now that we have a better understanding of the microservices that make up our storefront application and how they are communicating with each other, let‚Äôs take a look at another tool that can provide a useful way to understand how policies are being evaluated on these network flows - the Policy Board.\nPolicy Tiers Calico Enterprise makes it easy to define the ‚Äúguard rails‚Äù for your Kubernetes platform through the use of Policy Tiers. Policy Tiers allow platform engineering and security teams to enforce network policies that take precedence over those defined for specific applications like storefront.\nReturn to the Calico Enterprise UI and select Policies from the left navigation menu. Here you will see all of the network policies defined within the cluster and how they map to policy tiers.\nTiers are evaluated from left to right, and network policies within tiers are evaluated from top to bottom. This effectively means that a network policy in the Security tier (Fig. 4) needs to evaluate and pass traffic before any policy below it or to the right can see that same traffic. Tiers are tied to RBAC and provide a powerful way to implement security and platform controls for your entire cluster without having to involve application teams. Your lab environment already has a few policies in the Platform and Security tiers that provide some examples of some common use cases.\nEnabling Metrics Let‚Äôs enable some additional metrics in the Policy Board so we can gain visibility into how these policies are being evaluated. In the upper right-hand corner click the ‚Äúeye‚Äù icon and select ‚ÄúShow All‚Äù to show all the metrics on the Policy Board.\nYou should now see an additional set of metrics for each policy on the Policy Board - Connections, Allowed, Denied, and Passed. Passed traffic occurs in a policy that is at the end of a tier, and shows you the volume of traffic that is being passed to the next tier.\nYou should also see some denied traffic that is coming from the rogue pod that we deployed earlier in this lab. There are some network policies in the default tier that implement a zone-based architecture for the workloads in the storefront application - dmz, trusted, and restricted. In the next section we will take a closer look at how the rules within these policies are evaluating network traffic.\nPolicy View Metrics Let‚Äôs take a closer look at the denied traffic in the default.trusted policy by selecting the view/eye icon. Looking at a single policy, you can see the level of detailed visibility that Calico Enterprise provides on a per rule basis. This policy allows ingress traffic from dmz, allows workloads within the trusted zone to communicate with each other, and allows egress traffic out to the restricted zone.\nThe metrics displayed alongside policy rules makes it easy to diagnose issues with network policies and denied traffic. In this case, we can see that denied traffic is on the ingress rules of this policy because the rogue pod we deployed is not part of the dmz or trusted zones.\nUsing Flow Logs While the Policy Board and Flow Visualizer provide powerful ways to understand the traffic flows within your cluster, there can be situations where you need to access a greater level of detail to troubleshoot network connectivity issues. Let‚Äôs take a look ‚Äúunder the hood‚Äù at some of the raw flow log data that Calico Enterprise can generate.\n In the Calico Enterprise UI, select Kibana from the left navigation menu and login using the Kibana user credentials that were provided as part of your lab setup. Calico Enterprise includes a fully integrated deployment of Elastic to collect flow log data that drives a number of key capabilities like the Flow Visualizer, metrics in the dashboard and Policy Board, policy automation and testing features, and compliance and security.   Select Dashboard from the left navigation menu and then scroll down to see a tabular view of flow log data that has been generated by Calico Enterprise. Kibana provides its own set of powerful filtering capabilities to quickly drill into this data. Mouse over the source_namespace column and select the ‚ÄòFilter for value‚Äô icon next to one of the rows for storefront. Repeat this same selection for dest_namespace.  Now we are just looking at the flow log data within our storefront application. Expand one of these rows using the caret/arrow on the far left hand side.\nTake some time to review all of the fields that are provided in a single flow log entry. While there are over 30 different fields in each entry, some of the most useful in the context of troubleshooting are the following:\n source_name_aggr and dest_name_aggr provide the pod prefix for the aggregated flows between workloads, making it easy to make sense of endpoints that may be backed by replica sets policies.all_policies shows the tier order|tier|policy that applied to this flow and the result of evaluation - allow, deny, or pass source_labels.labels and dest_labels.labels which lists the Kubernetes labels that apply to the respective source and destination endpoints  These raw flow logs, with all of the Kubernetes context included, provides yet another valuable tool to gain visibility into your cluster and quickly make sense of network traffic to troubleshoot and diagnose issues.\nWrapping up One of the most foundational components to operating your Kubernetes platform is having tools that allow you to gain visibility into a cluster and understand how workloads are communicating with each other. In this lab, you used the capabilities of Calico Enterprise to visualize network traffic with the Flow Visualizer, understand policy evaluation with metrics in the Policy Board and policy editor, and took a look ‚Äúunder the hood‚Äù to see how much Kubernetes context that is preserved in flow logs - all of which help you troubleshoot and diagnose issues more quickly.\nStick around and feel free to explore Calico Enterprise in your lab setup.\n"
},
{
	"uri": "/intermediate/220_codepipeline/",
	"title": "CI/CD with CodePipeline",
	"tags": ["advanced", "operations", "ci/cd", "CON205"],
	"description": "",
	"content": "CI/CD with CodePipeline Continuous integration (CI) and continuous delivery (CD) are essential for deft organizations. Teams are more productive when they can make discrete changes frequently, release those changes programmatically and deliver updates without disruption.\nIn this module, we will build a CI/CD pipeline using AWS CodePipeline. The CI/CD pipeline will deploy a sample Kubernetes service, we will make a change to the GitHub repository and observe the automated delivery of this change to the cluster.\n"
},
{
	"uri": "/beginner/120_network-policies/calico-enterprise/lab-3/",
	"title": "Implementing Existing Security Controls in Kubernetes",
	"tags": [],
	"description": "",
	"content": "Getting Started Before you go any further, please remember to type lab3 set_up in the terminal to set up the environment for this lab.\nIntroduction In this lab, you will learn how Calico Enterprise makes it easy for security and compliance teams to extend existing enterprise controls to Kubernetes. The dynamic nature of workloads within a cluster presents a number of challenges for security and compliance practitioners. Calico Enterprise addresses these challenges with a tool set that extends Kubernetes itself and understands the declarative metadata within your cluster.\nYou will learn the following:  How to use declarative and label-based Calico Network Policies to enforce zone-based controls and security requirements How to use Policy Tiers as way to enforce higher precedent security and compliance policies that cannot be circumvented by application and development teams How to define Compliance Reports that leverage Kubernetes constructs and can be run on a continuous basis in lock step with your CI/CD pipeline How to use audit logs to monitor and alert on changes to network policies with git-friendly diffs  Setup For this lab, we will be using a fairly simple microservices application called storefront to understand the visibility and troubleshooting features of Calico Enterprise.\nStorefront (Fig 1) is a fairly simple microservices application running in Kubernetes. It has a frontend service that handles end-user requests, communicates with two business logic services which in turn make requests to a backend service. All of these containers communicate with a logging service and one of the business logic services, microservice 2, makes external requests to Twilio to provide some telephony for this application.\n Let‚Äôs quickly take a look at the pods (and their labels) running in the storefront namespace  kubectl get pods -n storefront --show-labels NAME LABELS backend-55f7dc6b68-8n9cc app=backend,fw-zone=restricted,pod-template-hash=55f7dc6b68 frontend-6d996fbd97-9g92v app=frontend,fw-zone=dmz,pod-template-hash=6d996fbd97 logging-7bc4bfcff5-5x49j app=logging,pod-template-hash=7bc4bfcff5 microservice1-7599d8d8f-4mc98 app=microservice1,fw-zone=trusted,pod-template-hash=7599d8d8f microservice2-fddf7cff8-fggd2 app=microservice2,fw-zone=trusted,pod-template-hash=fddf7cff8 Notice that there are some existing labels on the pods that make up our storefront application. Later in this lab we will use these labels to implement network policies in Calico Enterprise. But before we do that, we need to quickly understand how Policy Tiers work in Calico Enterprise.\nUsing the Policy Board The Policy Board can provide a useful way to understand how policies are being evaluated across policy tiers on network flows in the cluster.\n Login to the Calico Enterprise UI and select Policies from the left navigation menu. Here you will see the Policy Board with all of the network policies defined within the cluster and how they map to policy tiers.  Policy Tiers Calico Enterprise makes it easy to define the ‚Äúguard rails‚Äù for your Kubernetes platform through the use of Policy Tiers. Policy Tiers allow platform engineering and security teams to enforce network policies that take precedence over those defined for specific applications like storefront.\nTiers are evaluated from left to right, and network policies within tiers are evaluated from top to bottom. This effectively means that a network policy in the Security tier (Fig. 2) needs to evaluate and pass traffic before any policy below it or to the right can see that same traffic. Tiers are tied to RBAC and provide a powerful way to implement security and platform controls for your entire cluster without having to involve application teams. Your lab environment already has a few policies in the platform and security tiers that provide some examples of some common use cases. Policies specific to an application would typically go in the default tier.\nIn the next section, we will begin to implement policies in the default tier for our storefront application.\nImplementing Zone-based Network Policies One of the most widely adopted deployment models with traditional firewalls is using a zone-based architecture. This involves putting the frontend of an application in a DMZ, business logic services in Trusted zone, and our backend data store in Restricted - all with controls on how zones can communicate with each other. For our storefront application, it would look something like the following:\nUsing the labels that already exist on our pods, let‚Äôs start by creating a network policy for the DMZ zone. Select Policies from the left navigation menu. Then select +ADD POLICY at the bottom of the default tier.\nDMZ zone   Create a DMZ policy with the following attributes:\n Name the policy dmz and insert before tigera-compliance.default-deny Set the Scope to Namespace = storefront In the Applies To section, add a label selector for fw-zone=dmz Under Type select both Ingress and Egress    Add the following Ingress rules:\n Allow - Any protocol - From: Endpoint selector type=public Deny - Any protocol    Add the following Egress rules:\n Allow - Any protocol - To: Endpoint selector fw-zone=trusted OR app=logging Deny - Any protocol    Save the policy by selecting Enforce\n  The storefront.dmz policy you just created should be at the top of the default tier. If for some reason it is not, you can drag the policy to the top to change its order within the tier.\nTrusted zone  Now let‚Äôs +ADD POLICY again and create a Trusted policy with the following attributes:  Name the policy trusted and choose to add after storefront.dmz Set the Scope to Namespace = storefront In the Applies To section, add a label selector for fw-zone=trusted Under Type select both Ingress and Egress Add the following Ingress rules:  Allow - Any protocol - From: Endpoint selector fw-zone=dmz OR fw-zone=trusted Deny - Any protocol   Add the following Egress rules:  Allow - Any protocol - To: Endpoint selector fw-zone=restricted Deny - Any protocol     Save the policy by selecting Enforce  Restricted zone  Finally, let‚Äôs create a Restricted policy with the following attributes:  Name the policy restricted and choose to add after storefront.trusted Set the Scope to Namespace = storefront In the Applies To section, add a label selector for fw-zone=restricted Under Type select both Ingress and Egress Add the following Ingress rules:  Allow - Any protocol - From: Endpoint selector fw-zone=trusted OR fw-zone=restricted Deny - Any protocol   Add the following Egress rules:  Allow - Any protocol     Save the policy by selecting Enforce  The three policies we just created implement the controls that govern how workloads can communicate within and across each of these zones. As we add new microservices to storefront, placement within one of these zones is as simple as assigning the appropriate label to a pod.\n Check your work by going to the Dashboard in the left navigation menu and verify that the per policy bar graph is only showing green for allowed traffic, and not red for denied traffic. If there is a policy that is denying traffic, revisit that policy and view the real-time metrics for each rule to determine the source of your denied traffic.  Network Sets In the DMZ policy, we used a label type=public in the Ingress rule. This label references a Calico resource called a Network Set. Network Sets allow you to create long lists of CIDRs, IPs, or domains and abstract them with a label that can be referenced in network policies. The lab setup has an existing Network Set for all public IP CIDRs (‚Äútype=public‚Äù). Feel free to explore additional network sets under the left navigation menu.\nEnforcing Compliance with Network Policies As we touched on earlier, Policy Tiers can be a useful tool for defining the ‚Äúguard rails‚Äù for your entire Kubernetes platform. This allows operators to define security and compliance controls without having to interfere with the work of application developers. Given that tiers can also be controlled via RBAC, it also ensures that developers cannot inadvertently circumvent these controls. In this section we will leverage Policy Tiers and Calico‚Äôs ability to reference service accounts in network policies to implement controls for PCI compliance in our storefront application - or really any application in the cluster.\nService Accounts Calico network policies can be applied to endpoints using selectors that match labels on the endpoint, the endpoint‚Äôs namespace, or the endpoint‚Äôs service account. By applying selectors based on the endpoint‚Äôs service account, you can use Kubernetes RBAC to control which users can assign labels to service accounts. This allows you to separate groups who can deploy pods from those who can assign labels to service accounts.\nFor the purposes of this lab, we have already setup each of our microservices with service accounts. Service accounts with privileges to access workloads with payment card data have been labeled as ‚ÄúPCI=true‚Äù.\nLet‚Äôs create a network policy that will enforce that pods cannot communicate across the regulatory boundary we have setup with our PCI-labeled service accounts.\n Visit the Policy Board and select +ADD POLICY at the bottom of the security tier and create a new network policy with the following details:  Name the policy pci-whitelist and insert before global.pass Scope of the policy should be Global In the Applies To section, use the service account label selector of PCI=true In the Type section, make sure the Ingress and Egress are both selected Add an Ingress rule and select the ‚ÄòAdvanced mode‚Äô checkbox in the upper right  Deny - Any Protocol - To: service account label PCI=true, From: service account label where PCI!=true   Add an Egress rule with Pass - Any Protocol - To: Endpoint selector label k8s-app=kube-dns Add an Egress rule that Pass - Any Protocol - To: Endpoint selector label type=public Add an Egress rule and select the ‚ÄòAdvanced mode‚Äô checkbox in the upper right  Deny - Any Protocol - To: service account label where PCI!=true, From: service account label PCI=true     Save the policy by selecting Enforce  This policy ensures that only pods with PCI-labeled service accounts can talk to each other. For traffic that does not involve a PCI-labeled service account, we use the Pass action to ‚Äúpass‚Äù this traffic to the next tier of Calico network policies.\nCompliance Reports One of the challenges that many organizations face is being able to report the state of compliance across the infrastructure they use to run their business. This problem is compounded by the dynamic and ephemeral nature of Kubernetes - with new applications and services being created, deleted, and scaled in every dimension, how can organizations provide attestations as to the state of compliance?\nIn this section we will walk through a quick example of how to use Calico Enterprise to produce dynamic compliance reports that allow you to assess the state of compliance that is in lock step with your CI/CD pipeline.\nCIS benchmarks are best practices for the secure configuration of a target system - in our case Kubnernetes. Calico Enterprise supports a number of GlobalReport types that can be used for continuous compliance, and CIS benchmarks is one of them. Below is an example report definition:\napiVersion: projectcalico.org/v3 kind: GlobalReport metadata: name: daily-cis-results labels: deployment: production spec: reportType: cis-benchmark schedule: 0 0 * * * cis: highThreshold: 100 medThreshold: 50 includeUnscoredTests: true numFailedTests: 5 resultsFilters: - benchmarkSelection: { kubernetesVersion: \u0026#34;1.15\u0026#34; } exclude: [\u0026#34;1.1.4\u0026#34;, \u0026#34;1.2.5\u0026#34;] In the report there are a number of useful parameters, including a recurring schedule that uses standard cron syntax. These reports can also be run on-demand whenever there are changes to your Kubernetes cluster. You also have the ability to select a specific benchmark suite for your version of Kubernetes, and exclude specific benchmarks based on your environment.\n  Visit the Compliance Reports in the left navigation of the Calico Manager UI. Your lab environment already has a few instances of the daily-cis-results report that has been run, and a visual summary of the results\n  Expand one of the daily-cis-results rows and click the download icon on the right to see a more detailed CSV results for this report.\n  Explore some of the other report types (and the detailed CSVs) that have been run in the lab setup. This includes Inventory, Network Access, and Policy Audit.\n  Auditing Another important security and compliance requirement for many organizations is auditing - understanding what changed, when, and how made that change. Calico Enterprise provides detailed audit logs for all changes related to network policy and other resources associated with your Calico deployment. Let‚Äôs take a look.\n  Select Policies from the left navigation menu in the Calico Enterprise UI and go back to any one of the policies you created in the default tier earlier in this lab. Edit the policy and add a label of your choice as an OR with any existing labels. Select Enforce to save and deploy the policy.\n  Select the policy to edit again and scroll down to the bottom of the policy to see the Change log. Here you can see a detailed audit log of policy changes.\n  Expanding the most recent entry will show you the yaml for the version of the policy you just created, and selecting the diff button on the right will show you the git-friendly diff compared with the previous version.\n    Go back to the left navigation menu and select Kibana - now we will go take a look at all of the audit log data that Calico Enterprise generates. Select Dashboard from the left nav in Kibana and then ‚ÄúTigera Secure EE Audit Logs‚Äù\n  Explore some of the audit log entries in Kibana and the detail that is provided for each entry. Any of this data can be used to generate alerts for use cases like monitoring production environments, privileged access monitoring, and many others.\n  Wrapping Up One of the most fundamental components to operating your Kubernetes platform is having tools that allow you to implement the same security and compliance requirements that exist for your traditional infrastructure. In this lab, you used the capabilities of Calico Enterprise to implement zone-based policies, leverage service accounts and high precedence tiers to enforce PCI compliance, and took a look at some of the robust compliance reporting and auditing.\nStick around and feel free to explore Calico Enterprise in your lab setup.\n"
},
{
	"uri": "/intermediate/230_logging/",
	"title": "Logging with Elasticsearch, Fluent Bit, and Kibana (EFK)",
	"tags": ["intermediate", "operations", "logging", "CON206"],
	"description": "",
	"content": "Implement Logging with EFK In this Chapter, we will deploy a common Kubernetes logging pattern which consists of the following:\n  Fluent Bit: an open source and multi-platform Log Processor and Forwarder which allows you to collect data/logs from different sources, unify and send them to multiple destinations. It\u0026rsquo;s fully compatible with Docker and Kubernetes environments.\n  Amazon Elasticsearch Service: a fully managed service that makes it easy for you to deploy, secure, and run Elasticsearch cost effectively at scale.\n  Kibana: an open source frontend application that sits on top of the Elasticsearch, providing search and data visualization capabilities for data indexed in Elasticsearch.\n  Together, Fluent Bit, Elasticsearch and Kibana is also known as \u0026ldquo;EFK stack\u0026rdquo;.\nFluent Bit will forward logs from the individual instances in the cluster to a centralized logging backend where they are combined for higher-level reporting using ElasticSearch and Kibana.\n"
},
{
	"uri": "/intermediate/240_monitoring/",
	"title": "Monitoring using Prometheus and Grafana",
	"tags": ["intermediate", "operations", "monitoring", "CON206"],
	"description": "",
	"content": "Monitoring using Prometheus and Grafana In this Chapter, we will deploy Prometheus and Grafana to monitor Kubernetes cluster\nWhat is Prometheus? Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. Since its inception in 2012, many companies and organizations have adopted Prometheus, and the project has a very active developer and user community. It is now a standalone open source project and maintained independently of any company. Prometheus joined the Cloud Native Computing Foundation in 2016 as the second hosted project, after Kubernetes.\nWhat is Grafana? Grafana is open source visualization and analytics software. It allows you to query, visualize, alert on, and explore your metrics no matter where they are stored. In plain English, it provides you with tools to turn your time-series database (TSDB) data into beautiful graphs and visualizations.\n"
},
{
	"uri": "/intermediate/241_pixie/",
	"title": "Monitoring using Pixie",
	"tags": ["intermediate", "operations", "monitoring", "debugging"],
	"description": "",
	"content": "Monitoring using Pixie In this chapter, we will deploy Pixie to monitor an application on a Kubernetes cluster.\nWhat is Pixie? Pixie is an open-source observability platform for Kubernetes. It helps developers explore, monitor and debug their applications. Pixie‚Äôs features include:\nProgressive instrumentation\nPixie collects full-body request traces, system resource metrics, and Kubernetes events right out of box. Pixie\u0026rsquo;s auto-instrumentation capabilities require no code changes by the user and consume less than 5% overhead, because it powered by eBPF*. Users are also able to augment Pixie‚Äôs default instrumentation to collect custom metrics, traces, and logs.\nIn-cluster edge compute and storage\nPixie performs all data storage and computation entirely within a user‚Äôs Kubernetes cluster. This architecture allows the user to isolate data storage and computation within their environment for finer-grained context, faster performance, and a greater level of data security.\nProgrammatic data access\nPxL scripts are the API for querying data in Pixie. Users write PxL scripts to query and analyze their data, making exploration, debugging and analysis more efficient. Pixie‚Äôs UI, CLI, and API all accept PxL scripts as their input, so the same script can be reused at any Pixie interface. Pixie ships with a rich set of pre-built PxL scripts contributed by the open source community.\n* To learn more about the magic of eBPF, check out Brendan Gregg\u0026rsquo;s re:Invent 2019 talk covering BPF and its comprehensive usage at Netflix.\n"
},
{
	"uri": "/intermediate/245_x-ray/",
	"title": "Tracing with X-Ray",
	"tags": ["advanced", "operations", "monitoring", "CON205"],
	"description": "",
	"content": "Tracing with X-Ray As distributed systems evolve, monitoring and debugging services becomes challenging. Container-orchestration platforms like Kubernetes solve a lot of problems, but they also introduce new challenges for developers and operators in understanding how services interact and where latency exists. AWS X-Ray helps developers analyze and debug distributed services.\nIn this module, we are going to deploy the X-Ray agent as a DaemonSet, deploy sample front-end and back-end services that are instrumented with the X-Ray SDK, make some sample requests and then examine the traces and service maps in the AWS Management Console.\n"
},
{
	"uri": "/intermediate/246_monitoring_amp_amg/",
	"title": "Monitoring using Amazon Managed Service for Prometheus / Grafana",
	"tags": ["intermediate", "operations", "monitoring", "AMP", "AMG", "Amazon Managed Service for Prometheus", "Amazon Managed Service for Grafana"],
	"description": "",
	"content": "Introduction Amazon Managed Service for Prometheus (AMP) Amazon Managed Service for Prometheus is a monitoring service for metrics compatible with the open source Prometheus project, making it easier for you to securely monitor container environments. AMP is a solution for monitoring containers based on the popular Cloud Native Computing Foundation (CNCF) Prometheus project. AMP is powered by Cortex, an open source CNCF project that adds horizontal scalability to ingest, store, query, and alert on Prometheus metrics. AMP reduces the heavy lifting required to get started with monitoring applications across Amazon Elastic Kubernetes Service and Amazon Elastic Container Service, as well as self-managed Kubernetes clusters. AMP automatically scales as your monitoring needs grow. It offers highly available, multi-Availability Zone deployments, and integrates AWS security and compliance capabilities. AMP offers native support for the PromQL query language as well as over 150+ Prometheus exporters maintained by the open source community.\nLearn more about AMP   Amazon Managed Service for Grafana (AMG) Amazon Managed Service for Grafana is a fully managed service with rich, interactive data visualizations to help customers analyze, monitor, and alarm on metrics, logs, and traces across multiple data sources. You can create interactive dashboards and share them with anyone in your organization with an automatically scaled, highly available, and enterprise-secure service. With Amazon Managed Service for Grafana, you can manage user and team access to dashboards across AWS accounts, AWS regions, and data sources. Amazon Managed Service for Grafana provides an intuitive resource discovery experience to help you easily onboard your AWS accounts across multiple regions and securely access AWS services such as Amazon CloudWatch, AWS X-Ray, Amazon Elasticsearch Service, Amazon Timestream, AWS IoT SiteWise, and Amazon Managed Service for Prometheus.\nLearn more about AMG   "
},
{
	"uri": "/intermediate/250_cloudwatch_container_insights/",
	"title": "EKS CloudWatch Container Insights",
	"tags": ["intermediate", "operations", "monitoring", "CON206"],
	"description": "",
	"content": "In this chapter we will learn and leverage the new CloudWatch Container Insights to see how you can use native CloudWatch features to monitor your EKS Cluster performance.\nYou can use CloudWatch Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights is available for Amazon Elastic Container Service, Amazon Elastic Kubernetes Service, and Kubernetes platforms on Amazon EC2. The metrics include utilization for resources such as CPU, memory, disk, and network. Container Insights also provides diagnostic information, such as container restart failures, to help you isolate issues and resolve them quickly.\nIn order to complete this lab you will need to have a working EKS Cluster, With Helm installed. You will need to have completed the Start the Workshop\u0026hellip; through Launching your cluster with Eksctl and Install Helm CLI as well.\n To learn all about our Observability features using Amazon CloudWatch and AWS X-Ray, take a look at our One Observability Workshop\n "
},
{
	"uri": "/intermediate/260_weave_flux/",
	"title": "GitOps with Weave Flux",
	"tags": ["advanced", "operations", "ci/cd", "gitops"],
	"description": "",
	"content": "GitOps with Weave Flux GitOps, a term coined by Weaveworks, is a way to do continuous delivery. Git is used as single source of truth for deploying into your cluster. This is easy for a development team as they are already familiar with git and do not need to know other tools. Weave Flux is a tool that runs in your Kubernetes cluster and implements changes based on monitoring Git and image repositories.\nIn this module, we will create a Docker image build pipeline using AWS CodePipeline for a sample application in a GitHub repository. We will then commit Kubernetes manifests to GitHub and monitor Weave Flux managing the deployment.\nBelow is a diagram of what will be created:\n"
},
{
	"uri": "/intermediate/290_argocd/",
	"title": "Continuous Deployment with ArgoCD",
	"tags": ["intermediate", "cd", "gitops"],
	"description": "",
	"content": "Continuous Deployment with ArgoCD [Argo CD] (https://argoproj.github.io/argo-cd/) is a declarative, GitOps continuous delivery tool for Kubernetes. The core component of Argo CD is the Application Controller, which continuously monitors running applications and compares the live application state against the desired target state defined in the Git repository. This powers the following use cases:\nAutomated deployment : controller pushes the desired application state into the cluster automatically, either in response to a Git commit, a trigger from CI pipeline, or a manual user request.\nObservability : developers can quickly find if the application state is in sync with the desired state. Argo CD comes with a UI and CLI which helps to quickly inspect the application and find differences between the desired and the current live state.\nOperation : Argo CD UI visualizes the entire application resource hierarchy, not just top-level resources defined in the Git repo. For example, developers can see ReplicaSets and Pods produced by the Deployment defined in Git. From the UI, you can quickly see Pod logs and the corresponding Kubernetes events. This turns Argo CD into very powerful multi-cluster dashboard.\n"
},
{
	"uri": "/intermediate/265_spinnaker_eks/",
	"title": "Continuous Delivery with Spinnaker",
	"tags": ["intermediate", "cd", "spinnnaker", "devops"],
	"description": "",
	"content": "Continuous Delivery with Spinnaker Spinnaker is an open-source, multi-cloud continuous delivery platform, originally developed by Netflix, that helps you release software changes rapidly and reliably. Development team can focus on just application development and leave ops provisioning to Spinnaker for automating reinforcement of business and regulatory requirements. Spinnaker supports several CI systems and build tools like CodeBuild, Jenkins. You can integrate Spinnaker for configuring Artifacts from Git, Amazon S3, Amazon ECR etc.\nIn this workshop, we will focus on Application Deployment using Spinnaker:\n How to install Spinnaker and configure Spinnaker services using Kubernetes Operator in EKS Build simple Spinnaker CD pipeline  Stage - Add Ngnix deployment artifact manifest Testing -  Manually trigger the pipeline Test the deployment     Build Helm-based Spinnaker CD Pipeline  Configuration  Trigger: Continuous Delivery will be triggered automatically based on new image upload into ECR   Stage 1 - Bake the manifest from GitHub repo for deployment using Helm Stage 2 - Deploy the Helm artifact to EKS Testing -  Upload container image to ECR to trigger the deployment pipeline Test the deployment      "
},
{
	"uri": "/intermediate/270_custom_resource_definition/",
	"title": "Custom Resource Definition",
	"tags": ["advanced", "operations", "crd"],
	"description": "",
	"content": "Custom Resource Definition Introduction In this Chapter, we will review the Custom Resource Definition (CRD) concept, and some examples of usage.\nIn Kubernetes API, a resource is an endpoint storing the API objects in a collection. As an example, the pods resource contains a collection of Pod objects.\nCRD‚Äôs are extensions of Kubernetes API that stores collection of API objects of certain kind. They extend the Kubernetes API or allow you to add your own API into the cluster.\nTo create a CRD, you need to create a file, that defines your object kinds and lets the API Server manage the lifecycle. Applying a CRD into the cluster makes the Kubernetes API server to serve the specified custom resource.\nWhen a CRD is created, the Kubernetes API creates a new RESTful resource path, that can be accesed by a cluster or a single namespace.\n"
},
{
	"uri": "/beginner/060_helm/helm_nginx/addbitnamirepo/",
	"title": "Add the Bitnami Repository",
	"tags": [],
	"description": "",
	"content": "In the last slide, we saw that nginx offers many different products via the default Helm Chart repository, but the nginx standalone web server is not one of them.\nAfter a quick web search, we discover that there is a Chart for the nginx standalone web server available via the Bitnami Chart repository.\nTo add the Bitnami Chart repo to our local list of searchable charts:\nhelm repo add bitnami https://charts.bitnami.com/bitnami Once that completes, we can search all Bitnami Charts:\nhelm search repo bitnami Which results in:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/bitnami-common 0.0.9 0.0.9 DEPRECATED Chart with custom templates used in ... bitnami/airflow 10.2.5 2.1.2 Apache Airflow is a platform to programmaticall... bitnami/apache 8.5.8 2.4.48 Chart for Apache HTTP Server ...  Search once again for nginx\nhelm search repo nginx Now we are seeing more nginx options, across both repositories:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 9.3.7 1.21.1 Chart for the nginx server bitnami/nginx-ingress-controller 7.6.16 0.48.1 Chart for the nginx Ingress controller stable/nginx-ingress 1.41.3 v0.34.1 DEPRECATED! An nginx Ingress controller that us...  Or even search the Bitnami repo, just for nginx:\nhelm search repo bitnami/nginx Which narrows it down to nginx on Bitnami:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 9.3.7 1.21.1 Chart for the nginx server bitnami/nginx-ingress-controller 7.6.16 0.48.1 Chart for the nginx Ingress controller  In both of those last two searches, we see\nbitnami/nginx  as a search result. That\u0026rsquo;s the one we\u0026rsquo;re looking for, so let\u0026rsquo;s use Helm to install it to the EKS cluster.\n"
},
{
	"uri": "/intermediate/300_cis_eks_benchmark/",
	"title": "CIS EKS Benchmark assessment using kube-bench",
	"tags": ["intermediate", "cis eks benchmark", "kube-bench"],
	"description": "",
	"content": "CIS EKS Benchmark assessment using kube-bench   Security is a critical component of configuring and maintaining Kubernetes clusters and applications. Amazon EKS provides secure, managed Kubernetes clusters by default, but you still need to ensure that you configure the nodes and applications you run as part of the cluster to ensure a secure implementation.\nSince CIS Kubernetes Benchmark provides good practice guidance on security configurations for Kubernetes clusters, customers asked us for guidance on CIS Kubernetes Benchmark for Amazon EKS to meet their security and compliance requirements.\nIn this chapter, we take a look at how to assess the Amazon EKS cluster nodes you have created against the CIS EKS Kubernetes benchmark.\n"
},
{
	"uri": "/intermediate/310_opa_gatekeeper/",
	"title": "Using Open Policy Agent (OPA) for policy-based control in EKS",
	"tags": ["intermediate", "open policy agent", "opa"],
	"description": "",
	"content": "Using Open Policy Agent (OPA) for policy-based control in EKS   Security and governance is a critical component of configuring and managing fine-grained control for Kubernetes clusters and applications. Amazon EKS provides secure, managed Kubernetes clusters by default, but you still need to ensure that you configure and administer the applications appropriately that you run as part of the cluster.\nThe Open Policy Agent (OPA, pronounced ‚Äúoh-pa‚Äù) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that let‚Äôs you specify policy as code and simple APIs to offload policy decision-making from your software.\n AWS Blogs on OPA: Using Open Policy Agent on Amazon EKS Realize Policy-as-Code with AWS Cloud Development Kit through Open Policy Agent OCI Artifact Support in Amazon ECR  In this chapter, we take a look at how to implement OPA on an Amazon EKS cluster and take a look at a scenario to restrict container images from an approved ECR repository using a OPA policy.\n"
},
{
	"uri": "/advanced/350_opentelemetry/",
	"title": "Observability with AWS Distro for Open Telemetry",
	"tags": ["advanced", "observability", "metrics", "traces", "AWS Distro for Open Telemetry"],
	"description": "",
	"content": "Observability with AWS Distro for Open Telemetry Observability for containerized workloads is essential for business critical environments. In this module, we will configure an OpenTelemetry (OTEL) collector that gathers metrics and traces. With the OTEL collector configured with the following AWS Services: Amazon CloudWatch, Container Insights, Amazon Managed Service for Prometheus (AMP), Amazon Managed Grafana (AMG).\n"
},
{
	"uri": "/beginner/300_windows/considerations/",
	"title": "Considerations",
	"tags": [],
	"description": "",
	"content": "Before deploying Windows nodes, be aware of the following considerations.\n Windows workloads are supported with Amazon EKS clusters running Kubernetes version 1.14 or later. Amazon EC2 instance types C3, C4, D2, I2, M4 (excluding m4.16xlarge), and R3 instances are not supported for Windows workloads. Host networking mode is not supported for Windows workloads. Amazon EKS clusters must contain one or more Linux nodes to run core system pods that only run on Linux, such as coredns and the VPC resource controller. The kubelet and kube-proxy event logs are redirected to the EKS Windows Event Log and are set to a 200 MB limit. Windows nodes support one elastic network interface per node. The number of pods that you can run per Windows node is equal to the number of IP addresses available per elastic network interface for the node\u0026rsquo;s instance type, minus one. For more information, see IP addresses per network interface per instance type in the Amazon EC2 User Guide for Linux Instances. Group Managed Service Accounts (GMSA) for Windows pods and containers is not supported by Amazon EKS versions earlier than 1.16. You can follow the instructions in the Kubernetes documentation to enable and test this alpha feature on clusters that are earlier than 1.16.  GMSA use case is not covered in this lab but you can find more information in this blog post.\n "
},
{
	"uri": "/advanced/310_servicemesh_with_istio/",
	"title": "Service Mesh with Istio",
	"tags": ["advanced", "operations", "servicemesh"],
	"description": "",
	"content": "A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It‚Äôs responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application.\nService mesh solutions have two distinct components that behave somewhat differently:\n The¬†data planeis composed of a set of intelligent proxies (Envoy) deployed as sidecars. These proxies mediate and control all network communication between microservices along with¬†Mixer, a general-purpose policy and telemetry hub. The¬†control planemanages and configures the proxies to route traffic. Additionally, the control plane configures Mixers to enforce policies and collect telemetry.  "
},
{
	"uri": "/beginner/300_windows/windows_nodes/",
	"title": "Windows nodes",
	"tags": [],
	"description": "",
	"content": "Enable Windows support This procedure only works for clusters that were created with eksctl and assumes that your eksctl version is 0.24.0 or later.\n You can check your version with the following command\neksctl version The next command will deploy the VPC resource controller and VPC admission controller webhook that are required on Amazon EKS clusters to run Windows workloads.\neksctl utils \\  install-vpc-controllers \\  --cluster eksworkshop-eksctl \\  --approve Launch self-managed Windows nodes Create your node group with the following command\nmkdir ~/environment/windows cat \u0026lt;\u0026lt; EoF \u0026gt; ~/environment/windows/windows_nodes.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} nodeGroups: - name: windows-ng amiFamily: WindowsServer2004CoreContainer desiredCapacity: 1 instanceType: t2.large ssh: enableSsm: true EoF eksctl create nodegroup -f ~/environment/windows/windows_nodes.yaml You can verify that the Windows node has been added to your cluster by using the command line\nkubectl get nodes -l kubernetes.io/os=windows -L kubernetes.io/os Notice the Operating system in the OS column\nNAME STATUS ROLES AGE VERSION OS ip-192-168-95-199.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 6h33m v1.17.6-eks-4e7f64 windows  Or by using the AWS EC2 console\n"
},
{
	"uri": "/intermediate/320_eks_upgrades/",
	"title": "Patching/Upgrading your EKS Cluster",
	"tags": ["intermediate", "operations"],
	"description": "",
	"content": "Patching/Upgrading your EKS Cluster As EKS tracks upstream Kubernetes that means that customers can, and should, regularly upgrade their EKS so as to stay within the project\u0026rsquo;s upstream support window. This used to be the current version and two version back (n-2) - but it was recently extended to three versions back (n-3).\nThere is a new major version of Kubernetes every quarter which means that the Kubernetes support window has now gone from three quarters of a year to one full year.\nIn addition to upgrades to Kuberentes, there are other related upgrades to think about with your cluster as well:\n The Amazon Machine Image (AMI) of your Nodes - including not just the portion of Kubernetes that is part of the image, the kubelet, but everything else there (OS, containerd, etc.). The control plane always supports managing kubelets that are one version behind itself (n-1) to help facilitate this upgrade. The foundational DaemonSets that are on deployed onto every EKS cluster (kube-proxy, CoreDNS and the AWS CNI) which may need to be upgraded as you upgrade Kubernetes. Our documentation tells you if this is required and which versions you should upgrade to. And any Add-ons/Controllers/Drivers that you\u0026rsquo;ve added to extend Kubernetes and provide important cluster functionality may need to be upgraded as you upgrade Kuberentes  In this Chapter you\u0026rsquo;ll follow the AWS suggested process to upgrade your cluster from 1.20 to 1.21 including its Managed Node Group to get first-hand experience with this process and where EKS and Managed Node Groups help.\n"
},
{
	"uri": "/intermediate/320_eks_upgrades/theprocess/",
	"title": "The Upgrade Process",
	"tags": [],
	"description": "",
	"content": "The process goes as follows:\n (Optional) Check if the new version you are upgrading to has any API deprecations which will mean that you\u0026rsquo;ll need to change your YAML Spec files for them to continue to work on the new cluster. This is only the case with some version upgrades such as 1.15 to 1.16. There are various tools that can help with this such as kube-no-trouble. Since there are not any such deprecations going from 1.20 to 1.21 we\u0026rsquo;ll skip this step here. Run a kubectl get nodes and ensure that all of your Nodes are running the current version. Kubernetes can only support nodes that are one version behind - meaning they all need to match the current Kubernetes version so when you upgrade the EKS control plane they\u0026rsquo;ll then only be one version behind. For Fargate relaunching a Pod (maybe by deleted it and letting the ReplicaSet replace it) will bring it in line with the current Kubernetes version. Upgrade the EKS Control Plane to the new major version Check if the core add-ons (kubeproxy, CoreDNS and the CNI) that ship with EKS require an upgrade to conincide with the major version upgrade. This will be in our upgrade documentation. If so follow that documentation to upgrade those. In this case (a 1.20 to 1.21 upgrade) the documentation says we\u0026rsquo;ll need to upgrade both CoreDNS and kubeproxy. Upgrade any worker nodes so that the kubelet on them (which will now be one Kubernete major version behind) matches that of the EKS control plane. While you don\u0026rsquo;t have to do this immediatly it is a good idea to have the Nodes on the same version as the control plane as soon as is practical - plus it will make Step 2 easier the next time you have to upgrade. If you are using Managed Node Groups (as we are here) then EKS can help facilitate this with a safe automated process which orchestrates both the AWS and Kubernetes side of a rolling Node replacement/upgrade. If you are using Fargate then this will happen automatically the next time your Pods are replaced.  "
},
{
	"uri": "/intermediate/320_eks_upgrades/upgradeeks/",
	"title": "Upgrade EKS Control Plane",
	"tags": [],
	"description": "",
	"content": "The first step of this process is to upgrade the EKS Control Plane.\nSince we used eksctl to provision our cluster we\u0026rsquo;ll use that tool to do our upgrade as well.\nFirst we\u0026rsquo;ll run this command\neksctl upgrade cluster --name=eksworkshop-eksctl You\u0026rsquo;ll see in the output that it found our cluster, worked out that it is 1.20 and the next version is 1.21 (you can only go to the next version with EKS) and that everything is ready for us to proceed with an upgrade.\n$ eksctl upgrade cluster --name=eksworkshop-eksctl [‚Ñπ] eksctl version 0.66.0 [‚Ñπ] using region us-west-2 [‚Ñπ] (plan) would upgrade cluster \u0026quot;eksworkshop-eksctl\u0026quot; control plane from current version \u0026quot;1.20\u0026quot; to \u0026quot;1.21\u0026quot; [‚Ñπ] re-building cluster stack \u0026quot;eksctl-eksworkshop-eksctl-cluster\u0026quot; [‚úî] all resources in cluster stack \u0026quot;eksctl-eksworkshop-eksctl-cluster\u0026quot; are up-to-date [‚Ñπ] checking security group configuration for all nodegroups [‚Ñπ] all nodegroups have up-to-date configuration [!] no changes were applied, run again with '--approve' to apply the changes We\u0026rsquo;ll run it again with an \u0026ndash;approve appended to proceed\neksctl upgrade cluster --name=eksworkshop-eksctl --approve  This process should take approximately 25 minutes. You can continue to use the cluster during the control plane upgrade process but you might experience minor service interruptions. For example, if you attempt to connect to one of the EKS API servers just before or just after it\u0026rsquo;s terminated and replaced by a new API server running the new version of Kubernetes, you might experience temporary API call errors or connectivity issues. If this happens, retry your API operations until they succeed. Your existing Pods/workloads running in the data plane should not experience any interruption during the control plane upgrade.\n Given how long this step will take and that the cluster will continue to work maybe move on to other workshop chapters until this process completes then come back to finish once it completes.\n "
},
{
	"uri": "/intermediate/320_eks_upgrades/upgradeaddons/",
	"title": "Upgrade EKS Core Add-ons",
	"tags": [],
	"description": "",
	"content": "When you provision an EKS cluster you get three add-ons that run on top of the cluster and that are required for it to function properly:\n kubeproxy CoreDNS aws-node (AWS CNI or Network Plugin)  Looking at the the upgrade documentation for our 1.20 to 1.21 upgrade we see that we\u0026rsquo;ll need to upgrade the kubeproxy and CoreDNS. In addition to performing these steps manually with kubectl as documented there you\u0026rsquo;ll find that eksctl can do it for you as well.\nSince we are using eksctl in the workshop we\u0026rsquo;ll run the two necessary commands for it to do these updates for us:\neksctl utils update-kube-proxy --cluster=eksworkshop-eksctl --approve and then\neksctl utils update-coredns --cluster=eksworkshop-eksctl --approve We can confirm we succeeded by retrieving the versions of each with the commands:\nkubectl get daemonset kube-proxy --namespace kube-system -o=jsonpath=\u0026#39;{$.spec.template.spec.containers[:1].image}\u0026#39; kubectl describe deployment coredns --namespace kube-system | grep Image | cut -d \u0026#34;/\u0026#34; -f 3 "
},
{
	"uri": "/intermediate/320_eks_upgrades/upgrademng/",
	"title": "Upgrade Managed Node Group",
	"tags": [],
	"description": "",
	"content": "Finally we have gotten to the last step of the upgrade process which is upgrading our Nodes.\nThere are two ways to provision and manage your worker nodes - self-managed node groups and managed node groups. In this workshop eksctl was configured to use the managed node groups. This was helpful here as managed node groups make this easier for us by automating both the AWS and the Kubernetes side of the process.\nThe way that managed node groups does this is:\n Amazon EKS creates a new Amazon EC2 launch template version for the Auto Scaling group associated with your node group. The new template uses the target AMI for the update. The Auto Scaling group is updated to use the latest launch template with the new AMI. The Auto Scaling group maximum size and desired size are incremented by one up to twice the number of Availability Zones in the Region that the Auto Scaling group is deployed in. This is to ensure that at least one new instance comes up in every Availability Zone in the Region that your node group is deployed in. Amazon EKS checks the nodes in the node group for the eks.amazonaws.com/nodegroup-image label, and applies a eks.amazonaws.com/nodegroup=unschedulable:NoSchedule taint on all of the nodes in the node group that aren\u0026rsquo;t labeled with the latest AMI ID. This prevents nodes that have already been updated from a previous failed update from being tainted. Amazon EKS randomly selects a node in the node group and evicts all pods from it. After all of the pods are evicted, Amazon EKS cordons the node. This is done so that the service controller doesn\u0026rsquo;t send any new request to this node and removes this node from its list of healthy, active nodes. Amazon EKS sends a termination request to the Auto Scaling group for the cordoned node. Steps 5-7 are repeated until there are no nodes in the node group that are deployed with the earlier version of the launch template. The Auto Scaling group maximum size and desired size are decremented by 1 to return to your pre-update values.  If we instead had used a self-managed node group then we need to do the Kubernetes taint and draining steps ourselves to ensure Kubernetes knows that Node is going away and can manage that process gracefully in order for such an upgrade to be non-disruptive.\n The first step only applies to if we are using the cluster autoscaler. We don\u0026rsquo;t want conflicting Node scaling actions during our upgrade so we should scale that to zero to suspend it during this process using the command below. Unless you have done that chapter in the workshop and left it deployed you can skip this step.\nkubectl scale deployments/cluster-autoscaler --replicas=0 -n kube-system We can then trigger the MNG upgrade process by running the following eksctl command:\neksctl upgrade nodegroup --name=nodegroup --cluster=eksworkshop-eksctl --kubernetes-version=1.21 In another Terminal tab you can follow the progress with:\nkubectl get nodes --watch You\u0026rsquo;ll notice the new nodes come up (three one in each AZ), one of the older nodes go STATUS SchedulingDisabled, then eventually that node go away and another new node come up to replace it and so on as described in the process above until all the old Nodes have gone away. Then it\u0026rsquo;ll scale back down from 6 Nodes to the original 3.\n"
},
{
	"uri": "/beginner/300_windows/sample_app_deploy/",
	"title": "Deploy an application",
	"tags": [],
	"description": "",
	"content": "NodeSelector You must specify node selectors on your applications so that the pods land on a node with the appropriate operating system.\nFor Linux pods, use the following node selector text in your manifests. nodeSelector: kubernetes.io/os: linux kubernetes.io/arch: amd64  For Windows pods, use the following node selector text in your manifests.\nnodeSelector: kubernetes.io/os: windows kubernetes.io/arch: amd64  Our deployment file already has the proper node selectors so you won\u0026rsquo;t have to add them yourself.\n Deploy a Windows sample application We are now ready to deploy our Windows IIS container\nkubectl create namespace windows cat \u0026lt;\u0026lt; EoF \u0026gt; ~/environment/windows/windows_server_iis.yaml apiVersion: apps/v1 kind: Deployment metadata: name: windows-server-iis namespace: windows spec: selector: matchLabels: app: windows-server-iis tier: backend track: stable replicas: 1 template: metadata: labels: app: windows-server-iis tier: backend track: stable spec: containers: - name: windows-server-iis image: mcr.microsoft.com/windows/servercore:2004 ports: - name: http containerPort: 80 imagePullPolicy: IfNotPresent command: - powershell.exe - -command - \u0026#34;Add-WindowsFeature Web-Server; Invoke-WebRequest -UseBasicParsing -Uri \u0026#39;https://dotnetbinaries.blob.core.windows.net/servicemonitor/2.0.1.6/ServiceMonitor.exe\u0026#39; -OutFile \u0026#39;C:\\\\ServiceMonitor.exe\u0026#39;; echo \u0026#39;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;br/\u0026gt;\u0026lt;br/\u0026gt;\u0026lt;marquee\u0026gt;\u0026lt;H1\u0026gt;Hello EKS!!!\u0026lt;H1\u0026gt;\u0026lt;marquee\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;html\u0026gt;\u0026#39; \u0026gt; C:\\\\inetpub\\\\wwwroot\\\\default.html; C:\\\\ServiceMonitor.exe \u0026#39;w3svc\u0026#39;; \u0026#34; nodeSelector: kubernetes.io/os: windows --- apiVersion: v1 kind: Service metadata: name: windows-server-iis-service namespace: windows spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: windows-server-iis tier: backend track: stable sessionAffinity: None type: LoadBalancer EoF kubectl apply -f ~/environment/windows/windows_server_iis.yaml Let\u0026rsquo;s verify what we just deployed\nkubectl -n windows get svc,deploy,pods NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/windows-server-iis-service LoadBalancer 10.100.114.95 a52bab64f1fb34b338104c4ab20eb867-195815018.us-east-2.elb.amazonaws.com 80:31184/TCP 68s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/windows-server-iis 1/1 1 1 68s NAME READY STATUS RESTARTS AGE pod/windows-server-iis-7cff879775-7t8hk 1/1 Running 0 68s  It will take several minutes for the ELB to become healthy and start passing traffic to the pods.\n Finally, we will connect to the load-balancer\nexport WINDOWS_IIS_SVC=$(kubectl -n windows get svc -o jsonpath=\u0026#39;{.items[].status.loadBalancer.ingress[].hostname}\u0026#39;) echo http://${WINDOWS_IIS_SVC} Output\n"
},
{
	"uri": "/advanced/330_servicemesh_using_appmesh/",
	"title": "Service Mesh using AWS App Mesh",
	"tags": ["advanced", "operations", "servicemesh", "appmesh", "fargate"],
	"description": "",
	"content": "Meshifying with AWS App Mesh (Part 1)   Meshifying with AWS App Mesh (Part 2)   At re:invent 2018, we announced AWS App Mesh, a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. AWS App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications.\nService meshes like AWS App Mesh help you to run and monitor HTTP and TCP services at scale. Whether your application consists of AWS Fargate, Amazon EC2, Amazon ECS, Amazon Kubernetes Service, or Kubernetes clusters or instances, AWS App Mesh provides consistent routing and traffic monitoring functionality, giving you insight into problems and the ability to re-route traffic after failures or code changes.\nAWS App Mesh uses the open source Envoy proxy, giving you access to a wide range of tools from AWS partners and the open source community. Since all traffic in and out of each service goes through the Envoy proxy, all traffic can be routed, shaped, measured, and logged. This extra level of indirection lets you build your services in any desired languages without having to use a common set of communication libraries.\nIn this tutorial, we\u0026rsquo;ll walk you through the following, which are popular App Mesh use cases using the example of below Product Catalog Application deployment:\n Deploy a microservices-based application in Amazon EKS using AWS Fargate Configure an App Mesh Virtual Gateway to route traffic to the application services Create a Canary Deployment using App Mesh Enable observability features with App Mesh, including logging for Fargate, Amazon Cloudwatch Container Insights, and AWS X-Ray tracing  "
},
{
	"uri": "/intermediate/330_app_mesh/",
	"title": "Getting Started with AWS App Mesh",
	"tags": ["intermediate", "operations", "servicemesh", "appmesh"],
	"description": "",
	"content": "AWS App Mesh on Amazon EKS   Getting Fancy with AWS App Mesh on Amazon EKS   At re:invent 2018, we announced AWS App Mesh, a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications.\nService meshes like AWS App Mesh help you to run and monitor HTTP and TCP services at scale. Whether your application consists of AWS Fargate, Amazon EC2, Amazon ECS, Amazon Kubernetes Service, or Kubernetes clusters or instances, App Mesh provides consistent routing and traffic monitoring functionality, giving you insight into problems and the ability to re-route traffic after failures or code changes.\nApp Mesh uses the open source Envoy proxy, giving you access to a wide range of tools from AWS partners and the open source community. Since all traffic in and out of each service goes through the Envoy proxy, all traffic can be routed, shaped, measured, and logged. This extra level of indirection lets you build your services in any desired languages without having to use a common set of communication libraries.\nIn this tutorial, we\u0026rsquo;ll walk you through many popular App Mesh use cases.\nThey will take you through building an easy to understand standalone k8s microservices-based application, and then enabling App Mesh service mesh functionality for it.\nThe first two sections, Deploy the DJ App, and AWS App Mesh Integration should be performed in order.\n "
},
{
	"uri": "/advanced/340_appmesh_flagger/",
	"title": "Canary Deployment using Flagger in AWS App Mesh",
	"tags": ["advanced", "appmesh", "automatedcanary", "flagger"],
	"description": "",
	"content": "Flagger is a progressive delivery tool that automates the release process for applications running on Kubernetes. Flagger can be configured to automate the release process for Kubernetes workloads with a custom resource named canary.\nWe saw in the Canary chapter of Service Mesh using App Mesh workshop on how to deploy new version of detail service by changing the VirtualRouter configuration. In this workshop, we will show you how to use Flagger in App Mesh to automate the canary deployment of backend service detail of our Product Catalog Application.\nImage source:https://docs.flagger.app/\n"
},
{
	"uri": "/beginner/300_windows/calico_windows/",
	"title": "Create Network Policies Using Calico",
	"tags": [],
	"description": "",
	"content": "In this Chapter, we will create some network policies using Calico and see the rules in action.\nNetwork policies allow you to define rules that determine what type of traffic is allowed to flow between different services. Using network policies you can also define rules to restrict traffic. They are a means to improve your cluster\u0026rsquo;s security.\nFor example, you can only allow traffic from frontend to backend in your application.\nNetwork policies also help in isolating traffic within namespaces. For instance, if you have separate namespaces for development and production, you can prevent traffic flow between them by restrict pod to pod communication within the same namespace.\n"
},
{
	"uri": "/beginner/300_windows/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "calicoctl delete -f - \u0026lt; ~/environment/windows/deny_icmp.yaml unalias calicoctl kubectl delete -f https://docs.projectcalico.org/archive/v3.15/manifests/calicoctl.yaml kubectl delete -f ~/environment/windows/sample-deployments.yaml kubectl delete -f ~/environment/windows/user-rolebinding.yaml kubectl delete -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.6/calico.yaml kubectl delete -f ~/environment/windows/windows_server_iis.yaml kubectl delete namespace windows eksctl delete nodegroup \\  -f ~/environment/windows/windows_nodes.yaml \\  --approve \\  --wait rm -rf ~/environment/windows/ "
},
{
	"uri": "/beginner/060_helm/helm_nginx/installnginx/",
	"title": "Install bitnami/nginx",
	"tags": [],
	"description": "",
	"content": "Installing the Bitnami standalone nginx web server Chart involves us using the helm install command.\nA Helm Chart can be installed multiple times inside a Kubernetes cluster. This is because each installation of a Chart can be customized to suit a different purpose.\nFor this reason, you must supply a unique name for the installation, or ask Helm to generate a name for you.\nChallenge: How can you use Helm to deploy the bitnami/nginx chart?\nHINT: Use the helm utility to install the bitnami/nginx chart and specify the name mywebserver for the Kubernetes deployment. Consult the helm install documentation or run the helm install --help command to figure out the syntax.\n  Expand here to see the solution   helm install mywebserver bitnami/nginx    Once you run this command, the output will contain the information about the deployment status, revision, namespace, etc, similar to:\nNAME: mywebserver LAST DEPLOYED: Thu Jul 15 13:52:34 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** NGINX can be accessed through the following DNS name from within your cluster: mywebserver-nginx.default.svc.cluster.local (port 80) To access NGINX from outside the cluster, follow the steps below: 1. Get the NGINX URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: \u0026#39;kubectl get svc --namespace default -w mywebserver-nginx\u0026#39; export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath=\u0026#34;{.spec.ports[0].port}\u0026#34; services mywebserver-nginx) export SERVICE_IP=$(kubectl get svc --namespace default mywebserver-nginx -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) echo \u0026#34;http://${SERVICE_IP}:${SERVICE_PORT}\u0026#34;  In order to review the underlying Kubernetes services, pods and deployments, run:\nkubectl get svc,po,deploy  In the following kubectl command examples, it may take a minute or two for each of these objects' DESIRED and CURRENT values to match; if they don\u0026rsquo;t match on the first try, wait a few seconds, and run the command again to check the status.\n The first object shown in this output is a Deployment. A Deployment object manages rollouts (and rollbacks) of different versions of an application.\nYou can inspect this Deployment object in more detail by running the following command:\nkubectl describe deployment mywebserver The next object shown created by the Chart is a Pod. A Pod is a group of one or more containers.\nTo verify the Pod object was successfully deployed, we can run the following command:\nkubectl get pods -l app.kubernetes.io/name=nginx And you should see output similar to:\nNAME READY STATUS RESTARTS AGE mywebserver-nginx-85985c8466-tczst 1/1 Running 0 10s  The third object that this Chart creates for us is a Service. A Service enables us to contact this nginx web server from the Internet, via an Elastic Load Balancer (ELB).\nTo get the complete URL of this Service, run:\nkubectl get service mywebserver-nginx -o wide That should output something similar to:\nNAME TYPE CLUSTER-IP EXTERNAL-IP mywebserver-nginx LoadBalancer 10.100.223.99 abc123.amazonaws.com  Copy the value for EXTERNAL-IP, open a new tab in your web browser, and paste it in.\nIt may take a couple minutes for the ELB and its associated DNS name to become available; if you get an error, wait one minute, and hit reload.\n When the Service does come online, you should see a welcome message similar to:\nCongratulations! You\u0026rsquo;ve now successfully deployed the nginx standalone web server to your EKS cluster!\n"
},
{
	"uri": "/advanced/410_batch/",
	"title": "Batch Processing with Argo Workflow",
	"tags": ["advanced", "batch", "CON205"],
	"description": "",
	"content": "Batch Processing In this Chapter, we will deploy common batch processing scenarios using Kubernetes and Argo.\nWhat is Argo? Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD (Custom Resource Definition).\n Define workflows where each step in the workflow is a container. Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a directed acyclic graph (DAG). Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo Workflows on Kubernetes. Run CI/CD pipelines natively on Kubernetes without configuring complex software development products.  Argo is a Cloud Native Computing Foundation (CNCF) hosted project.\n"
},
{
	"uri": "/advanced/420_kubeflow/",
	"title": "Machine Learning using Kubeflow",
	"tags": ["advanced", "kubeflow", "ml", "OPN401"],
	"description": "",
	"content": "Machine Learning using Kubeflow   Kubeflow provides a simple, portable, and scalable way of running Machine Learning workloads on Kubernetes.\nIn this module, we will install Kubeflow on Amazon EKS, run a single-node training and inference using TensorFlow, train and deploy model locally and remotely using Fairing, setup Kubeflow pipeline and review how to call AWS managed services such as Sagemaker for training and inference.\n"
},
{
	"uri": "/advanced/430_emr_on_eks/",
	"title": "EMR on EKS",
	"tags": ["advanced", "emr on eks", "spark"],
	"description": "",
	"content": "EMR on EKS EMR on EKS is a deployment option in EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. There are several advantages of running optimized spark runtime provided by Amazon EMR on EKS such as 3x faster performance, fully managed lifecycle of these jobs, built-in monitoring and logging functionality, integrates securely with Kubernetes and more. Because Kubernetes can natively run Spark jobs, if you use multi-tenant EKS environment (shared with other micro-services), your spark jobs are deployed in seconds vs minutes when compared to EC2 based deployments.\nIn this module, we will review how to setup your EKS cluster and run a sample spark job, setup monitoring and logging for these jobs, configure autoscaling, use Kubernetes node selectors for jobs that need to meet certain constraints such as run in single-az, use spot best practices for running EMR on EKS, and use the serverless compute engine AWS Fargate with Amazon EKS to support EMR workloads.\nFor more hands-on labs, see the dedicated EMR on EKS Workshop.\n"
},
{
	"uri": "/beginner/060_helm/helm_nginx/cleaningup/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "To remove all the objects that the Helm Chart created, we can use Helm uninstall.\nBefore we uninstall our application, we can verify what we have running via the Helm list command:\nhelm list You should see output similar to below, which show that mywebserver is installed: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mywebserver default 1 2021-07-15 13:52:34.563653342 \u0026#43;0000 UTC deployed nginx-9.3.7 1.21.1  It was a lot of fun; we had some great times sending HTTP back and forth, but now its time to uninstall this deployment. To uninstall:\nhelm uninstall mywebserver And you should be met with the output: release \u0026#34;mywebserver\u0026#34; uninstalled  kubectl will also demonstrate that our pods and service are no longer available:\nkubectl get pods -l app.kubernetes.io/name=nginx kubectl get service mywebserver-nginx -o wide As would trying to access the service via the web browser via a page reload.\nWith that, cleanup is complete.\n"
},
{
	"uri": "/910_conclusion/",
	"title": "Conclusion",
	"tags": ["beginner"],
	"description": "",
	"content": "Conclusion "
},
{
	"uri": "/920_cleanup/",
	"title": "Cleanup",
	"tags": ["beginner"],
	"description": "",
	"content": "Cleanup "
},
{
	"uri": "/tags/advanced/",
	"title": "advanced",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/aws-distro-for-open-telemetry/",
	"title": "AWS Distro for Open Telemetry",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/metrics/",
	"title": "metrics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/observability/",
	"title": "observability",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/traces/",
	"title": "traces",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/con205/",
	"title": "CON205",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/intermediate/",
	"title": "intermediate",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/beginner/",
	"title": "beginner",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/con206/",
	"title": "CON206",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/emr-on-eks/",
	"title": "emr on eks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/spark/",
	"title": "spark",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/beginner/300_windows/",
	"title": "Windows containers on EKS",
	"tags": ["beginner"],
	"description": "",
	"content": "Windows containers on EKS Many development teams build and support applications designed to run on Windows Servers and with Windows Container Support on EKS, they can deploy them on Kubernetes alongside Linux applications. This ability will provide more consistency in system logging, performance monitoring, and code deployment pipelines.\nTo demonstrate how this feature works, you will add a new node group which contains 2 Windows servers 2019 to our EKS Cluster and deploy a Windows sample application. Finally, you will test the application to ensure it is running as expected.\n"
},
{
	"uri": "/tags/con203/",
	"title": "CON203",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/appmesh/",
	"title": "appmesh",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/fargate/",
	"title": "fargate",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/operations/",
	"title": "operations",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/servicemesh/",
	"title": "servicemesh",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/automatedcanary/",
	"title": "automatedcanary",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/batch/",
	"title": "batch",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/flagger/",
	"title": "flagger",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/amazon-managed-service-for-grafana/",
	"title": "Amazon Managed Service for Grafana",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/amazon-managed-service-for-prometheus/",
	"title": "Amazon Managed Service for Prometheus",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/amg/",
	"title": "AMG",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/amp/",
	"title": "AMP",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/monitoring/",
	"title": "monitoring",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tabs-example/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Tabs with regular text:  Tab 1 Tab 2  echo \"This is tab 1.\"  println \"This is tab 2.\"   $(function(){$(\"#tab\").tabs();}); Tabs with code blocks:  Tab 1 Tab 2  echo \u0026#34;This is tab 1.\u0026#34;   println \u0026#34;This is tab 2.\u0026#34;    $(function(){$(\"#tab_with_code\").tabs();}); Tabs showing installation process:  eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text) Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS} Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#tab_installation\").tabs();}); Second set of tabs showing installation process:  eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text) Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS} Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#more_tab_installation\").tabs();}); "
},
{
	"uri": "/tabs-example/tabs/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/cd/",
	"title": "cd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/ci/cd/",
	"title": "ci/cd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/cis-eks-benchmark/",
	"title": "cis eks benchmark",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/crd/",
	"title": "crd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/authors/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": "Thanks to our wonderful contributors for making Open Source a better place! Please go to Contributors page to checkout authors for this Workshop\n"
},
{
	"uri": "/tags/debugging/",
	"title": "debugging",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/devops/",
	"title": "devops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tabs-example/tabs/eks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text) Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS} Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text When your cluster status is ACTIVE you can proceed.\n"
},
{
	"uri": "/tabs-example/tabs/launcheks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "We start by initializing the Terraform state:\nterraform init We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n "
},
{
	"uri": "/tags/gitops/",
	"title": "gitops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/020_prerequisites/eu-west-1/",
	"title": "Ireland",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n"
},
{
	"uri": "/020_prerequisites/self_paced/eu-west-1/",
	"title": "Ireland",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n"
},
{
	"uri": "/tags/kube-bench/",
	"title": "kube-bench",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/kubeflow/",
	"title": "kubeflow",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/logging/",
	"title": "logging",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/ml/",
	"title": "ml",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/more_resources/",
	"title": "More Resources",
	"tags": [],
	"description": "",
	"content": "Discover more AWS resources for building and running your application on AWS:\n Containers from the Couch - Check out our latest container shows, and learn all about running containers!  More Workshops  Amazon ECS Workshop - Learn how to use Stelligent Mu to deploy a microservice architecture that runs in AWS Fargate Amazon Lightsail Workshop - If you are getting started with the cloud and looking for a way to run an extremely low cost environment Lightsail is perfect. Learn how to deploy to Amazon Lightsail with this workshop.  Tools for AWS Fargate and Amazon ECS  Containers on AWS - Learn common best-practices for running containers on AWS   fargate - Command line tool for interacting with AWS Fargate. With just a single command you can build, push, and launch your container in Fargate, orchestrated by ECS. Wonqa is a tool for spinning up disposable QA environments in AWS Fargate, with SSL enabled by Let\u0026rsquo;s Encrypt. More details about Wonqa on the Wonder Engineering blog coldbrew - Fantastic tool that provisions ECS infrastructure, builds and deploys your container, and connects your services to an application load balancer automatically. Has a great developer experience for day to day use mu - Automates everything relating to ECS devops and CI/CD. This framework lets you write a simple metadata file and it constructs all the infrastructure you need so that you can deploy to ECS by simply pushing to your Git repo.  Courses  Microservices with Docker, Flask, and React - Learn how to build, test, and deploy microservices powered by Docker, Flask, React Amazon ECS!  "
},
{
	"uri": "/020_prerequisites/self_paced/us-east-2/",
	"title": "Ohio",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n"
},
{
	"uri": "/020_prerequisites/us-east-2/",
	"title": "Ohio",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n"
},
{
	"uri": "/tags/opa/",
	"title": "opa",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/open-policy-agent/",
	"title": "open policy agent",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/opn401/",
	"title": "OPN401",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/020_prerequisites/self_paced/us-west-2/",
	"title": "Oregon",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n"
},
{
	"uri": "/020_prerequisites/us-west-2/",
	"title": "Oregon",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n"
},
{
	"uri": "/020_prerequisites/ap-southeast-1/",
	"title": "Singapore",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://ap-southeast-1.console.aws.amazon.com/cloud9/home?region=ap-southeast-1\n"
},
{
	"uri": "/020_prerequisites/self_paced/ap-southeast-1/",
	"title": "Singapore",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://ap-southeast-1.console.aws.amazon.com/cloud9/home?region=ap-southeast-1\n"
},
{
	"uri": "/tags/spinnnaker/",
	"title": "spinnnaker",
	"tags": [],
	"description": "",
	"content": ""
}]